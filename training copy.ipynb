{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\python.exe\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env acii\n",
    "# with validation set from Training set\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# From Stackoverflow\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "from numpy.random import seed\n",
    "from numpy import array, vstack, hstack, stack\n",
    "from utils import unison_shuffled_copies, unison_shuffled_copies_two, NDStandardScaler\n",
    "from utils import mk_dirs, create_csv, create_dirs, f1_m, precision_m, recall_m, create_multicsv\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy.random import seed\n",
    "from numpy import array, vstack, hstack, stack\n",
    "import random as rn\n",
    "rn.seed(4)\n",
    "import gc\n",
    "import datetime\n",
    "import inspect\n",
    "import pickle\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mega_model import *\n",
    "from mega_model_resnet import *\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_addons as tfa\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "import h5py\n",
    "import neurokit2 as nk\n",
    "from statistics import mean, mode, StatisticsError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "seed(2)\n",
    "tf.random.set_seed(42)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "\n",
    "main_path = r\"X:\\Thesis\\matb2\\Processed_Data\"\n",
    "\n",
    "with open(os.path.join(main_path, 'cola_ecg.pickle'), 'rb') as handle:\n",
    "    sub_dict_ecg = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(main_path, 'cola_labels.pickle'), 'rb') as handle:\n",
    "    sub_label_ecg = pickle.load(handle)\n",
    "    \n",
    "with open(os.path.join(main_path, 'cola_eda.pickle'), 'rb') as handle:\n",
    "    sub_dict_eda = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecga (Conv1D)       (None, 2560, 32)     2080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edaa (Conv1D)       (None, 2560, 32)     6176        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecga (Activation)    (None, 2560, 32)     0           conv_stage1_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edaa (Activation)    (None, 2560, 32)     0           conv_stage1_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecgb (Conv1D)       (None, 854, 32)      65568       act_stage1_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edab (Conv1D)       (None, 854, 32)      65568       act_stage1_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecgb (Activation)    (None, 854, 32)      0           conv_stage1_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edab (Activation)    (None, 854, 32)      0           conv_stage1_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_ecg (MaxPooling1D)    (None, 427, 32)      0           act_stage1_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_eda (MaxPooling1D)    (None, 427, 32)      0           act_stage1_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 427, 1, 32)   0           mp_stage1_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 427, 1, 32)   0           mp_stage1_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 427, 2, 32)   0           tf.expand_dims[0][0]             \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 427, 32, 2)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 427, 32, 2)   6           tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 427, 32, 2)   0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 427, 2, 32)   0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Attention_laye ((None, 427, 1), (No 32          tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 427, 32)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 attention_layer[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 427, 32)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecga (Conv1D)       (None, 427, 64)      131136      tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edaa (Conv1D)       (None, 427, 64)      131136      tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecga (Activation)    (None, 427, 64)      0           conv_stage2_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edaa (Activation)    (None, 427, 64)      0           conv_stage2_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecgb (Conv1D)       (None, 143, 64)      131136      act_stage2_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edab (Conv1D)       (None, 143, 64)      131136      act_stage2_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecgb (Activation)    (None, 143, 64)      0           conv_stage2_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edab (Activation)    (None, 143, 64)      0           conv_stage2_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_ecg (MaxPooling1D)    (None, 71, 64)       0           act_stage2_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_eda (MaxPooling1D)    (None, 71, 64)       0           act_stage2_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 71, 2, 64)    0           tf.expand_dims_2[0][0]           \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 71, 64, 2)    0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 71, 64, 2)    6           tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 71, 64, 2)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 71, 2, 64)    0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (Attention_la ((None, 71, 1), (Non 64          tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 71, 64)       0           mp_stage2_eda[0][0]              \n",
      "                                                                 attention_layer_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 71, 64)       0           mp_stage2_ecg[0][0]              \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_5 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_eda[0][0]              \n",
      "                                                                 tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecga (Conv1D)       (None, 71, 128)      278656      tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edaa (Conv1D)       (None, 71, 128)      278656      tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecga (Activation)    (None, 71, 128)      0           conv_stage3_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edaa (Activation)    (None, 71, 128)      0           conv_stage3_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecgb (Conv1D)       (None, 24, 128)      278656      act_stage3_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edab (Conv1D)       (None, 24, 128)      278656      act_stage3_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecgb (Activation)    (None, 24, 128)      0           conv_stage3_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edab (Activation)    (None, 24, 128)      0           conv_stage3_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_ecg (MaxPooling1D)    (None, 12, 128)      0           act_stage3_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_eda (MaxPooling1D)    (None, 12, 128)      0           act_stage3_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (None, 12, 2, 128)   0           tf.expand_dims_4[0][0]           \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (None, 12, 128, 2)   0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12, 128, 2)   6           tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 12, 128, 2)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_5 (TFOpL (None, 12, 2, 128)   0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (Attention_la ((None, 12, 1), (Non 128         tf.compat.v1.transpose_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 12, 128)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 attention_layer_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None, 12, 128)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecga (Conv1D)       (None, 12, 256)      459008      tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edaa (Conv1D)       (None, 12, 256)      459008      tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecga (Activation)    (None, 12, 256)      0           conv_stage4_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edaa (Activation)    (None, 12, 256)      0           conv_stage4_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecgb (Conv1D)       (None, 4, 256)       459008      act_stage4_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edab (Conv1D)       (None, 4, 256)       459008      act_stage4_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecgb (Activation)    (None, 4, 256)       0           conv_stage4_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edab (Activation)    (None, 4, 256)       0           conv_stage4_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_ecg (MaxPooling1D)    (None, 2, 256)       0           act_stage4_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_eda (MaxPooling1D)    (None, 2, 256)       0           act_stage4_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           mp_stage4_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           mp_stage4_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 512)          262656      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 512)          262656      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            2050        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,667,508\n",
      "Trainable params: 4,667,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type II, Stage two_three\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220523-222508\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1105\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 17s - loss: 0.6942 - acc: 0.4689 - f1_score: 0.3192 - val_loss: 0.6933 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6937 - acc: 0.4691 - f1_score: 0.3195 - val_loss: 0.6920 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6932 - acc: 0.4689 - f1_score: 0.3209 - val_loss: 0.6908 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4697 - f1_score: 0.3277 - val_loss: 0.6896 - val_acc: 0.4145 - val_f1_score: 0.2931\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4781 - f1_score: 0.3506 - val_loss: 0.6883 - val_acc: 0.4209 - val_f1_score: 0.3109\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6919 - acc: 0.4953 - f1_score: 0.3920 - val_loss: 0.6870 - val_acc: 0.4872 - val_f1_score: 0.4498\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.5083 - f1_score: 0.4268 - val_loss: 0.6858 - val_acc: 0.5876 - val_f1_score: 0.5868\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6910 - acc: 0.5159 - f1_score: 0.4696 - val_loss: 0.6845 - val_acc: 0.6624 - val_f1_score: 0.6571\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6906 - acc: 0.5331 - f1_score: 0.5180 - val_loss: 0.6832 - val_acc: 0.6581 - val_f1_score: 0.6444\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6902 - acc: 0.5485 - f1_score: 0.5477 - val_loss: 0.6819 - val_acc: 0.6603 - val_f1_score: 0.6422\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.5564 - f1_score: 0.5555 - val_loss: 0.6807 - val_acc: 0.6667 - val_f1_score: 0.6464\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6894 - acc: 0.5551 - f1_score: 0.5507 - val_loss: 0.6795 - val_acc: 0.6688 - val_f1_score: 0.6467\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5551 - f1_score: 0.5486 - val_loss: 0.6784 - val_acc: 0.6752 - val_f1_score: 0.6515\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5553 - f1_score: 0.5462 - val_loss: 0.6773 - val_acc: 0.6795 - val_f1_score: 0.6553\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6884 - acc: 0.5554 - f1_score: 0.5450 - val_loss: 0.6762 - val_acc: 0.6795 - val_f1_score: 0.6536\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5557 - f1_score: 0.5432 - val_loss: 0.6752 - val_acc: 0.6795 - val_f1_score: 0.6536\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6877 - acc: 0.5547 - f1_score: 0.5411 - val_loss: 0.6741 - val_acc: 0.6774 - val_f1_score: 0.6508\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6874 - acc: 0.5551 - f1_score: 0.5406 - val_loss: 0.6731 - val_acc: 0.6731 - val_f1_score: 0.6453\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6871 - acc: 0.5550 - f1_score: 0.5400 - val_loss: 0.6722 - val_acc: 0.6752 - val_f1_score: 0.6462\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5551 - f1_score: 0.5390 - val_loss: 0.6713 - val_acc: 0.6774 - val_f1_score: 0.6481\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5547 - f1_score: 0.5374 - val_loss: 0.6704 - val_acc: 0.6752 - val_f1_score: 0.6443\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5547 - f1_score: 0.5374 - val_loss: 0.6696 - val_acc: 0.6752 - val_f1_score: 0.6443\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5550 - f1_score: 0.5367 - val_loss: 0.6687 - val_acc: 0.6795 - val_f1_score: 0.6480\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5566 - f1_score: 0.5371 - val_loss: 0.6679 - val_acc: 0.6795 - val_f1_score: 0.6480\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5564 - f1_score: 0.5369 - val_loss: 0.6672 - val_acc: 0.6774 - val_f1_score: 0.6451\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5573 - f1_score: 0.5375 - val_loss: 0.6665 - val_acc: 0.6774 - val_f1_score: 0.6451\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5587 - f1_score: 0.5380 - val_loss: 0.6657 - val_acc: 0.6774 - val_f1_score: 0.6451\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5593 - f1_score: 0.5381 - val_loss: 0.6650 - val_acc: 0.6816 - val_f1_score: 0.6488\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.59      0.61       215\n",
      "           1       0.67      0.73      0.70       253\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.66      0.66      0.66       468\n",
      "weighted avg       0.66      0.66      0.66       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1106\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 12s - loss: 0.6934 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7054 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7058 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6921 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7063 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.4917 - f1_score: 0.3299 - val_loss: 0.7070 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 5/300\n",
      "27/27 - 4s - loss: 0.6908 - acc: 0.4917 - f1_score: 0.3304 - val_loss: 0.7076 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6902 - acc: 0.4920 - f1_score: 0.3337 - val_loss: 0.7083 - val_acc: 0.0983 - val_f1_score: 0.0941\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6896 - acc: 0.4938 - f1_score: 0.3395 - val_loss: 0.7091 - val_acc: 0.1496 - val_f1_score: 0.1494\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6890 - acc: 0.4972 - f1_score: 0.3515 - val_loss: 0.7099 - val_acc: 0.2415 - val_f1_score: 0.2344\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5017 - f1_score: 0.3680 - val_loss: 0.7107 - val_acc: 0.2906 - val_f1_score: 0.2670\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5106 - f1_score: 0.3994 - val_loss: 0.7114 - val_acc: 0.3632 - val_f1_score: 0.3200\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6874 - acc: 0.5225 - f1_score: 0.4405 - val_loss: 0.7120 - val_acc: 0.4359 - val_f1_score: 0.3694\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6870 - acc: 0.5393 - f1_score: 0.4958 - val_loss: 0.7126 - val_acc: 0.4637 - val_f1_score: 0.3875\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5561 - f1_score: 0.5344 - val_loss: 0.7130 - val_acc: 0.5085 - val_f1_score: 0.4164\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5621 - f1_score: 0.5549 - val_loss: 0.7133 - val_acc: 0.5192 - val_f1_score: 0.4231\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5640 - f1_score: 0.5622 - val_loss: 0.7136 - val_acc: 0.5470 - val_f1_score: 0.4407\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5608 - f1_score: 0.5606 - val_loss: 0.7138 - val_acc: 0.5684 - val_f1_score: 0.4543\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6851 - acc: 0.5589 - f1_score: 0.5589 - val_loss: 0.7139 - val_acc: 0.5748 - val_f1_score: 0.4583\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5590 - f1_score: 0.5586 - val_loss: 0.7139 - val_acc: 0.5855 - val_f1_score: 0.4651\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5589 - f1_score: 0.5580 - val_loss: 0.7137 - val_acc: 0.5983 - val_f1_score: 0.4733\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6842 - acc: 0.5589 - f1_score: 0.5572 - val_loss: 0.7135 - val_acc: 0.6026 - val_f1_score: 0.4760\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5603 - f1_score: 0.5581 - val_loss: 0.7133 - val_acc: 0.6026 - val_f1_score: 0.4760\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5589 - f1_score: 0.5562 - val_loss: 0.7130 - val_acc: 0.6090 - val_f1_score: 0.4801\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5574 - f1_score: 0.5540 - val_loss: 0.7128 - val_acc: 0.6154 - val_f1_score: 0.4842\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5582 - f1_score: 0.5541 - val_loss: 0.7127 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5585 - f1_score: 0.5542 - val_loss: 0.7123 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5585 - f1_score: 0.5537 - val_loss: 0.7118 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5582 - f1_score: 0.5529 - val_loss: 0.7116 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5579 - f1_score: 0.5523 - val_loss: 0.7113 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5576 - f1_score: 0.5515 - val_loss: 0.7112 - val_acc: 0.6303 - val_f1_score: 0.4939\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5574 - f1_score: 0.5512 - val_loss: 0.7110 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5577 - f1_score: 0.5512 - val_loss: 0.7107 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5586 - f1_score: 0.5518 - val_loss: 0.7102 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5580 - f1_score: 0.5507 - val_loss: 0.7100 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5583 - f1_score: 0.5509 - val_loss: 0.7095 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5585 - f1_score: 0.5504 - val_loss: 0.7092 - val_acc: 0.6389 - val_f1_score: 0.4995\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5580 - f1_score: 0.5499 - val_loss: 0.7086 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5587 - f1_score: 0.5499 - val_loss: 0.7083 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5589 - f1_score: 0.5495 - val_loss: 0.7081 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5574 - f1_score: 0.5476 - val_loss: 0.7081 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5570 - f1_score: 0.5469 - val_loss: 0.7079 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5580 - f1_score: 0.5478 - val_loss: 0.7075 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5587 - f1_score: 0.5482 - val_loss: 0.7073 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5585 - f1_score: 0.5478 - val_loss: 0.7070 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5590 - f1_score: 0.5483 - val_loss: 0.7067 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5602 - f1_score: 0.5490 - val_loss: 0.7064 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6789 - acc: 0.5600 - f1_score: 0.5485 - val_loss: 0.7062 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5603 - f1_score: 0.5487 - val_loss: 0.7059 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5612 - f1_score: 0.5493 - val_loss: 0.7056 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5611 - f1_score: 0.5487 - val_loss: 0.7056 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5618 - f1_score: 0.5497 - val_loss: 0.7051 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5619 - f1_score: 0.5492 - val_loss: 0.7053 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5619 - f1_score: 0.5490 - val_loss: 0.7057 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5622 - f1_score: 0.5493 - val_loss: 0.7055 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5628 - f1_score: 0.5496 - val_loss: 0.7054 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5642 - f1_score: 0.5507 - val_loss: 0.7054 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5641 - f1_score: 0.5502 - val_loss: 0.7058 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6772 - acc: 0.5644 - f1_score: 0.5506 - val_loss: 0.7058 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6770 - acc: 0.5650 - f1_score: 0.5505 - val_loss: 0.7065 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6769 - acc: 0.5657 - f1_score: 0.5515 - val_loss: 0.7064 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6767 - acc: 0.5651 - f1_score: 0.5507 - val_loss: 0.7065 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6766 - acc: 0.5651 - f1_score: 0.5509 - val_loss: 0.7057 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6764 - acc: 0.5653 - f1_score: 0.5506 - val_loss: 0.7055 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6762 - acc: 0.5660 - f1_score: 0.5507 - val_loss: 0.7064 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 64/300\n",
      "27/27 - 3s - loss: 0.6761 - acc: 0.5657 - f1_score: 0.5502 - val_loss: 0.7073 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 65/300\n",
      "27/27 - 3s - loss: 0.6759 - acc: 0.5661 - f1_score: 0.5509 - val_loss: 0.7070 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 66/300\n",
      "27/27 - 3s - loss: 0.6758 - acc: 0.5666 - f1_score: 0.5508 - val_loss: 0.7074 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 67/300\n",
      "27/27 - 3s - loss: 0.6756 - acc: 0.5676 - f1_score: 0.5517 - val_loss: 0.7079 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 68/300\n",
      "27/27 - 3s - loss: 0.6755 - acc: 0.5673 - f1_score: 0.5514 - val_loss: 0.7081 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 69/300\n",
      "27/27 - 3s - loss: 0.6753 - acc: 0.5674 - f1_score: 0.5517 - val_loss: 0.7079 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 70/300\n",
      "27/27 - 3s - loss: 0.6751 - acc: 0.5679 - f1_score: 0.5521 - val_loss: 0.7074 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 71/300\n",
      "27/27 - 3s - loss: 0.6750 - acc: 0.5695 - f1_score: 0.5532 - val_loss: 0.7077 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 72/300\n",
      "27/27 - 3s - loss: 0.6748 - acc: 0.5696 - f1_score: 0.5532 - val_loss: 0.7080 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 73/300\n",
      "27/27 - 3s - loss: 0.6747 - acc: 0.5702 - f1_score: 0.5533 - val_loss: 0.7086 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00073: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.15      0.25       171\n",
      "           1       0.66      0.96      0.78       297\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.66      0.55      0.51       468\n",
      "weighted avg       0.66      0.66      0.59       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1175\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 16s - loss: 0.6948 - acc: 0.4517 - f1_score: 0.3111 - val_loss: 0.6830 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6942 - acc: 0.4517 - f1_score: 0.3114 - val_loss: 0.6833 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6936 - acc: 0.4515 - f1_score: 0.3122 - val_loss: 0.6838 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6930 - acc: 0.4548 - f1_score: 0.3227 - val_loss: 0.6842 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6925 - acc: 0.4683 - f1_score: 0.3605 - val_loss: 0.6843 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.4911 - f1_score: 0.4173 - val_loss: 0.6847 - val_acc: 0.7003 - val_f1_score: 0.4203\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6914 - acc: 0.5090 - f1_score: 0.4716 - val_loss: 0.6849 - val_acc: 0.6684 - val_f1_score: 0.4875\n",
      "Epoch 8/300\n",
      "28/28 - 4s - loss: 0.6909 - acc: 0.5418 - f1_score: 0.5377 - val_loss: 0.6850 - val_acc: 0.6366 - val_f1_score: 0.5638\n",
      "Epoch 9/300\n",
      "28/28 - 4s - loss: 0.6904 - acc: 0.5612 - f1_score: 0.5592 - val_loss: 0.6850 - val_acc: 0.5836 - val_f1_score: 0.5547\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5642 - f1_score: 0.5558 - val_loss: 0.6850 - val_acc: 0.5623 - val_f1_score: 0.5532\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.5638 - f1_score: 0.5507 - val_loss: 0.6850 - val_acc: 0.5544 - val_f1_score: 0.5509\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.5639 - f1_score: 0.5471 - val_loss: 0.6852 - val_acc: 0.5597 - val_f1_score: 0.5571\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6884 - acc: 0.5641 - f1_score: 0.5440 - val_loss: 0.6853 - val_acc: 0.5544 - val_f1_score: 0.5532\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5643 - f1_score: 0.5427 - val_loss: 0.6854 - val_acc: 0.5597 - val_f1_score: 0.5592\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5656 - f1_score: 0.5420 - val_loss: 0.6855 - val_acc: 0.5544 - val_f1_score: 0.5542\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5668 - f1_score: 0.5420 - val_loss: 0.6858 - val_acc: 0.5544 - val_f1_score: 0.5543\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5686 - f1_score: 0.5425 - val_loss: 0.6859 - val_acc: 0.5517 - val_f1_score: 0.5517\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5701 - f1_score: 0.5426 - val_loss: 0.6862 - val_acc: 0.5464 - val_f1_score: 0.5464\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5721 - f1_score: 0.5434 - val_loss: 0.6862 - val_acc: 0.5438 - val_f1_score: 0.5438\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5733 - f1_score: 0.5444 - val_loss: 0.6867 - val_acc: 0.5438 - val_f1_score: 0.5437\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5741 - f1_score: 0.5437 - val_loss: 0.6872 - val_acc: 0.5411 - val_f1_score: 0.5411\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5736 - f1_score: 0.5416 - val_loss: 0.6874 - val_acc: 0.5411 - val_f1_score: 0.5411\n",
      "Epoch 23/300\n",
      "28/28 - 4s - loss: 0.6843 - acc: 0.5739 - f1_score: 0.5418 - val_loss: 0.6880 - val_acc: 0.5385 - val_f1_score: 0.5384\n",
      "Epoch 24/300\n",
      "28/28 - 4s - loss: 0.6839 - acc: 0.5758 - f1_score: 0.5425 - val_loss: 0.6886 - val_acc: 0.5358 - val_f1_score: 0.5357\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5754 - f1_score: 0.5411 - val_loss: 0.6890 - val_acc: 0.5332 - val_f1_score: 0.5330\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5752 - f1_score: 0.5410 - val_loss: 0.6895 - val_acc: 0.5332 - val_f1_score: 0.5330\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5759 - f1_score: 0.5399 - val_loss: 0.6896 - val_acc: 0.5332 - val_f1_score: 0.5330\n",
      "Epoch 28/300\n",
      "28/28 - 4s - loss: 0.6825 - acc: 0.5764 - f1_score: 0.5400 - val_loss: 0.6902 - val_acc: 0.5252 - val_f1_score: 0.5249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       258\n",
      "           1       0.41      0.36      0.39       119\n",
      "\n",
      "    accuracy                           0.64       377\n",
      "   macro avg       0.57      0.56      0.56       377\n",
      "weighted avg       0.62      0.64      0.63       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1194\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 13s - loss: 0.6932 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7094 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 2/300\n",
      "28/28 - 5s - loss: 0.6926 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7099 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7107 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7114 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6908 - acc: 0.4832 - f1_score: 0.3260 - val_loss: 0.7123 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.4834 - f1_score: 0.3266 - val_loss: 0.7138 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.4831 - f1_score: 0.3270 - val_loss: 0.7143 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.4841 - f1_score: 0.3313 - val_loss: 0.7152 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6888 - acc: 0.4883 - f1_score: 0.3425 - val_loss: 0.7170 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.4918 - f1_score: 0.3517 - val_loss: 0.7185 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.4944 - f1_score: 0.3580 - val_loss: 0.7189 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5009 - f1_score: 0.3741 - val_loss: 0.7197 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5071 - f1_score: 0.3931 - val_loss: 0.7210 - val_acc: 0.2022 - val_f1_score: 0.1697\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5134 - f1_score: 0.4090 - val_loss: 0.7211 - val_acc: 0.2044 - val_f1_score: 0.1742\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5234 - f1_score: 0.4345 - val_loss: 0.7207 - val_acc: 0.2066 - val_f1_score: 0.1813\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5344 - f1_score: 0.4655 - val_loss: 0.7218 - val_acc: 0.2132 - val_f1_score: 0.1924\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5414 - f1_score: 0.4840 - val_loss: 0.7221 - val_acc: 0.2220 - val_f1_score: 0.2058\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5467 - f1_score: 0.5021 - val_loss: 0.7216 - val_acc: 0.2813 - val_f1_score: 0.2793\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5597 - f1_score: 0.5341 - val_loss: 0.7224 - val_acc: 0.3407 - val_f1_score: 0.3397\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5623 - f1_score: 0.5465 - val_loss: 0.7220 - val_acc: 0.4022 - val_f1_score: 0.3913\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5682 - f1_score: 0.5610 - val_loss: 0.7218 - val_acc: 0.4593 - val_f1_score: 0.4264\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5622 - f1_score: 0.5600 - val_loss: 0.7221 - val_acc: 0.4813 - val_f1_score: 0.4344\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5617 - f1_score: 0.5609 - val_loss: 0.7225 - val_acc: 0.5055 - val_f1_score: 0.4487\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5628 - f1_score: 0.5625 - val_loss: 0.7214 - val_acc: 0.5275 - val_f1_score: 0.4645\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5617 - f1_score: 0.5617 - val_loss: 0.7214 - val_acc: 0.5297 - val_f1_score: 0.4604\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5610 - f1_score: 0.5609 - val_loss: 0.7221 - val_acc: 0.5297 - val_f1_score: 0.4585\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5613 - f1_score: 0.5610 - val_loss: 0.7216 - val_acc: 0.5253 - val_f1_score: 0.4472\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5620 - f1_score: 0.5613 - val_loss: 0.7221 - val_acc: 0.5297 - val_f1_score: 0.4502\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5622 - f1_score: 0.5611 - val_loss: 0.7220 - val_acc: 0.5341 - val_f1_score: 0.4510\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5625 - f1_score: 0.5610 - val_loss: 0.7222 - val_acc: 0.5407 - val_f1_score: 0.4509\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5615 - f1_score: 0.5596 - val_loss: 0.7214 - val_acc: 0.5429 - val_f1_score: 0.4499\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5629 - f1_score: 0.5604 - val_loss: 0.7212 - val_acc: 0.5451 - val_f1_score: 0.4490\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5622 - f1_score: 0.5591 - val_loss: 0.7192 - val_acc: 0.5451 - val_f1_score: 0.4465\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5613 - f1_score: 0.5575 - val_loss: 0.7189 - val_acc: 0.5473 - val_f1_score: 0.4479\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5616 - f1_score: 0.5573 - val_loss: 0.7170 - val_acc: 0.5560 - val_f1_score: 0.4536\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5625 - f1_score: 0.5573 - val_loss: 0.7158 - val_acc: 0.5538 - val_f1_score: 0.4496\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5617 - f1_score: 0.5561 - val_loss: 0.7150 - val_acc: 0.5538 - val_f1_score: 0.4470\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5612 - f1_score: 0.5552 - val_loss: 0.7154 - val_acc: 0.5538 - val_f1_score: 0.4470\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5609 - f1_score: 0.5548 - val_loss: 0.7158 - val_acc: 0.5538 - val_f1_score: 0.4470\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5610 - f1_score: 0.5548 - val_loss: 0.7163 - val_acc: 0.5538 - val_f1_score: 0.4470\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5616 - f1_score: 0.5555 - val_loss: 0.7160 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5622 - f1_score: 0.5558 - val_loss: 0.7145 - val_acc: 0.5604 - val_f1_score: 0.4512\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5628 - f1_score: 0.5557 - val_loss: 0.7145 - val_acc: 0.5604 - val_f1_score: 0.4512\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5633 - f1_score: 0.5558 - val_loss: 0.7137 - val_acc: 0.5626 - val_f1_score: 0.4526\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00044: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.20      0.28       208\n",
      "           1       0.54      0.80      0.65       247\n",
      "\n",
      "    accuracy                           0.53       455\n",
      "   macro avg       0.50      0.50      0.46       455\n",
      "weighted avg       0.51      0.53      0.48       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1337\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 13s - loss: 0.6947 - acc: 0.4482 - f1_score: 0.3095 - val_loss: 0.6862 - val_acc: 0.7222 - val_f1_score: 0.4194\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6941 - acc: 0.4484 - f1_score: 0.3100 - val_loss: 0.6866 - val_acc: 0.7179 - val_f1_score: 0.4521\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6935 - acc: 0.4504 - f1_score: 0.3161 - val_loss: 0.6869 - val_acc: 0.7094 - val_f1_score: 0.5578\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6929 - acc: 0.4646 - f1_score: 0.3494 - val_loss: 0.6871 - val_acc: 0.7201 - val_f1_score: 0.6536\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4896 - f1_score: 0.4063 - val_loss: 0.6872 - val_acc: 0.6560 - val_f1_score: 0.6313\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.5130 - f1_score: 0.4725 - val_loss: 0.6873 - val_acc: 0.6218 - val_f1_score: 0.6101\n",
      "Epoch 7/300\n",
      "27/27 - 4s - loss: 0.6913 - acc: 0.5347 - f1_score: 0.5283 - val_loss: 0.6873 - val_acc: 0.5940 - val_f1_score: 0.5885\n",
      "Epoch 8/300\n",
      "27/27 - 4s - loss: 0.6908 - acc: 0.5585 - f1_score: 0.5547 - val_loss: 0.6872 - val_acc: 0.5791 - val_f1_score: 0.5751\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6903 - acc: 0.5645 - f1_score: 0.5501 - val_loss: 0.6870 - val_acc: 0.5684 - val_f1_score: 0.5655\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.5661 - f1_score: 0.5455 - val_loss: 0.6868 - val_acc: 0.5598 - val_f1_score: 0.5575\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6893 - acc: 0.5664 - f1_score: 0.5413 - val_loss: 0.6866 - val_acc: 0.5406 - val_f1_score: 0.5393\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6889 - acc: 0.5664 - f1_score: 0.5376 - val_loss: 0.6864 - val_acc: 0.5427 - val_f1_score: 0.5417\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6884 - acc: 0.5692 - f1_score: 0.5382 - val_loss: 0.6862 - val_acc: 0.5406 - val_f1_score: 0.5397\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5718 - f1_score: 0.5385 - val_loss: 0.6860 - val_acc: 0.5385 - val_f1_score: 0.5376\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5719 - f1_score: 0.5379 - val_loss: 0.6858 - val_acc: 0.5385 - val_f1_score: 0.5376\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5716 - f1_score: 0.5356 - val_loss: 0.6856 - val_acc: 0.5363 - val_f1_score: 0.5356\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5724 - f1_score: 0.5364 - val_loss: 0.6855 - val_acc: 0.5385 - val_f1_score: 0.5378\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5724 - f1_score: 0.5344 - val_loss: 0.6853 - val_acc: 0.5406 - val_f1_score: 0.5400\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5713 - f1_score: 0.5334 - val_loss: 0.6852 - val_acc: 0.5406 - val_f1_score: 0.5400\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5719 - f1_score: 0.5328 - val_loss: 0.6851 - val_acc: 0.5385 - val_f1_score: 0.5379\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5722 - f1_score: 0.5326 - val_loss: 0.6850 - val_acc: 0.5363 - val_f1_score: 0.5358\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5729 - f1_score: 0.5327 - val_loss: 0.6849 - val_acc: 0.5363 - val_f1_score: 0.5358\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5748 - f1_score: 0.5339 - val_loss: 0.6848 - val_acc: 0.5342 - val_f1_score: 0.5339\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5754 - f1_score: 0.5341 - val_loss: 0.6847 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       335\n",
      "           1       0.51      0.50      0.50       133\n",
      "\n",
      "    accuracy                           0.72       468\n",
      "   macro avg       0.65      0.65      0.65       468\n",
      "weighted avg       0.72      0.72      0.72       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1390\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 13s - loss: 0.6941 - acc: 0.4670 - f1_score: 0.3184 - val_loss: 0.6940 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6935 - acc: 0.4670 - f1_score: 0.3186 - val_loss: 0.6930 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6930 - acc: 0.4678 - f1_score: 0.3217 - val_loss: 0.6921 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.4724 - f1_score: 0.3379 - val_loss: 0.6911 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6920 - acc: 0.4876 - f1_score: 0.3765 - val_loss: 0.6901 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.5079 - f1_score: 0.4263 - val_loss: 0.6891 - val_acc: 0.4444 - val_f1_score: 0.3109\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6910 - acc: 0.5217 - f1_score: 0.4672 - val_loss: 0.6881 - val_acc: 0.4658 - val_f1_score: 0.3706\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6905 - acc: 0.5340 - f1_score: 0.5171 - val_loss: 0.6871 - val_acc: 0.5235 - val_f1_score: 0.4982\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5508 - f1_score: 0.5499 - val_loss: 0.6862 - val_acc: 0.5598 - val_f1_score: 0.5598\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6896 - acc: 0.5602 - f1_score: 0.5594 - val_loss: 0.6852 - val_acc: 0.5812 - val_f1_score: 0.5751\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5616 - f1_score: 0.5574 - val_loss: 0.6843 - val_acc: 0.5876 - val_f1_score: 0.5755\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5602 - f1_score: 0.5534 - val_loss: 0.6834 - val_acc: 0.5983 - val_f1_score: 0.5806\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5593 - f1_score: 0.5502 - val_loss: 0.6825 - val_acc: 0.6047 - val_f1_score: 0.5854\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5596 - f1_score: 0.5487 - val_loss: 0.6817 - val_acc: 0.6068 - val_f1_score: 0.5864\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5587 - f1_score: 0.5465 - val_loss: 0.6809 - val_acc: 0.6111 - val_f1_score: 0.5901\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5595 - f1_score: 0.5456 - val_loss: 0.6801 - val_acc: 0.6175 - val_f1_score: 0.5947\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5577 - f1_score: 0.5431 - val_loss: 0.6793 - val_acc: 0.6197 - val_f1_score: 0.5938\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5577 - f1_score: 0.5416 - val_loss: 0.6786 - val_acc: 0.6197 - val_f1_score: 0.5929\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5579 - f1_score: 0.5414 - val_loss: 0.6779 - val_acc: 0.6175 - val_f1_score: 0.5891\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6859 - acc: 0.5585 - f1_score: 0.5410 - val_loss: 0.6772 - val_acc: 0.6154 - val_f1_score: 0.5863\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5592 - f1_score: 0.5410 - val_loss: 0.6766 - val_acc: 0.6154 - val_f1_score: 0.5843\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5599 - f1_score: 0.5414 - val_loss: 0.6760 - val_acc: 0.6154 - val_f1_score: 0.5843\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6851 - acc: 0.5606 - f1_score: 0.5414 - val_loss: 0.6754 - val_acc: 0.6154 - val_f1_score: 0.5843\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6848 - acc: 0.5611 - f1_score: 0.5412 - val_loss: 0.6748 - val_acc: 0.6175 - val_f1_score: 0.5861\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5614 - f1_score: 0.5413 - val_loss: 0.6742 - val_acc: 0.6197 - val_f1_score: 0.5879\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5624 - f1_score: 0.5416 - val_loss: 0.6737 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5635 - f1_score: 0.5423 - val_loss: 0.6732 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5644 - f1_score: 0.5426 - val_loss: 0.6727 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5664 - f1_score: 0.5435 - val_loss: 0.6722 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5663 - f1_score: 0.5429 - val_loss: 0.6718 - val_acc: 0.6282 - val_f1_score: 0.5950\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5669 - f1_score: 0.5429 - val_loss: 0.6713 - val_acc: 0.6346 - val_f1_score: 0.6004\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5667 - f1_score: 0.5431 - val_loss: 0.6710 - val_acc: 0.6325 - val_f1_score: 0.5975\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5682 - f1_score: 0.5435 - val_loss: 0.6706 - val_acc: 0.6368 - val_f1_score: 0.6011\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5687 - f1_score: 0.5438 - val_loss: 0.6702 - val_acc: 0.6346 - val_f1_score: 0.5981\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5690 - f1_score: 0.5429 - val_loss: 0.6698 - val_acc: 0.6368 - val_f1_score: 0.5999\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5689 - f1_score: 0.5432 - val_loss: 0.6694 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5686 - f1_score: 0.5421 - val_loss: 0.6691 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5689 - f1_score: 0.5425 - val_loss: 0.6688 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5683 - f1_score: 0.5412 - val_loss: 0.6684 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5686 - f1_score: 0.5415 - val_loss: 0.6681 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5671 - f1_score: 0.5388 - val_loss: 0.6677 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5686 - f1_score: 0.5406 - val_loss: 0.6674 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5684 - f1_score: 0.5400 - val_loss: 0.6671 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5682 - f1_score: 0.5395 - val_loss: 0.6668 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5692 - f1_score: 0.5403 - val_loss: 0.6666 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5693 - f1_score: 0.5402 - val_loss: 0.6663 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5699 - f1_score: 0.5406 - val_loss: 0.6661 - val_acc: 0.6303 - val_f1_score: 0.5886\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5702 - f1_score: 0.5403 - val_loss: 0.6658 - val_acc: 0.6261 - val_f1_score: 0.5826\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5711 - f1_score: 0.5409 - val_loss: 0.6656 - val_acc: 0.6239 - val_f1_score: 0.5796\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5712 - f1_score: 0.5414 - val_loss: 0.6653 - val_acc: 0.6239 - val_f1_score: 0.5783\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5711 - f1_score: 0.5404 - val_loss: 0.6651 - val_acc: 0.6261 - val_f1_score: 0.5800\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5721 - f1_score: 0.5406 - val_loss: 0.6648 - val_acc: 0.6261 - val_f1_score: 0.5800\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5715 - f1_score: 0.5410 - val_loss: 0.6646 - val_acc: 0.6325 - val_f1_score: 0.5852\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00053: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.66      0.48       120\n",
      "           1       0.84      0.63      0.72       348\n",
      "\n",
      "    accuracy                           0.64       468\n",
      "   macro avg       0.61      0.64      0.60       468\n",
      "weighted avg       0.72      0.64      0.66       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1400\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 13s - loss: 0.6929 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7127 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7138 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7150 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7165 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.4878 - f1_score: 0.3291 - val_loss: 0.7180 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6893 - acc: 0.4902 - f1_score: 0.3387 - val_loss: 0.7197 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6886 - acc: 0.4951 - f1_score: 0.3530 - val_loss: 0.7214 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5064 - f1_score: 0.3812 - val_loss: 0.7231 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6874 - acc: 0.5193 - f1_score: 0.4140 - val_loss: 0.7247 - val_acc: 0.1474 - val_f1_score: 0.1364\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5318 - f1_score: 0.4449 - val_loss: 0.7263 - val_acc: 0.1731 - val_f1_score: 0.1684\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5412 - f1_score: 0.4790 - val_loss: 0.7277 - val_acc: 0.2457 - val_f1_score: 0.2454\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5561 - f1_score: 0.5225 - val_loss: 0.7289 - val_acc: 0.3226 - val_f1_score: 0.3138\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5663 - f1_score: 0.5531 - val_loss: 0.7300 - val_acc: 0.3932 - val_f1_score: 0.3629\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5635 - f1_score: 0.5596 - val_loss: 0.7311 - val_acc: 0.4423 - val_f1_score: 0.3917\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5640 - f1_score: 0.5635 - val_loss: 0.7318 - val_acc: 0.4744 - val_f1_score: 0.4048\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5627 - f1_score: 0.5626 - val_loss: 0.7324 - val_acc: 0.5000 - val_f1_score: 0.4198\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5606 - f1_score: 0.5602 - val_loss: 0.7328 - val_acc: 0.5171 - val_f1_score: 0.4288\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5609 - f1_score: 0.5596 - val_loss: 0.7331 - val_acc: 0.5321 - val_f1_score: 0.4385\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5612 - f1_score: 0.5592 - val_loss: 0.7332 - val_acc: 0.5385 - val_f1_score: 0.4378\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5598 - f1_score: 0.5568 - val_loss: 0.7333 - val_acc: 0.5363 - val_f1_score: 0.4340\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5612 - f1_score: 0.5578 - val_loss: 0.7334 - val_acc: 0.5470 - val_f1_score: 0.4407\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5603 - f1_score: 0.5565 - val_loss: 0.7331 - val_acc: 0.5556 - val_f1_score: 0.4435\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5622 - f1_score: 0.5577 - val_loss: 0.7330 - val_acc: 0.5577 - val_f1_score: 0.4421\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5624 - f1_score: 0.5573 - val_loss: 0.7326 - val_acc: 0.5641 - val_f1_score: 0.4433\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5611 - f1_score: 0.5555 - val_loss: 0.7319 - val_acc: 0.5684 - val_f1_score: 0.4459\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5611 - f1_score: 0.5551 - val_loss: 0.7314 - val_acc: 0.5769 - val_f1_score: 0.4512\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5611 - f1_score: 0.5546 - val_loss: 0.7308 - val_acc: 0.5833 - val_f1_score: 0.4551\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5606 - f1_score: 0.5537 - val_loss: 0.7302 - val_acc: 0.5812 - val_f1_score: 0.4478\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5612 - f1_score: 0.5537 - val_loss: 0.7299 - val_acc: 0.5897 - val_f1_score: 0.4530\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5605 - f1_score: 0.5527 - val_loss: 0.7294 - val_acc: 0.5962 - val_f1_score: 0.4569\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5590 - f1_score: 0.5506 - val_loss: 0.7290 - val_acc: 0.5940 - val_f1_score: 0.4524\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5595 - f1_score: 0.5511 - val_loss: 0.7281 - val_acc: 0.5962 - val_f1_score: 0.4537\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5592 - f1_score: 0.5500 - val_loss: 0.7275 - val_acc: 0.5962 - val_f1_score: 0.4537\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5592 - f1_score: 0.5498 - val_loss: 0.7268 - val_acc: 0.6004 - val_f1_score: 0.4562\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5590 - f1_score: 0.5489 - val_loss: 0.7266 - val_acc: 0.6004 - val_f1_score: 0.4562\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5587 - f1_score: 0.5486 - val_loss: 0.7257 - val_acc: 0.6026 - val_f1_score: 0.4542\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5585 - f1_score: 0.5476 - val_loss: 0.7252 - val_acc: 0.6068 - val_f1_score: 0.4567\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5587 - f1_score: 0.5478 - val_loss: 0.7243 - val_acc: 0.6090 - val_f1_score: 0.4580\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5590 - f1_score: 0.5475 - val_loss: 0.7239 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6792 - acc: 0.5590 - f1_score: 0.5474 - val_loss: 0.7232 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5598 - f1_score: 0.5474 - val_loss: 0.7230 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5595 - f1_score: 0.5470 - val_loss: 0.7225 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6789 - acc: 0.5605 - f1_score: 0.5478 - val_loss: 0.7220 - val_acc: 0.6090 - val_f1_score: 0.4511\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5619 - f1_score: 0.5490 - val_loss: 0.7216 - val_acc: 0.6090 - val_f1_score: 0.4511\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5618 - f1_score: 0.5488 - val_loss: 0.7209 - val_acc: 0.6090 - val_f1_score: 0.4475\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5619 - f1_score: 0.5488 - val_loss: 0.7199 - val_acc: 0.6111 - val_f1_score: 0.4487\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5619 - f1_score: 0.5483 - val_loss: 0.7192 - val_acc: 0.6154 - val_f1_score: 0.4512\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5629 - f1_score: 0.5489 - val_loss: 0.7188 - val_acc: 0.6175 - val_f1_score: 0.4524\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6782 - acc: 0.5627 - f1_score: 0.5484 - val_loss: 0.7183 - val_acc: 0.6175 - val_f1_score: 0.4524\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5628 - f1_score: 0.5485 - val_loss: 0.7178 - val_acc: 0.6175 - val_f1_score: 0.4524\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5637 - f1_score: 0.5491 - val_loss: 0.7173 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5642 - f1_score: 0.5491 - val_loss: 0.7175 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5634 - f1_score: 0.5486 - val_loss: 0.7165 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5644 - f1_score: 0.5489 - val_loss: 0.7164 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5644 - f1_score: 0.5491 - val_loss: 0.7156 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5650 - f1_score: 0.5490 - val_loss: 0.7158 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5651 - f1_score: 0.5494 - val_loss: 0.7151 - val_acc: 0.6218 - val_f1_score: 0.4548\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5666 - f1_score: 0.5502 - val_loss: 0.7153 - val_acc: 0.6218 - val_f1_score: 0.4548\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.12      0.17       156\n",
      "           1       0.66      0.85      0.74       312\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.48      0.49      0.46       468\n",
      "weighted avg       0.54      0.61      0.55       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1419\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 14s - loss: 0.6938 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.6998 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.6987 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6929 - acc: 0.4757 - f1_score: 0.3226 - val_loss: 0.6974 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6926 - acc: 0.4756 - f1_score: 0.3230 - val_loss: 0.6964 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6922 - acc: 0.4756 - f1_score: 0.3240 - val_loss: 0.6955 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4757 - f1_score: 0.3262 - val_loss: 0.6947 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6916 - acc: 0.4796 - f1_score: 0.3364 - val_loss: 0.6938 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4847 - f1_score: 0.3509 - val_loss: 0.6928 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.4948 - f1_score: 0.3763 - val_loss: 0.6921 - val_acc: 0.3165 - val_f1_score: 0.2427\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5013 - f1_score: 0.3932 - val_loss: 0.6914 - val_acc: 0.3231 - val_f1_score: 0.2534\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5062 - f1_score: 0.4075 - val_loss: 0.6906 - val_acc: 0.3429 - val_f1_score: 0.2883\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.5181 - f1_score: 0.4368 - val_loss: 0.6897 - val_acc: 0.3780 - val_f1_score: 0.3465\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5267 - f1_score: 0.4682 - val_loss: 0.6889 - val_acc: 0.4154 - val_f1_score: 0.4024\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.5360 - f1_score: 0.5038 - val_loss: 0.6883 - val_acc: 0.4440 - val_f1_score: 0.4412\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5481 - f1_score: 0.5289 - val_loss: 0.6876 - val_acc: 0.4923 - val_f1_score: 0.4921\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6890 - acc: 0.5555 - f1_score: 0.5486 - val_loss: 0.6870 - val_acc: 0.5473 - val_f1_score: 0.5439\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5594 - f1_score: 0.5559 - val_loss: 0.6865 - val_acc: 0.5736 - val_f1_score: 0.5668\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5580 - f1_score: 0.5565 - val_loss: 0.6859 - val_acc: 0.5912 - val_f1_score: 0.5785\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5573 - f1_score: 0.5570 - val_loss: 0.6854 - val_acc: 0.6044 - val_f1_score: 0.5901\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5574 - f1_score: 0.5574 - val_loss: 0.6846 - val_acc: 0.6198 - val_f1_score: 0.5997\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6880 - acc: 0.5558 - f1_score: 0.5552 - val_loss: 0.6840 - val_acc: 0.6308 - val_f1_score: 0.6083\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6878 - acc: 0.5547 - f1_score: 0.5531 - val_loss: 0.6835 - val_acc: 0.6396 - val_f1_score: 0.6159\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5544 - f1_score: 0.5522 - val_loss: 0.6830 - val_acc: 0.6418 - val_f1_score: 0.6169\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6874 - acc: 0.5561 - f1_score: 0.5536 - val_loss: 0.6822 - val_acc: 0.6440 - val_f1_score: 0.6159\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5571 - f1_score: 0.5531 - val_loss: 0.6817 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6871 - acc: 0.5580 - f1_score: 0.5538 - val_loss: 0.6812 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5588 - f1_score: 0.5543 - val_loss: 0.6807 - val_acc: 0.6440 - val_f1_score: 0.6139\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5584 - f1_score: 0.5534 - val_loss: 0.6801 - val_acc: 0.6440 - val_f1_score: 0.6128\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5565 - f1_score: 0.5510 - val_loss: 0.6795 - val_acc: 0.6462 - val_f1_score: 0.6136\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5575 - f1_score: 0.5517 - val_loss: 0.6789 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5586 - f1_score: 0.5522 - val_loss: 0.6782 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5573 - f1_score: 0.5503 - val_loss: 0.6776 - val_acc: 0.6527 - val_f1_score: 0.6192\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5578 - f1_score: 0.5504 - val_loss: 0.6769 - val_acc: 0.6484 - val_f1_score: 0.6122\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5586 - f1_score: 0.5498 - val_loss: 0.6762 - val_acc: 0.6484 - val_f1_score: 0.6110\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5561 - f1_score: 0.5467 - val_loss: 0.6754 - val_acc: 0.6527 - val_f1_score: 0.6147\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5574 - f1_score: 0.5466 - val_loss: 0.6750 - val_acc: 0.6527 - val_f1_score: 0.6147\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5571 - f1_score: 0.5465 - val_loss: 0.6746 - val_acc: 0.6549 - val_f1_score: 0.6165\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5570 - f1_score: 0.5456 - val_loss: 0.6742 - val_acc: 0.6549 - val_f1_score: 0.6165\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5565 - f1_score: 0.5452 - val_loss: 0.6737 - val_acc: 0.6593 - val_f1_score: 0.6190\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5568 - f1_score: 0.5445 - val_loss: 0.6733 - val_acc: 0.6593 - val_f1_score: 0.6190\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5574 - f1_score: 0.5453 - val_loss: 0.6728 - val_acc: 0.6615 - val_f1_score: 0.6209\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6847 - acc: 0.5574 - f1_score: 0.5452 - val_loss: 0.6721 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5571 - f1_score: 0.5436 - val_loss: 0.6717 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5570 - f1_score: 0.5435 - val_loss: 0.6713 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5565 - f1_score: 0.5428 - val_loss: 0.6706 - val_acc: 0.6637 - val_f1_score: 0.6215\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5562 - f1_score: 0.5420 - val_loss: 0.6700 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 47/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5564 - f1_score: 0.5414 - val_loss: 0.6695 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 48/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5578 - f1_score: 0.5423 - val_loss: 0.6692 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 49/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5578 - f1_score: 0.5426 - val_loss: 0.6688 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 50/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5573 - f1_score: 0.5420 - val_loss: 0.6684 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 51/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5578 - f1_score: 0.5423 - val_loss: 0.6679 - val_acc: 0.6681 - val_f1_score: 0.6239\n",
      "Epoch 52/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5586 - f1_score: 0.5426 - val_loss: 0.6676 - val_acc: 0.6659 - val_f1_score: 0.6221\n",
      "Epoch 53/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5583 - f1_score: 0.5428 - val_loss: 0.6669 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 54/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5597 - f1_score: 0.5433 - val_loss: 0.6662 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 55/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5603 - f1_score: 0.5430 - val_loss: 0.6660 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 56/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5600 - f1_score: 0.5430 - val_loss: 0.6655 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 57/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5599 - f1_score: 0.5423 - val_loss: 0.6653 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 58/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5597 - f1_score: 0.5425 - val_loss: 0.6648 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 59/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5600 - f1_score: 0.5423 - val_loss: 0.6643 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 60/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5604 - f1_score: 0.5418 - val_loss: 0.6640 - val_acc: 0.6791 - val_f1_score: 0.6332\n",
      "Epoch 61/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5602 - f1_score: 0.5412 - val_loss: 0.6634 - val_acc: 0.6835 - val_f1_score: 0.6369\n",
      "Epoch 62/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5602 - f1_score: 0.5405 - val_loss: 0.6629 - val_acc: 0.6923 - val_f1_score: 0.6444\n",
      "Epoch 63/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5613 - f1_score: 0.5413 - val_loss: 0.6626 - val_acc: 0.6857 - val_f1_score: 0.6388\n",
      "Epoch 64/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5609 - f1_score: 0.5416 - val_loss: 0.6621 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 65/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5620 - f1_score: 0.5411 - val_loss: 0.6619 - val_acc: 0.6901 - val_f1_score: 0.6425\n",
      "Epoch 66/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5617 - f1_score: 0.5418 - val_loss: 0.6615 - val_acc: 0.6879 - val_f1_score: 0.6407\n",
      "Epoch 67/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5617 - f1_score: 0.5417 - val_loss: 0.6612 - val_acc: 0.6857 - val_f1_score: 0.6388\n",
      "Epoch 68/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5625 - f1_score: 0.5428 - val_loss: 0.6608 - val_acc: 0.6901 - val_f1_score: 0.6425\n",
      "Epoch 69/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5623 - f1_score: 0.5422 - val_loss: 0.6601 - val_acc: 0.7011 - val_f1_score: 0.6519\n",
      "Epoch 70/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5620 - f1_score: 0.5408 - val_loss: 0.6596 - val_acc: 0.7055 - val_f1_score: 0.6557\n",
      "Epoch 71/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5632 - f1_score: 0.5409 - val_loss: 0.6593 - val_acc: 0.7033 - val_f1_score: 0.6538\n",
      "Epoch 72/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5626 - f1_score: 0.5404 - val_loss: 0.6586 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 73/300\n",
      "28/28 - 3s - loss: 0.6811 - acc: 0.5635 - f1_score: 0.5405 - val_loss: 0.6581 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 74/300\n",
      "28/28 - 3s - loss: 0.6811 - acc: 0.5645 - f1_score: 0.5399 - val_loss: 0.6580 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 75/300\n",
      "28/28 - 3s - loss: 0.6810 - acc: 0.5639 - f1_score: 0.5408 - val_loss: 0.6579 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 76/300\n",
      "28/28 - 3s - loss: 0.6809 - acc: 0.5642 - f1_score: 0.5411 - val_loss: 0.6576 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 77/300\n",
      "28/28 - 3s - loss: 0.6808 - acc: 0.5643 - f1_score: 0.5406 - val_loss: 0.6570 - val_acc: 0.7077 - val_f1_score: 0.6563\n",
      "Epoch 78/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5656 - f1_score: 0.5408 - val_loss: 0.6568 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 79/300\n",
      "28/28 - 3s - loss: 0.6806 - acc: 0.5654 - f1_score: 0.5407 - val_loss: 0.6567 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 80/300\n",
      "28/28 - 3s - loss: 0.6805 - acc: 0.5651 - f1_score: 0.5409 - val_loss: 0.6564 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 81/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5649 - f1_score: 0.5405 - val_loss: 0.6560 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 82/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5652 - f1_score: 0.5408 - val_loss: 0.6556 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 83/300\n",
      "28/28 - 3s - loss: 0.6803 - acc: 0.5661 - f1_score: 0.5408 - val_loss: 0.6549 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 84/300\n",
      "28/28 - 3s - loss: 0.6802 - acc: 0.5662 - f1_score: 0.5404 - val_loss: 0.6546 - val_acc: 0.7077 - val_f1_score: 0.6520\n",
      "Epoch 85/300\n",
      "28/28 - 3s - loss: 0.6801 - acc: 0.5665 - f1_score: 0.5400 - val_loss: 0.6545 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 86/300\n",
      "28/28 - 3s - loss: 0.6800 - acc: 0.5662 - f1_score: 0.5401 - val_loss: 0.6543 - val_acc: 0.7011 - val_f1_score: 0.6478\n",
      "Epoch 87/300\n",
      "28/28 - 3s - loss: 0.6799 - acc: 0.5658 - f1_score: 0.5409 - val_loss: 0.6542 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 88/300\n",
      "28/28 - 3s - loss: 0.6798 - acc: 0.5652 - f1_score: 0.5408 - val_loss: 0.6540 - val_acc: 0.7033 - val_f1_score: 0.6511\n",
      "Epoch 89/300\n",
      "28/28 - 3s - loss: 0.6797 - acc: 0.5664 - f1_score: 0.5412 - val_loss: 0.6535 - val_acc: 0.7011 - val_f1_score: 0.6464\n",
      "Epoch 90/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5667 - f1_score: 0.5405 - val_loss: 0.6530 - val_acc: 0.7099 - val_f1_score: 0.6539\n",
      "Epoch 91/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5669 - f1_score: 0.5396 - val_loss: 0.6530 - val_acc: 0.7011 - val_f1_score: 0.6464\n",
      "Epoch 92/300\n",
      "28/28 - 3s - loss: 0.6795 - acc: 0.5664 - f1_score: 0.5405 - val_loss: 0.6527 - val_acc: 0.7011 - val_f1_score: 0.6464\n",
      "Epoch 93/300\n",
      "28/28 - 3s - loss: 0.6794 - acc: 0.5659 - f1_score: 0.5397 - val_loss: 0.6527 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 94/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5664 - f1_score: 0.5409 - val_loss: 0.6527 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 95/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5668 - f1_score: 0.5413 - val_loss: 0.6524 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 96/300\n",
      "28/28 - 3s - loss: 0.6792 - acc: 0.5669 - f1_score: 0.5410 - val_loss: 0.6524 - val_acc: 0.7055 - val_f1_score: 0.6544\n",
      "Epoch 97/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5671 - f1_score: 0.5426 - val_loss: 0.6519 - val_acc: 0.7011 - val_f1_score: 0.6464\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00097: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.52       136\n",
      "           1       0.80      0.78      0.79       319\n",
      "\n",
      "    accuracy                           0.71       455\n",
      "   macro avg       0.65      0.66      0.66       455\n",
      "weighted avg       0.71      0.71      0.71       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1517\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 16s - loss: 0.6951 - acc: 0.4426 - f1_score: 0.3068 - val_loss: 0.6748 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 2/300\n",
      "28/28 - 4s - loss: 0.6943 - acc: 0.4426 - f1_score: 0.3079 - val_loss: 0.6776 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6936 - acc: 0.4437 - f1_score: 0.3173 - val_loss: 0.6801 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6928 - acc: 0.4638 - f1_score: 0.3656 - val_loss: 0.6825 - val_acc: 0.9145 - val_f1_score: 0.4777\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4920 - f1_score: 0.4407 - val_loss: 0.6848 - val_acc: 0.7892 - val_f1_score: 0.4411\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6914 - acc: 0.5299 - f1_score: 0.5239 - val_loss: 0.6868 - val_acc: 0.5271 - val_f1_score: 0.3608\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6907 - acc: 0.5647 - f1_score: 0.5613 - val_loss: 0.6888 - val_acc: 0.3590 - val_f1_score: 0.2773\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.5765 - f1_score: 0.5616 - val_loss: 0.6906 - val_acc: 0.3105 - val_f1_score: 0.2535\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6893 - acc: 0.5786 - f1_score: 0.5576 - val_loss: 0.6923 - val_acc: 0.2963 - val_f1_score: 0.2444\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5793 - f1_score: 0.5539 - val_loss: 0.6938 - val_acc: 0.2849 - val_f1_score: 0.2392\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5796 - f1_score: 0.5517 - val_loss: 0.6952 - val_acc: 0.2821 - val_f1_score: 0.2417\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5813 - f1_score: 0.5517 - val_loss: 0.6969 - val_acc: 0.2877 - val_f1_score: 0.2497\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5850 - f1_score: 0.5538 - val_loss: 0.6983 - val_acc: 0.2821 - val_f1_score: 0.2458\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5858 - f1_score: 0.5529 - val_loss: 0.6996 - val_acc: 0.2821 - val_f1_score: 0.2458\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5866 - f1_score: 0.5535 - val_loss: 0.7012 - val_acc: 0.2792 - val_f1_score: 0.2457\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5849 - f1_score: 0.5493 - val_loss: 0.7029 - val_acc: 0.2792 - val_f1_score: 0.2457\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5859 - f1_score: 0.5503 - val_loss: 0.7044 - val_acc: 0.2821 - val_f1_score: 0.2496\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5860 - f1_score: 0.5494 - val_loss: 0.7060 - val_acc: 0.2764 - val_f1_score: 0.2455\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5865 - f1_score: 0.5495 - val_loss: 0.7075 - val_acc: 0.2735 - val_f1_score: 0.2434\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5870 - f1_score: 0.5503 - val_loss: 0.7091 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5875 - f1_score: 0.5488 - val_loss: 0.7108 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       351\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       351\n",
      "   macro avg       0.50      0.46      0.48       351\n",
      "weighted avg       1.00      0.93      0.96       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1544\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "29/29 - 15s - loss: 0.6941 - acc: 0.4669 - f1_score: 0.3183 - val_loss: 0.6931 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/300\n",
      "29/29 - 3s - loss: 0.6935 - acc: 0.4669 - f1_score: 0.3183 - val_loss: 0.6909 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 3/300\n",
      "29/29 - 3s - loss: 0.6930 - acc: 0.4672 - f1_score: 0.3215 - val_loss: 0.6886 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 4/300\n",
      "29/29 - 3s - loss: 0.6924 - acc: 0.4724 - f1_score: 0.3384 - val_loss: 0.6863 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 5/300\n",
      "29/29 - 3s - loss: 0.6918 - acc: 0.4886 - f1_score: 0.3803 - val_loss: 0.6839 - val_acc: 0.4135 - val_f1_score: 0.3563\n",
      "Epoch 6/300\n",
      "29/29 - 3s - loss: 0.6913 - acc: 0.5057 - f1_score: 0.4284 - val_loss: 0.6814 - val_acc: 0.5865 - val_f1_score: 0.5847\n",
      "Epoch 7/300\n",
      "29/29 - 3s - loss: 0.6907 - acc: 0.5202 - f1_score: 0.4794 - val_loss: 0.6789 - val_acc: 0.6346 - val_f1_score: 0.6341\n",
      "Epoch 8/300\n",
      "29/29 - 4s - loss: 0.6902 - acc: 0.5416 - f1_score: 0.5323 - val_loss: 0.6766 - val_acc: 0.7692 - val_f1_score: 0.7621\n",
      "Epoch 9/300\n",
      "29/29 - 4s - loss: 0.6897 - acc: 0.5541 - f1_score: 0.5541 - val_loss: 0.6742 - val_acc: 0.7981 - val_f1_score: 0.7895\n",
      "Epoch 10/300\n",
      "29/29 - 3s - loss: 0.6892 - acc: 0.5602 - f1_score: 0.5576 - val_loss: 0.6719 - val_acc: 0.8077 - val_f1_score: 0.7987\n",
      "Epoch 11/300\n",
      "29/29 - 3s - loss: 0.6888 - acc: 0.5577 - f1_score: 0.5520 - val_loss: 0.6697 - val_acc: 0.8077 - val_f1_score: 0.7987\n",
      "Epoch 12/300\n",
      "29/29 - 4s - loss: 0.6883 - acc: 0.5590 - f1_score: 0.5500 - val_loss: 0.6675 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 13/300\n",
      "29/29 - 4s - loss: 0.6879 - acc: 0.5587 - f1_score: 0.5485 - val_loss: 0.6654 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 14/300\n",
      "29/29 - 4s - loss: 0.6875 - acc: 0.5588 - f1_score: 0.5468 - val_loss: 0.6634 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 15/300\n",
      "29/29 - 4s - loss: 0.6871 - acc: 0.5590 - f1_score: 0.5454 - val_loss: 0.6614 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 16/300\n",
      "29/29 - 4s - loss: 0.6868 - acc: 0.5591 - f1_score: 0.5447 - val_loss: 0.6597 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 17/300\n",
      "29/29 - 4s - loss: 0.6864 - acc: 0.5580 - f1_score: 0.5423 - val_loss: 0.6579 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 18/300\n",
      "29/29 - 4s - loss: 0.6861 - acc: 0.5587 - f1_score: 0.5421 - val_loss: 0.6562 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 19/300\n",
      "29/29 - 4s - loss: 0.6857 - acc: 0.5581 - f1_score: 0.5405 - val_loss: 0.6545 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 20/300\n",
      "29/29 - 4s - loss: 0.6855 - acc: 0.5586 - f1_score: 0.5402 - val_loss: 0.6529 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 21/300\n",
      "29/29 - 4s - loss: 0.6852 - acc: 0.5592 - f1_score: 0.5402 - val_loss: 0.6514 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 22/300\n",
      "29/29 - 4s - loss: 0.6849 - acc: 0.5605 - f1_score: 0.5410 - val_loss: 0.6499 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 23/300\n",
      "29/29 - 4s - loss: 0.6846 - acc: 0.5616 - f1_score: 0.5415 - val_loss: 0.6485 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 24/300\n",
      "29/29 - 4s - loss: 0.6843 - acc: 0.5620 - f1_score: 0.5412 - val_loss: 0.6472 - val_acc: 0.7692 - val_f1_score: 0.7538\n",
      "Epoch 25/300\n",
      "29/29 - 4s - loss: 0.6841 - acc: 0.5634 - f1_score: 0.5417 - val_loss: 0.6459 - val_acc: 0.7692 - val_f1_score: 0.7538\n",
      "Epoch 26/300\n",
      "29/29 - 4s - loss: 0.6838 - acc: 0.5636 - f1_score: 0.5416 - val_loss: 0.6446 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 27/300\n",
      "29/29 - 4s - loss: 0.6836 - acc: 0.5652 - f1_score: 0.5424 - val_loss: 0.6433 - val_acc: 0.7885 - val_f1_score: 0.7719\n",
      "Epoch 28/300\n",
      "29/29 - 4s - loss: 0.6834 - acc: 0.5657 - f1_score: 0.5423 - val_loss: 0.6421 - val_acc: 0.7981 - val_f1_score: 0.7811\n",
      "Epoch 29/300\n",
      "29/29 - 4s - loss: 0.6831 - acc: 0.5663 - f1_score: 0.5425 - val_loss: 0.6411 - val_acc: 0.7981 - val_f1_score: 0.7811\n",
      "Epoch 30/300\n",
      "29/29 - 4s - loss: 0.6829 - acc: 0.5687 - f1_score: 0.5439 - val_loss: 0.6399 - val_acc: 0.8077 - val_f1_score: 0.7902\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.76        43\n",
      "           1       0.82      0.87      0.84        61\n",
      "\n",
      "    accuracy                           0.81       104\n",
      "   macro avg       0.81      0.79      0.80       104\n",
      "weighted avg       0.81      0.81      0.81       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1624\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 14s - loss: 0.6946 - acc: 0.4556 - f1_score: 0.3130 - val_loss: 0.6850 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6940 - acc: 0.4556 - f1_score: 0.3134 - val_loss: 0.6854 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4561 - f1_score: 0.3166 - val_loss: 0.6857 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4600 - f1_score: 0.3309 - val_loss: 0.6860 - val_acc: 0.6695 - val_f1_score: 0.4092\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4815 - f1_score: 0.3831 - val_loss: 0.6863 - val_acc: 0.6524 - val_f1_score: 0.4025\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5028 - f1_score: 0.4406 - val_loss: 0.6864 - val_acc: 0.6211 - val_f1_score: 0.4775\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6910 - acc: 0.5272 - f1_score: 0.5022 - val_loss: 0.6866 - val_acc: 0.5299 - val_f1_score: 0.4827\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6904 - acc: 0.5560 - f1_score: 0.5558 - val_loss: 0.6867 - val_acc: 0.4701 - val_f1_score: 0.4596\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5651 - f1_score: 0.5616 - val_loss: 0.6869 - val_acc: 0.4672 - val_f1_score: 0.4651\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6893 - acc: 0.5678 - f1_score: 0.5588 - val_loss: 0.6869 - val_acc: 0.4672 - val_f1_score: 0.4672\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6888 - acc: 0.5671 - f1_score: 0.5550 - val_loss: 0.6869 - val_acc: 0.4758 - val_f1_score: 0.4756\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5668 - f1_score: 0.5523 - val_loss: 0.6869 - val_acc: 0.4672 - val_f1_score: 0.4664\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6878 - acc: 0.5671 - f1_score: 0.5506 - val_loss: 0.6870 - val_acc: 0.4644 - val_f1_score: 0.4628\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5672 - f1_score: 0.5490 - val_loss: 0.6870 - val_acc: 0.4615 - val_f1_score: 0.4594\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5670 - f1_score: 0.5474 - val_loss: 0.6872 - val_acc: 0.4615 - val_f1_score: 0.4590\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5688 - f1_score: 0.5479 - val_loss: 0.6874 - val_acc: 0.4587 - val_f1_score: 0.4555\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5685 - f1_score: 0.5471 - val_loss: 0.6875 - val_acc: 0.4587 - val_f1_score: 0.4555\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5695 - f1_score: 0.5475 - val_loss: 0.6878 - val_acc: 0.4644 - val_f1_score: 0.4602\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5719 - f1_score: 0.5485 - val_loss: 0.6879 - val_acc: 0.4672 - val_f1_score: 0.4628\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5711 - f1_score: 0.5475 - val_loss: 0.6882 - val_acc: 0.4672 - val_f1_score: 0.4628\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5732 - f1_score: 0.5479 - val_loss: 0.6885 - val_acc: 0.4615 - val_f1_score: 0.4564\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5739 - f1_score: 0.5482 - val_loss: 0.6887 - val_acc: 0.4615 - val_f1_score: 0.4564\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5761 - f1_score: 0.5497 - val_loss: 0.6890 - val_acc: 0.4558 - val_f1_score: 0.4501\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5764 - f1_score: 0.5496 - val_loss: 0.6894 - val_acc: 0.4587 - val_f1_score: 0.4519\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5772 - f1_score: 0.5495 - val_loss: 0.6898 - val_acc: 0.4587 - val_f1_score: 0.4512\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5779 - f1_score: 0.5501 - val_loss: 0.6902 - val_acc: 0.4587 - val_f1_score: 0.4504\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5768 - f1_score: 0.5477 - val_loss: 0.6906 - val_acc: 0.4558 - val_f1_score: 0.4472\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182E9045D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.64       223\n",
      "           1       0.34      0.31      0.33       128\n",
      "\n",
      "    accuracy                           0.53       351\n",
      "   macro avg       0.48      0.48      0.48       351\n",
      "weighted avg       0.52      0.53      0.53       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1674\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 14s - loss: 0.6930 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7197 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7201 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7206 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7213 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6906 - acc: 0.4857 - f1_score: 0.3281 - val_loss: 0.7219 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.4864 - f1_score: 0.3323 - val_loss: 0.7228 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.4899 - f1_score: 0.3430 - val_loss: 0.7235 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.5003 - f1_score: 0.3691 - val_loss: 0.7243 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5119 - f1_score: 0.3991 - val_loss: 0.7250 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6878 - acc: 0.5228 - f1_score: 0.4283 - val_loss: 0.7255 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6874 - acc: 0.5359 - f1_score: 0.4658 - val_loss: 0.7260 - val_acc: 0.0431 - val_f1_score: 0.0418\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5494 - f1_score: 0.5045 - val_loss: 0.7264 - val_acc: 0.0554 - val_f1_score: 0.0548\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5666 - f1_score: 0.5466 - val_loss: 0.7269 - val_acc: 0.0892 - val_f1_score: 0.0892\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5735 - f1_score: 0.5671 - val_loss: 0.7271 - val_acc: 0.1538 - val_f1_score: 0.1492\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5742 - f1_score: 0.5726 - val_loss: 0.7269 - val_acc: 0.2677 - val_f1_score: 0.2410\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5695 - f1_score: 0.5695 - val_loss: 0.7269 - val_acc: 0.3477 - val_f1_score: 0.2973\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5671 - f1_score: 0.5670 - val_loss: 0.7265 - val_acc: 0.3938 - val_f1_score: 0.3276\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5650 - f1_score: 0.5641 - val_loss: 0.7261 - val_acc: 0.4185 - val_f1_score: 0.3432\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5641 - f1_score: 0.5626 - val_loss: 0.7256 - val_acc: 0.4523 - val_f1_score: 0.3642\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5641 - f1_score: 0.5617 - val_loss: 0.7251 - val_acc: 0.4738 - val_f1_score: 0.3772\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5630 - f1_score: 0.5597 - val_loss: 0.7246 - val_acc: 0.4892 - val_f1_score: 0.3865\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5632 - f1_score: 0.5596 - val_loss: 0.7238 - val_acc: 0.5015 - val_f1_score: 0.3938\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5640 - f1_score: 0.5598 - val_loss: 0.7230 - val_acc: 0.5169 - val_f1_score: 0.4029\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5629 - f1_score: 0.5580 - val_loss: 0.7221 - val_acc: 0.5262 - val_f1_score: 0.4084\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5637 - f1_score: 0.5584 - val_loss: 0.7211 - val_acc: 0.5323 - val_f1_score: 0.4120\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5634 - f1_score: 0.5575 - val_loss: 0.7204 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5639 - f1_score: 0.5576 - val_loss: 0.7197 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5632 - f1_score: 0.5565 - val_loss: 0.7188 - val_acc: 0.5508 - val_f1_score: 0.4229\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5639 - f1_score: 0.5567 - val_loss: 0.7178 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5630 - f1_score: 0.5553 - val_loss: 0.7172 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5641 - f1_score: 0.5561 - val_loss: 0.7165 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5646 - f1_score: 0.5562 - val_loss: 0.7154 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5637 - f1_score: 0.5551 - val_loss: 0.7145 - val_acc: 0.5692 - val_f1_score: 0.4337\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6815 - acc: 0.5637 - f1_score: 0.5547 - val_loss: 0.7130 - val_acc: 0.5785 - val_f1_score: 0.4391\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5626 - f1_score: 0.5529 - val_loss: 0.7118 - val_acc: 0.5846 - val_f1_score: 0.4427\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6812 - acc: 0.5622 - f1_score: 0.5520 - val_loss: 0.7112 - val_acc: 0.5877 - val_f1_score: 0.4445\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6810 - acc: 0.5624 - f1_score: 0.5521 - val_loss: 0.7100 - val_acc: 0.5908 - val_f1_score: 0.4463\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6809 - acc: 0.5624 - f1_score: 0.5517 - val_loss: 0.7093 - val_acc: 0.6000 - val_f1_score: 0.4518\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5627 - f1_score: 0.5514 - val_loss: 0.7089 - val_acc: 0.6000 - val_f1_score: 0.4518\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6806 - acc: 0.5627 - f1_score: 0.5513 - val_loss: 0.7087 - val_acc: 0.6000 - val_f1_score: 0.4518\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5624 - f1_score: 0.5510 - val_loss: 0.7079 - val_acc: 0.6031 - val_f1_score: 0.4536\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6803 - acc: 0.5627 - f1_score: 0.5507 - val_loss: 0.7075 - val_acc: 0.6062 - val_f1_score: 0.4554\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6802 - acc: 0.5619 - f1_score: 0.5498 - val_loss: 0.7073 - val_acc: 0.6092 - val_f1_score: 0.4572\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6800 - acc: 0.5632 - f1_score: 0.5507 - val_loss: 0.7073 - val_acc: 0.6092 - val_f1_score: 0.4572\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6799 - acc: 0.5623 - f1_score: 0.5501 - val_loss: 0.7057 - val_acc: 0.6154 - val_f1_score: 0.4608\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6798 - acc: 0.5639 - f1_score: 0.5510 - val_loss: 0.7051 - val_acc: 0.6154 - val_f1_score: 0.4608\n",
      "Epoch 47/300\n",
      "28/28 - 3s - loss: 0.6797 - acc: 0.5643 - f1_score: 0.5514 - val_loss: 0.7043 - val_acc: 0.6185 - val_f1_score: 0.4627\n",
      "Epoch 48/300\n",
      "28/28 - 3s - loss: 0.6795 - acc: 0.5649 - f1_score: 0.5516 - val_loss: 0.7035 - val_acc: 0.6185 - val_f1_score: 0.4627\n",
      "Epoch 49/300\n",
      "28/28 - 3s - loss: 0.6794 - acc: 0.5649 - f1_score: 0.5515 - val_loss: 0.7029 - val_acc: 0.6215 - val_f1_score: 0.4645\n",
      "Epoch 50/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5649 - f1_score: 0.5514 - val_loss: 0.7022 - val_acc: 0.6215 - val_f1_score: 0.4645\n",
      "Epoch 51/300\n",
      "28/28 - 3s - loss: 0.6792 - acc: 0.5664 - f1_score: 0.5525 - val_loss: 0.7020 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 52/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5656 - f1_score: 0.5519 - val_loss: 0.7013 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 53/300\n",
      "28/28 - 3s - loss: 0.6790 - acc: 0.5664 - f1_score: 0.5524 - val_loss: 0.7005 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 54/300\n",
      "28/28 - 3s - loss: 0.6789 - acc: 0.5673 - f1_score: 0.5532 - val_loss: 0.6989 - val_acc: 0.6277 - val_f1_score: 0.4682\n",
      "Epoch 55/300\n",
      "28/28 - 3s - loss: 0.6788 - acc: 0.5684 - f1_score: 0.5540 - val_loss: 0.6978 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 56/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5684 - f1_score: 0.5535 - val_loss: 0.6975 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 57/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5678 - f1_score: 0.5530 - val_loss: 0.6964 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 58/300\n",
      "28/28 - 3s - loss: 0.6784 - acc: 0.5691 - f1_score: 0.5537 - val_loss: 0.6964 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 59/300\n",
      "28/28 - 3s - loss: 0.6783 - acc: 0.5691 - f1_score: 0.5538 - val_loss: 0.6967 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 60/300\n",
      "28/28 - 3s - loss: 0.6782 - acc: 0.5691 - f1_score: 0.5539 - val_loss: 0.6966 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 61/300\n",
      "28/28 - 3s - loss: 0.6781 - acc: 0.5701 - f1_score: 0.5549 - val_loss: 0.6957 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 62/300\n",
      "28/28 - 3s - loss: 0.6780 - acc: 0.5694 - f1_score: 0.5535 - val_loss: 0.6959 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 63/300\n",
      "28/28 - 3s - loss: 0.6779 - acc: 0.5701 - f1_score: 0.5548 - val_loss: 0.6957 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 64/300\n",
      "28/28 - 3s - loss: 0.6778 - acc: 0.5703 - f1_score: 0.5541 - val_loss: 0.6961 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 65/300\n",
      "28/28 - 3s - loss: 0.6778 - acc: 0.5704 - f1_score: 0.5553 - val_loss: 0.6948 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 66/300\n",
      "28/28 - 3s - loss: 0.6777 - acc: 0.5701 - f1_score: 0.5542 - val_loss: 0.6946 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 67/300\n",
      "28/28 - 4s - loss: 0.6776 - acc: 0.5704 - f1_score: 0.5543 - val_loss: 0.6947 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 68/300\n",
      "28/28 - 4s - loss: 0.6775 - acc: 0.5703 - f1_score: 0.5541 - val_loss: 0.6944 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 69/300\n",
      "28/28 - 3s - loss: 0.6774 - acc: 0.5708 - f1_score: 0.5546 - val_loss: 0.6935 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 70/300\n",
      "28/28 - 3s - loss: 0.6773 - acc: 0.5712 - f1_score: 0.5547 - val_loss: 0.6932 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 71/300\n",
      "28/28 - 3s - loss: 0.6772 - acc: 0.5720 - f1_score: 0.5554 - val_loss: 0.6936 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 72/300\n",
      "28/28 - 3s - loss: 0.6771 - acc: 0.5710 - f1_score: 0.5548 - val_loss: 0.6917 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 73/300\n",
      "28/28 - 3s - loss: 0.6770 - acc: 0.5718 - f1_score: 0.5550 - val_loss: 0.6922 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 74/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5712 - f1_score: 0.5544 - val_loss: 0.6926 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 75/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5721 - f1_score: 0.5557 - val_loss: 0.6916 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 76/300\n",
      "28/28 - 3s - loss: 0.6768 - acc: 0.5718 - f1_score: 0.5549 - val_loss: 0.6919 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 77/300\n",
      "28/28 - 3s - loss: 0.6767 - acc: 0.5722 - f1_score: 0.5558 - val_loss: 0.6910 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 78/300\n",
      "28/28 - 3s - loss: 0.6766 - acc: 0.5714 - f1_score: 0.5544 - val_loss: 0.6912 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 79/300\n",
      "28/28 - 3s - loss: 0.6765 - acc: 0.5718 - f1_score: 0.5548 - val_loss: 0.6921 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 80/300\n",
      "28/28 - 3s - loss: 0.6764 - acc: 0.5724 - f1_score: 0.5557 - val_loss: 0.6918 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 81/300\n",
      "28/28 - 3s - loss: 0.6764 - acc: 0.5725 - f1_score: 0.5557 - val_loss: 0.6920 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 82/300\n",
      "28/28 - 3s - loss: 0.6763 - acc: 0.5722 - f1_score: 0.5554 - val_loss: 0.6912 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 83/300\n",
      "28/28 - 3s - loss: 0.6762 - acc: 0.5727 - f1_score: 0.5556 - val_loss: 0.6913 - val_acc: 0.6400 - val_f1_score: 0.4755\n",
      "Epoch 84/300\n",
      "28/28 - 3s - loss: 0.6761 - acc: 0.5734 - f1_score: 0.5564 - val_loss: 0.6916 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 85/300\n",
      "28/28 - 3s - loss: 0.6760 - acc: 0.5739 - f1_score: 0.5572 - val_loss: 0.6908 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 86/300\n",
      "28/28 - 3s - loss: 0.6760 - acc: 0.5738 - f1_score: 0.5571 - val_loss: 0.6901 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 87/300\n",
      "28/28 - 3s - loss: 0.6759 - acc: 0.5731 - f1_score: 0.5562 - val_loss: 0.6902 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 88/300\n",
      "28/28 - 3s - loss: 0.6758 - acc: 0.5732 - f1_score: 0.5561 - val_loss: 0.6894 - val_acc: 0.6431 - val_f1_score: 0.4774\n",
      "Epoch 89/300\n",
      "28/28 - 3s - loss: 0.6757 - acc: 0.5737 - f1_score: 0.5561 - val_loss: 0.6911 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00089: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182B3A199D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18       129\n",
      "           1       0.63      1.00      0.77       196\n",
      "\n",
      "    accuracy                           0.64       325\n",
      "   macro avg       0.81      0.55      0.48       325\n",
      "weighted avg       0.78      0.64      0.54       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1688\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 17s - loss: 0.6943 - acc: 0.4618 - f1_score: 0.3159 - val_loss: 0.6901 - val_acc: 0.5333 - val_f1_score: 0.3478\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6938 - acc: 0.4620 - f1_score: 0.3162 - val_loss: 0.6893 - val_acc: 0.5333 - val_f1_score: 0.3528\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4614 - f1_score: 0.3178 - val_loss: 0.6883 - val_acc: 0.5513 - val_f1_score: 0.4173\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4644 - f1_score: 0.3277 - val_loss: 0.6873 - val_acc: 0.5949 - val_f1_score: 0.5406\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6922 - acc: 0.4777 - f1_score: 0.3620 - val_loss: 0.6863 - val_acc: 0.6872 - val_f1_score: 0.6748\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6917 - acc: 0.4963 - f1_score: 0.4116 - val_loss: 0.6853 - val_acc: 0.6974 - val_f1_score: 0.6939\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6913 - acc: 0.5110 - f1_score: 0.4656 - val_loss: 0.6843 - val_acc: 0.6821 - val_f1_score: 0.6815\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6908 - acc: 0.5356 - f1_score: 0.5275 - val_loss: 0.6832 - val_acc: 0.6872 - val_f1_score: 0.6871\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6904 - acc: 0.5554 - f1_score: 0.5550 - val_loss: 0.6821 - val_acc: 0.6821 - val_f1_score: 0.6814\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5588 - f1_score: 0.5537 - val_loss: 0.6811 - val_acc: 0.6769 - val_f1_score: 0.6755\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6895 - acc: 0.5572 - f1_score: 0.5470 - val_loss: 0.6798 - val_acc: 0.6744 - val_f1_score: 0.6728\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6891 - acc: 0.5568 - f1_score: 0.5441 - val_loss: 0.6791 - val_acc: 0.6692 - val_f1_score: 0.6659\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5571 - f1_score: 0.5412 - val_loss: 0.6782 - val_acc: 0.6692 - val_f1_score: 0.6655\n",
      "Epoch 14/300\n",
      "28/28 - 4s - loss: 0.6883 - acc: 0.5581 - f1_score: 0.5404 - val_loss: 0.6772 - val_acc: 0.6718 - val_f1_score: 0.6679\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6880 - acc: 0.5575 - f1_score: 0.5383 - val_loss: 0.6760 - val_acc: 0.6718 - val_f1_score: 0.6679\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5581 - f1_score: 0.5379 - val_loss: 0.6752 - val_acc: 0.6718 - val_f1_score: 0.6679\n",
      "Epoch 17/300\n",
      "28/28 - 4s - loss: 0.6873 - acc: 0.5592 - f1_score: 0.5375 - val_loss: 0.6742 - val_acc: 0.6744 - val_f1_score: 0.6704\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5591 - f1_score: 0.5368 - val_loss: 0.6731 - val_acc: 0.6744 - val_f1_score: 0.6704\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5595 - f1_score: 0.5363 - val_loss: 0.6721 - val_acc: 0.6744 - val_f1_score: 0.6704\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5599 - f1_score: 0.5370 - val_loss: 0.6717 - val_acc: 0.6718 - val_f1_score: 0.6672\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5628 - f1_score: 0.5386 - val_loss: 0.6712 - val_acc: 0.6744 - val_f1_score: 0.6687\n",
      "Epoch 22/300\n",
      "28/28 - 4s - loss: 0.6857 - acc: 0.5652 - f1_score: 0.5391 - val_loss: 0.6705 - val_acc: 0.6795 - val_f1_score: 0.6735\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5647 - f1_score: 0.5387 - val_loss: 0.6700 - val_acc: 0.6821 - val_f1_score: 0.6758\n",
      "Epoch 24/300\n",
      "28/28 - 4s - loss: 0.6852 - acc: 0.5652 - f1_score: 0.5378 - val_loss: 0.6693 - val_acc: 0.6795 - val_f1_score: 0.6730\n",
      "Epoch 25/300\n",
      "28/28 - 4s - loss: 0.6849 - acc: 0.5657 - f1_score: 0.5377 - val_loss: 0.6689 - val_acc: 0.6846 - val_f1_score: 0.6777\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5664 - f1_score: 0.5375 - val_loss: 0.6684 - val_acc: 0.6821 - val_f1_score: 0.6749\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182A76783A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.70      0.73       224\n",
      "           1       0.63      0.69      0.66       166\n",
      "\n",
      "    accuracy                           0.70       390\n",
      "   macro avg       0.69      0.70      0.69       390\n",
      "weighted avg       0.70      0.70      0.70       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1717\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 18s - loss: 0.6940 - acc: 0.4663 - f1_score: 0.3180 - val_loss: 0.6961 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4663 - f1_score: 0.3182 - val_loss: 0.6967 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6926 - acc: 0.4662 - f1_score: 0.3200 - val_loss: 0.6973 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.4727 - f1_score: 0.3390 - val_loss: 0.6981 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4897 - f1_score: 0.3831 - val_loss: 0.6989 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5061 - f1_score: 0.4296 - val_loss: 0.6998 - val_acc: 0.4744 - val_f1_score: 0.3653\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5215 - f1_score: 0.4823 - val_loss: 0.7007 - val_acc: 0.5171 - val_f1_score: 0.4585\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5429 - f1_score: 0.5337 - val_loss: 0.7017 - val_acc: 0.5726 - val_f1_score: 0.5645\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5522 - f1_score: 0.5521 - val_loss: 0.7026 - val_acc: 0.5556 - val_f1_score: 0.5547\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6880 - acc: 0.5624 - f1_score: 0.5599 - val_loss: 0.7036 - val_acc: 0.5513 - val_f1_score: 0.5512\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5619 - f1_score: 0.5564 - val_loss: 0.7045 - val_acc: 0.5427 - val_f1_score: 0.5425\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5623 - f1_score: 0.5545 - val_loss: 0.7055 - val_acc: 0.5256 - val_f1_score: 0.5242\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5634 - f1_score: 0.5538 - val_loss: 0.7064 - val_acc: 0.5085 - val_f1_score: 0.5059\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5641 - f1_score: 0.5535 - val_loss: 0.7072 - val_acc: 0.4915 - val_f1_score: 0.4873\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5627 - f1_score: 0.5508 - val_loss: 0.7080 - val_acc: 0.4701 - val_f1_score: 0.4624\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5628 - f1_score: 0.5499 - val_loss: 0.7087 - val_acc: 0.4615 - val_f1_score: 0.4513\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6847 - acc: 0.5634 - f1_score: 0.5497 - val_loss: 0.7094 - val_acc: 0.4658 - val_f1_score: 0.4536\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5631 - f1_score: 0.5482 - val_loss: 0.7100 - val_acc: 0.4744 - val_f1_score: 0.4609\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5634 - f1_score: 0.5477 - val_loss: 0.7106 - val_acc: 0.4701 - val_f1_score: 0.4541\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5630 - f1_score: 0.5466 - val_loss: 0.7111 - val_acc: 0.4658 - val_f1_score: 0.4489\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5641 - f1_score: 0.5467 - val_loss: 0.7116 - val_acc: 0.4786 - val_f1_score: 0.4595\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5641 - f1_score: 0.5463 - val_loss: 0.7120 - val_acc: 0.4701 - val_f1_score: 0.4468\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5648 - f1_score: 0.5459 - val_loss: 0.7125 - val_acc: 0.4615 - val_f1_score: 0.4358\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5656 - f1_score: 0.5465 - val_loss: 0.7129 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5663 - f1_score: 0.5466 - val_loss: 0.7133 - val_acc: 0.4573 - val_f1_score: 0.4230\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5675 - f1_score: 0.5474 - val_loss: 0.7136 - val_acc: 0.4615 - val_f1_score: 0.4263\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5690 - f1_score: 0.5482 - val_loss: 0.7140 - val_acc: 0.4573 - val_f1_score: 0.4204\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6812 - acc: 0.5687 - f1_score: 0.5476 - val_loss: 0.7142 - val_acc: 0.4530 - val_f1_score: 0.4117\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.51      0.62       162\n",
      "           1       0.39      0.71      0.50        72\n",
      "\n",
      "    accuracy                           0.57       234\n",
      "   macro avg       0.60      0.61      0.56       234\n",
      "weighted avg       0.67      0.57      0.59       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220523-222508\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1765\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py:166 update_state  *\n        self.weights_intermediate.assign_add(_weighted_sum(y_true, sample_weight))\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:844 assign_add  **\n        assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:55 assign_add_variable_op\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 2 and 1. Shapes are [2] and [1]. for '{{node AssignAddVariableOp_7}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_7/resource, Sum_6)' with input shapes: [], [1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28384/1036828569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Testing on {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n\u001b[0m\u001b[0;32m     75\u001b[0m                             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_ecg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_eda\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                             callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1131\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py:166 update_state  *\n        self.weights_intermediate.assign_add(_weighted_sum(y_true, sample_weight))\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:844 assign_add  **\n        assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:55 assign_add_variable_op\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    c:\\Users\\Anubhav\\anaconda3\\envs\\acii\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 2 and 1. Shapes are [2] and [1]. for '{{node AssignAddVariableOp_7}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_7/resource, Sum_6)' with input shapes: [], [1].\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['II']\n",
    "attx_st = ['two_three', 'all']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in sub_dict_ecg.keys():\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=20, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_model(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))    \n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecga (Conv1D)       (None, 2560, 32)     2080        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edaa (Conv1D)       (None, 2560, 32)     6176        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecga (Activation)    (None, 2560, 32)     0           conv_stage1_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edaa (Activation)    (None, 2560, 32)     0           conv_stage1_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecgb (Conv1D)       (None, 854, 32)      65568       act_stage1_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edab (Conv1D)       (None, 854, 32)      65568       act_stage1_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecgb (Activation)    (None, 854, 32)      0           conv_stage1_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edab (Activation)    (None, 854, 32)      0           conv_stage1_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_ecg (MaxPooling1D)    (None, 427, 32)      0           act_stage1_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_eda (MaxPooling1D)    (None, 427, 32)      0           act_stage1_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (None, 427, 1, 32)   0           mp_stage1_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (None, 427, 1, 32)   0           mp_stage1_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (None, 427, 2, 32)   0           tf.expand_dims_4[0][0]           \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (None, 427, 32, 2)   0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 427, 32, 2)   6           tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 427, 32, 2)   0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_5 (TFOpL (None, 427, 2, 32)   0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (Attention_la ((None, 427, 1), (No 32          tf.compat.v1.transpose_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 427, 32)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 attention_layer_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None, 427, 32)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecga (Conv1D)       (None, 427, 64)      131136      tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edaa (Conv1D)       (None, 427, 64)      131136      tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecga (Activation)    (None, 427, 64)      0           conv_stage2_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edaa (Activation)    (None, 427, 64)      0           conv_stage2_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecgb (Conv1D)       (None, 143, 64)      131136      act_stage2_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edab (Conv1D)       (None, 143, 64)      131136      act_stage2_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecgb (Activation)    (None, 143, 64)      0           conv_stage2_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edab (Activation)    (None, 143, 64)      0           conv_stage2_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_ecg (MaxPooling1D)    (None, 71, 64)       0           act_stage2_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_eda (MaxPooling1D)    (None, 71, 64)       0           act_stage2_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_6 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_7 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_9 (TFOpLambda)        (None, 71, 2, 64)    0           tf.expand_dims_6[0][0]           \n",
      "                                                                 tf.expand_dims_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_6 (TFOpL (None, 71, 64, 2)    0           tf.concat_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 71, 64, 2)    6           tf.compat.v1.transpose_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_3 (TFOpLambda)       (None, 71, 64, 2)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_7 (TFOpL (None, 71, 2, 64)    0           tf.nn.relu_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_3 (Attention_la ((None, 71, 1), (Non 64          tf.compat.v1.transpose_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_7 (TFOpLambda) (None, 71, 64)       0           mp_stage2_eda[0][0]              \n",
      "                                                                 attention_layer_3[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_6 (TFOpLambda) (None, 71, 64)       0           mp_stage2_ecg[0][0]              \n",
      "                                                                 attention_layer_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_10 (TFOpLambda)       (None, 71, 128)      0           mp_stage2_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_11 (TFOpLambda)       (None, 71, 128)      0           mp_stage2_eda[0][0]              \n",
      "                                                                 tf.math.multiply_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecga (Conv1D)       (None, 71, 128)      278656      tf.concat_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edaa (Conv1D)       (None, 71, 128)      278656      tf.concat_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecga (Activation)    (None, 71, 128)      0           conv_stage3_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edaa (Activation)    (None, 71, 128)      0           conv_stage3_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecgb (Conv1D)       (None, 24, 128)      278656      act_stage3_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edab (Conv1D)       (None, 24, 128)      278656      act_stage3_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecgb (Activation)    (None, 24, 128)      0           conv_stage3_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edab (Activation)    (None, 24, 128)      0           conv_stage3_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_ecg (MaxPooling1D)    (None, 12, 128)      0           act_stage3_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_eda (MaxPooling1D)    (None, 12, 128)      0           act_stage3_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_8 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_9 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_12 (TFOpLambda)       (None, 12, 2, 128)   0           tf.expand_dims_8[0][0]           \n",
      "                                                                 tf.expand_dims_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_8 (TFOpL (None, 12, 128, 2)   0           tf.concat_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 12, 128, 2)   6           tf.compat.v1.transpose_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_4 (TFOpLambda)       (None, 12, 128, 2)   0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_9 (TFOpL (None, 12, 2, 128)   0           tf.nn.relu_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_4 (Attention_la ((None, 12, 1), (Non 128         tf.compat.v1.transpose_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_9 (TFOpLambda) (None, 12, 128)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 attention_layer_4[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_8 (TFOpLambda) (None, 12, 128)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 attention_layer_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_13 (TFOpLambda)       (None, 12, 256)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_14 (TFOpLambda)       (None, 12, 256)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 tf.math.multiply_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecga (Conv1D)       (None, 12, 256)      459008      tf.concat_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edaa (Conv1D)       (None, 12, 256)      459008      tf.concat_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecga (Activation)    (None, 12, 256)      0           conv_stage4_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edaa (Activation)    (None, 12, 256)      0           conv_stage4_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecgb (Conv1D)       (None, 4, 256)       459008      act_stage4_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edab (Conv1D)       (None, 4, 256)       459008      act_stage4_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecgb (Activation)    (None, 4, 256)       0           conv_stage4_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edab (Activation)    (None, 4, 256)       0           conv_stage4_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_ecg (MaxPooling1D)    (None, 2, 256)       0           act_stage4_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_eda (MaxPooling1D)    (None, 2, 256)       0           act_stage4_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 512)          0           mp_stage4_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 512)          0           mp_stage4_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          262656      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          262656      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 512)          262656      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 512)          262656      dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            2050        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,667,508\n",
      "Trainable params: 4,667,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type II, Stage two_three\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-134101\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1818\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 34s - loss: 0.6938 - acc: 0.4727 - f1_score: 0.3210 - val_loss: 0.6989 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6933 - acc: 0.4727 - f1_score: 0.3210 - val_loss: 0.6979 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4728 - f1_score: 0.3215 - val_loss: 0.6969 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4730 - f1_score: 0.3228 - val_loss: 0.6958 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.4766 - f1_score: 0.3354 - val_loss: 0.6948 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6913 - acc: 0.4863 - f1_score: 0.3634 - val_loss: 0.6938 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.5009 - f1_score: 0.3970 - val_loss: 0.6928 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.5195 - f1_score: 0.4424 - val_loss: 0.6918 - val_acc: 0.3718 - val_f1_score: 0.2887\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6899 - acc: 0.5286 - f1_score: 0.4793 - val_loss: 0.6909 - val_acc: 0.3974 - val_f1_score: 0.3503\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6895 - acc: 0.5422 - f1_score: 0.5207 - val_loss: 0.6899 - val_acc: 0.4722 - val_f1_score: 0.4625\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5563 - f1_score: 0.5523 - val_loss: 0.6891 - val_acc: 0.5171 - val_f1_score: 0.5169\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5579 - f1_score: 0.5579 - val_loss: 0.6882 - val_acc: 0.5620 - val_f1_score: 0.5563\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5571 - f1_score: 0.5558 - val_loss: 0.6873 - val_acc: 0.6068 - val_f1_score: 0.5924\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5590 - f1_score: 0.5557 - val_loss: 0.6865 - val_acc: 0.6282 - val_f1_score: 0.6097\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5586 - f1_score: 0.5535 - val_loss: 0.6857 - val_acc: 0.6239 - val_f1_score: 0.6011\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5579 - f1_score: 0.5515 - val_loss: 0.6849 - val_acc: 0.6239 - val_f1_score: 0.5993\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5590 - f1_score: 0.5518 - val_loss: 0.6841 - val_acc: 0.6303 - val_f1_score: 0.6048\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5566 - f1_score: 0.5477 - val_loss: 0.6834 - val_acc: 0.6346 - val_f1_score: 0.6084\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5563 - f1_score: 0.5470 - val_loss: 0.6826 - val_acc: 0.6325 - val_f1_score: 0.6047\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5569 - f1_score: 0.5466 - val_loss: 0.6819 - val_acc: 0.6303 - val_f1_score: 0.6019\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5566 - f1_score: 0.5455 - val_loss: 0.6812 - val_acc: 0.6303 - val_f1_score: 0.5999\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5556 - f1_score: 0.5437 - val_loss: 0.6805 - val_acc: 0.6368 - val_f1_score: 0.6054\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5554 - f1_score: 0.5428 - val_loss: 0.6798 - val_acc: 0.6432 - val_f1_score: 0.6108\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5567 - f1_score: 0.5436 - val_loss: 0.6791 - val_acc: 0.6432 - val_f1_score: 0.6097\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6848 - acc: 0.5567 - f1_score: 0.5428 - val_loss: 0.6785 - val_acc: 0.6432 - val_f1_score: 0.6097\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5558 - f1_score: 0.5410 - val_loss: 0.6778 - val_acc: 0.6453 - val_f1_score: 0.6104\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5564 - f1_score: 0.5413 - val_loss: 0.6772 - val_acc: 0.6474 - val_f1_score: 0.6111\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5566 - f1_score: 0.5407 - val_loss: 0.6766 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5567 - f1_score: 0.5403 - val_loss: 0.6760 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5574 - f1_score: 0.5398 - val_loss: 0.6754 - val_acc: 0.6517 - val_f1_score: 0.6147\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5582 - f1_score: 0.5401 - val_loss: 0.6749 - val_acc: 0.6538 - val_f1_score: 0.6165\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5579 - f1_score: 0.5400 - val_loss: 0.6744 - val_acc: 0.6581 - val_f1_score: 0.6201\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5595 - f1_score: 0.5410 - val_loss: 0.6738 - val_acc: 0.6603 - val_f1_score: 0.6219\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5599 - f1_score: 0.5409 - val_loss: 0.6733 - val_acc: 0.6624 - val_f1_score: 0.6237\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5602 - f1_score: 0.5404 - val_loss: 0.6728 - val_acc: 0.6624 - val_f1_score: 0.6226\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5611 - f1_score: 0.5414 - val_loss: 0.6723 - val_acc: 0.6645 - val_f1_score: 0.6244\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5612 - f1_score: 0.5408 - val_loss: 0.6718 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5618 - f1_score: 0.5410 - val_loss: 0.6713 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5612 - f1_score: 0.5401 - val_loss: 0.6708 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5625 - f1_score: 0.5411 - val_loss: 0.6704 - val_acc: 0.6688 - val_f1_score: 0.6280\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5632 - f1_score: 0.5412 - val_loss: 0.6699 - val_acc: 0.6688 - val_f1_score: 0.6280\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5629 - f1_score: 0.5410 - val_loss: 0.6695 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5629 - f1_score: 0.5407 - val_loss: 0.6690 - val_acc: 0.6709 - val_f1_score: 0.6274\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5658 - f1_score: 0.5427 - val_loss: 0.6686 - val_acc: 0.6709 - val_f1_score: 0.6274\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5645 - f1_score: 0.5418 - val_loss: 0.6682 - val_acc: 0.6752 - val_f1_score: 0.6310\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5650 - f1_score: 0.5420 - val_loss: 0.6678 - val_acc: 0.6774 - val_f1_score: 0.6328\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5666 - f1_score: 0.5426 - val_loss: 0.6674 - val_acc: 0.6774 - val_f1_score: 0.6316\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5666 - f1_score: 0.5426 - val_loss: 0.6670 - val_acc: 0.6774 - val_f1_score: 0.6316\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5669 - f1_score: 0.5426 - val_loss: 0.6666 - val_acc: 0.6795 - val_f1_score: 0.6334\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5671 - f1_score: 0.5425 - val_loss: 0.6662 - val_acc: 0.6774 - val_f1_score: 0.6303\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5671 - f1_score: 0.5426 - val_loss: 0.6659 - val_acc: 0.6795 - val_f1_score: 0.6321\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5673 - f1_score: 0.5417 - val_loss: 0.6655 - val_acc: 0.6795 - val_f1_score: 0.6321\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5663 - f1_score: 0.5412 - val_loss: 0.6652 - val_acc: 0.6795 - val_f1_score: 0.6321\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5674 - f1_score: 0.5415 - val_loss: 0.6649 - val_acc: 0.6795 - val_f1_score: 0.6321\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5676 - f1_score: 0.5424 - val_loss: 0.6645 - val_acc: 0.6816 - val_f1_score: 0.6339\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5666 - f1_score: 0.5407 - val_loss: 0.6642 - val_acc: 0.6880 - val_f1_score: 0.6393\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5679 - f1_score: 0.5417 - val_loss: 0.6639 - val_acc: 0.6880 - val_f1_score: 0.6393\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5679 - f1_score: 0.5409 - val_loss: 0.6636 - val_acc: 0.6880 - val_f1_score: 0.6393\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5676 - f1_score: 0.5410 - val_loss: 0.6633 - val_acc: 0.6902 - val_f1_score: 0.6411\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5683 - f1_score: 0.5418 - val_loss: 0.6629 - val_acc: 0.6923 - val_f1_score: 0.6430\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5679 - f1_score: 0.5402 - val_loss: 0.6627 - val_acc: 0.6923 - val_f1_score: 0.6430\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5673 - f1_score: 0.5400 - val_loss: 0.6624 - val_acc: 0.6923 - val_f1_score: 0.6430\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5677 - f1_score: 0.5401 - val_loss: 0.6621 - val_acc: 0.6923 - val_f1_score: 0.6430\n",
      "Epoch 64/300\n",
      "27/27 - 3s - loss: 0.6789 - acc: 0.5677 - f1_score: 0.5394 - val_loss: 0.6618 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 65/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5674 - f1_score: 0.5399 - val_loss: 0.6615 - val_acc: 0.6923 - val_f1_score: 0.6416\n",
      "Epoch 66/300\n",
      "27/27 - 3s - loss: 0.6787 - acc: 0.5674 - f1_score: 0.5386 - val_loss: 0.6612 - val_acc: 0.6902 - val_f1_score: 0.6385\n",
      "Epoch 67/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5676 - f1_score: 0.5386 - val_loss: 0.6610 - val_acc: 0.6880 - val_f1_score: 0.6353\n",
      "Epoch 68/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5680 - f1_score: 0.5390 - val_loss: 0.6607 - val_acc: 0.6902 - val_f1_score: 0.6371\n",
      "Epoch 69/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5673 - f1_score: 0.5384 - val_loss: 0.6605 - val_acc: 0.6902 - val_f1_score: 0.6357\n",
      "Epoch 70/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5684 - f1_score: 0.5396 - val_loss: 0.6602 - val_acc: 0.6923 - val_f1_score: 0.6375\n",
      "Epoch 71/300\n",
      "27/27 - 3s - loss: 0.6782 - acc: 0.5683 - f1_score: 0.5385 - val_loss: 0.6600 - val_acc: 0.6923 - val_f1_score: 0.6375\n",
      "Epoch 72/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5690 - f1_score: 0.5388 - val_loss: 0.6597 - val_acc: 0.6923 - val_f1_score: 0.6375\n",
      "Epoch 73/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5693 - f1_score: 0.5391 - val_loss: 0.6595 - val_acc: 0.6923 - val_f1_score: 0.6375\n",
      "Epoch 74/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5687 - f1_score: 0.5387 - val_loss: 0.6593 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 75/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5683 - f1_score: 0.5382 - val_loss: 0.6590 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 76/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5687 - f1_score: 0.5379 - val_loss: 0.6589 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 77/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5693 - f1_score: 0.5388 - val_loss: 0.6587 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 78/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5689 - f1_score: 0.5383 - val_loss: 0.6584 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 79/300\n",
      "27/27 - 3s - loss: 0.6774 - acc: 0.5693 - f1_score: 0.5383 - val_loss: 0.6582 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 80/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5705 - f1_score: 0.5392 - val_loss: 0.6581 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00080: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018564B803A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.60      0.51       125\n",
      "           1       0.83      0.73      0.78       343\n",
      "\n",
      "    accuracy                           0.69       468\n",
      "   macro avg       0.64      0.66      0.64       468\n",
      "weighted avg       0.73      0.69      0.70       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1892\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 13s - loss: 0.6957 - acc: 0.4313 - f1_score: 0.3013 - val_loss: 0.6726 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6949 - acc: 0.4308 - f1_score: 0.3026 - val_loss: 0.6761 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6941 - acc: 0.4358 - f1_score: 0.3178 - val_loss: 0.6793 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6933 - acc: 0.4565 - f1_score: 0.3739 - val_loss: 0.6824 - val_acc: 0.9402 - val_f1_score: 0.4846\n",
      "Epoch 5/300\n",
      "27/27 - 4s - loss: 0.6926 - acc: 0.4921 - f1_score: 0.4604 - val_loss: 0.6853 - val_acc: 0.8056 - val_f1_score: 0.4670\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.5295 - f1_score: 0.5294 - val_loss: 0.6880 - val_acc: 0.5235 - val_f1_score: 0.3803\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6911 - acc: 0.5695 - f1_score: 0.5549 - val_loss: 0.6906 - val_acc: 0.3825 - val_f1_score: 0.3086\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6905 - acc: 0.5811 - f1_score: 0.5499 - val_loss: 0.6930 - val_acc: 0.3205 - val_f1_score: 0.2692\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.5860 - f1_score: 0.5473 - val_loss: 0.6952 - val_acc: 0.2949 - val_f1_score: 0.2520\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5889 - f1_score: 0.5448 - val_loss: 0.6974 - val_acc: 0.2799 - val_f1_score: 0.2417\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5928 - f1_score: 0.5451 - val_loss: 0.6994 - val_acc: 0.2607 - val_f1_score: 0.2282\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5948 - f1_score: 0.5454 - val_loss: 0.7014 - val_acc: 0.2500 - val_f1_score: 0.2205\n",
      "Epoch 13/300\n",
      "27/27 - 4s - loss: 0.6873 - acc: 0.5966 - f1_score: 0.5455 - val_loss: 0.7034 - val_acc: 0.2415 - val_f1_score: 0.2143\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5980 - f1_score: 0.5455 - val_loss: 0.7054 - val_acc: 0.2372 - val_f1_score: 0.2112\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5973 - f1_score: 0.5433 - val_loss: 0.7073 - val_acc: 0.2350 - val_f1_score: 0.2096\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5974 - f1_score: 0.5422 - val_loss: 0.7091 - val_acc: 0.2308 - val_f1_score: 0.2064\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5979 - f1_score: 0.5432 - val_loss: 0.7109 - val_acc: 0.2265 - val_f1_score: 0.2032\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5984 - f1_score: 0.5409 - val_loss: 0.7127 - val_acc: 0.2222 - val_f1_score: 0.2000\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5992 - f1_score: 0.5424 - val_loss: 0.7145 - val_acc: 0.2201 - val_f1_score: 0.1984\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5990 - f1_score: 0.5412 - val_loss: 0.7162 - val_acc: 0.2201 - val_f1_score: 0.1984\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5997 - f1_score: 0.5421 - val_loss: 0.7179 - val_acc: 0.2201 - val_f1_score: 0.1984\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       468\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       468\n",
      "   macro avg       0.50      0.49      0.49       468\n",
      "weighted avg       1.00      0.97      0.99       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1929\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 16s - loss: 0.6932 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7087 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7094 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7101 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.4820 - f1_score: 0.3252 - val_loss: 0.7112 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6905 - acc: 0.4821 - f1_score: 0.3268 - val_loss: 0.7124 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.4852 - f1_score: 0.3375 - val_loss: 0.7136 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.4904 - f1_score: 0.3521 - val_loss: 0.7148 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5025 - f1_score: 0.3814 - val_loss: 0.7161 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5150 - f1_score: 0.4149 - val_loss: 0.7173 - val_acc: 0.2308 - val_f1_score: 0.1955\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5296 - f1_score: 0.4517 - val_loss: 0.7186 - val_acc: 0.2671 - val_f1_score: 0.2445\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5376 - f1_score: 0.4837 - val_loss: 0.7197 - val_acc: 0.3205 - val_f1_score: 0.3173\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5466 - f1_score: 0.5209 - val_loss: 0.7208 - val_acc: 0.4209 - val_f1_score: 0.4150\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5561 - f1_score: 0.5493 - val_loss: 0.7217 - val_acc: 0.4829 - val_f1_score: 0.4491\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5571 - f1_score: 0.5562 - val_loss: 0.7226 - val_acc: 0.4850 - val_f1_score: 0.4216\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5631 - f1_score: 0.5631 - val_loss: 0.7234 - val_acc: 0.5000 - val_f1_score: 0.4300\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5602 - f1_score: 0.5595 - val_loss: 0.7241 - val_acc: 0.5085 - val_f1_score: 0.4338\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5599 - f1_score: 0.5585 - val_loss: 0.7245 - val_acc: 0.5043 - val_f1_score: 0.4269\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5638 - f1_score: 0.5613 - val_loss: 0.7250 - val_acc: 0.5064 - val_f1_score: 0.4283\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5644 - f1_score: 0.5611 - val_loss: 0.7254 - val_acc: 0.5064 - val_f1_score: 0.4262\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5634 - f1_score: 0.5592 - val_loss: 0.7257 - val_acc: 0.5107 - val_f1_score: 0.4290\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5653 - f1_score: 0.5604 - val_loss: 0.7258 - val_acc: 0.5150 - val_f1_score: 0.4318\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5641 - f1_score: 0.5584 - val_loss: 0.7260 - val_acc: 0.5150 - val_f1_score: 0.4318\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5644 - f1_score: 0.5581 - val_loss: 0.7260 - val_acc: 0.5150 - val_f1_score: 0.4274\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5642 - f1_score: 0.5575 - val_loss: 0.7257 - val_acc: 0.5192 - val_f1_score: 0.4302\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5657 - f1_score: 0.5582 - val_loss: 0.7255 - val_acc: 0.5214 - val_f1_score: 0.4316\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5645 - f1_score: 0.5561 - val_loss: 0.7254 - val_acc: 0.5214 - val_f1_score: 0.4316\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5642 - f1_score: 0.5558 - val_loss: 0.7249 - val_acc: 0.5235 - val_f1_score: 0.4330\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5641 - f1_score: 0.5548 - val_loss: 0.7246 - val_acc: 0.5214 - val_f1_score: 0.4269\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5637 - f1_score: 0.5539 - val_loss: 0.7243 - val_acc: 0.5235 - val_f1_score: 0.4283\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5632 - f1_score: 0.5526 - val_loss: 0.7243 - val_acc: 0.5256 - val_f1_score: 0.4296\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5629 - f1_score: 0.5517 - val_loss: 0.7242 - val_acc: 0.5278 - val_f1_score: 0.4286\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5624 - f1_score: 0.5511 - val_loss: 0.7236 - val_acc: 0.5278 - val_f1_score: 0.4286\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5619 - f1_score: 0.5499 - val_loss: 0.7233 - val_acc: 0.5278 - val_f1_score: 0.4286\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.22      0.31       248\n",
      "           1       0.47      0.78      0.59       220\n",
      "\n",
      "    accuracy                           0.48       468\n",
      "   macro avg       0.50      0.50      0.45       468\n",
      "weighted avg       0.50      0.48      0.44       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1933\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 14s - loss: 0.6938 - acc: 0.4699 - f1_score: 0.3197 - val_loss: 0.7016 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6931 - acc: 0.4698 - f1_score: 0.3199 - val_loss: 0.7012 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6925 - acc: 0.4706 - f1_score: 0.3237 - val_loss: 0.7009 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4772 - f1_score: 0.3451 - val_loss: 0.7008 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4961 - f1_score: 0.3909 - val_loss: 0.7007 - val_acc: 0.3291 - val_f1_score: 0.2476\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5111 - f1_score: 0.4380 - val_loss: 0.7008 - val_acc: 0.3291 - val_f1_score: 0.2476\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5302 - f1_score: 0.4931 - val_loss: 0.7008 - val_acc: 0.3376 - val_f1_score: 0.2926\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5518 - f1_score: 0.5439 - val_loss: 0.7010 - val_acc: 0.4231 - val_f1_score: 0.4200\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5591 - f1_score: 0.5590 - val_loss: 0.7011 - val_acc: 0.4359 - val_f1_score: 0.4277\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5661 - f1_score: 0.5638 - val_loss: 0.7013 - val_acc: 0.4359 - val_f1_score: 0.4189\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5647 - f1_score: 0.5597 - val_loss: 0.7015 - val_acc: 0.4359 - val_f1_score: 0.4132\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6871 - acc: 0.5659 - f1_score: 0.5591 - val_loss: 0.7017 - val_acc: 0.4444 - val_f1_score: 0.4179\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5665 - f1_score: 0.5580 - val_loss: 0.7019 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5651 - f1_score: 0.5556 - val_loss: 0.7020 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5641 - f1_score: 0.5535 - val_loss: 0.7021 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5644 - f1_score: 0.5528 - val_loss: 0.7021 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5641 - f1_score: 0.5516 - val_loss: 0.7021 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5637 - f1_score: 0.5500 - val_loss: 0.7021 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5642 - f1_score: 0.5495 - val_loss: 0.7020 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5644 - f1_score: 0.5491 - val_loss: 0.7019 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5634 - f1_score: 0.5472 - val_loss: 0.7017 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5647 - f1_score: 0.5481 - val_loss: 0.7014 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5659 - f1_score: 0.5485 - val_loss: 0.7012 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5661 - f1_score: 0.5478 - val_loss: 0.7011 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5666 - f1_score: 0.5480 - val_loss: 0.7008 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5669 - f1_score: 0.5478 - val_loss: 0.7005 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5675 - f1_score: 0.5477 - val_loss: 0.7003 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5679 - f1_score: 0.5480 - val_loss: 0.6999 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5683 - f1_score: 0.5477 - val_loss: 0.6997 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.29      0.36       128\n",
      "           1       0.42      0.61      0.50       106\n",
      "\n",
      "    accuracy                           0.44       234\n",
      "   macro avg       0.45      0.45      0.43       234\n",
      "weighted avg       0.45      0.44      0.42       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-134101\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Type II, Stage two_three\n",
      "Average Accuracy:  0.6458333333333333\n",
      "F1 score for Baseline:  0.5419607188903183\n",
      "F1 score for Stress:  0.4643751447606184\n",
      "Macro F1:  0.5031679318254684\n",
      "Weighted F1:  0.6382142108176341\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type II, Stage all\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-135244\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1818\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 15s - loss: 0.6931 - acc: 0.4727 - f1_score: 0.3210 - val_loss: 0.6983 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6926 - acc: 0.4728 - f1_score: 0.3213 - val_loss: 0.6973 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.4730 - f1_score: 0.3216 - val_loss: 0.6963 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.4731 - f1_score: 0.3236 - val_loss: 0.6953 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.4756 - f1_score: 0.3315 - val_loss: 0.6944 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.4823 - f1_score: 0.3524 - val_loss: 0.6934 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.4946 - f1_score: 0.3816 - val_loss: 0.6925 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5106 - f1_score: 0.4203 - val_loss: 0.6917 - val_acc: 0.3654 - val_f1_score: 0.2728\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6896 - acc: 0.5280 - f1_score: 0.4639 - val_loss: 0.6908 - val_acc: 0.3910 - val_f1_score: 0.3321\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5412 - f1_score: 0.5053 - val_loss: 0.6900 - val_acc: 0.4551 - val_f1_score: 0.4391\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6888 - acc: 0.5611 - f1_score: 0.5495 - val_loss: 0.6892 - val_acc: 0.5043 - val_f1_score: 0.5041\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5631 - f1_score: 0.5614 - val_loss: 0.6884 - val_acc: 0.5449 - val_f1_score: 0.5431\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6882 - acc: 0.5592 - f1_score: 0.5591 - val_loss: 0.6876 - val_acc: 0.5598 - val_f1_score: 0.5530\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5593 - f1_score: 0.5577 - val_loss: 0.6869 - val_acc: 0.5769 - val_f1_score: 0.5661\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5563 - f1_score: 0.5529 - val_loss: 0.6861 - val_acc: 0.6132 - val_f1_score: 0.5973\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5571 - f1_score: 0.5525 - val_loss: 0.6854 - val_acc: 0.6175 - val_f1_score: 0.5981\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6870 - acc: 0.5573 - f1_score: 0.5516 - val_loss: 0.6847 - val_acc: 0.6368 - val_f1_score: 0.6138\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5563 - f1_score: 0.5490 - val_loss: 0.6841 - val_acc: 0.6410 - val_f1_score: 0.6167\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6865 - acc: 0.5563 - f1_score: 0.5485 - val_loss: 0.6834 - val_acc: 0.6432 - val_f1_score: 0.6185\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5564 - f1_score: 0.5477 - val_loss: 0.6828 - val_acc: 0.6389 - val_f1_score: 0.6130\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5561 - f1_score: 0.5467 - val_loss: 0.6822 - val_acc: 0.6410 - val_f1_score: 0.6139\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6859 - acc: 0.5557 - f1_score: 0.5455 - val_loss: 0.6816 - val_acc: 0.6389 - val_f1_score: 0.6111\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5556 - f1_score: 0.5446 - val_loss: 0.6809 - val_acc: 0.6389 - val_f1_score: 0.6102\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5560 - f1_score: 0.5448 - val_loss: 0.6803 - val_acc: 0.6389 - val_f1_score: 0.6092\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5544 - f1_score: 0.5424 - val_loss: 0.6797 - val_acc: 0.6389 - val_f1_score: 0.6082\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6851 - acc: 0.5558 - f1_score: 0.5431 - val_loss: 0.6792 - val_acc: 0.6389 - val_f1_score: 0.6082\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5551 - f1_score: 0.5419 - val_loss: 0.6786 - val_acc: 0.6453 - val_f1_score: 0.6136\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5567 - f1_score: 0.5421 - val_loss: 0.6780 - val_acc: 0.6432 - val_f1_score: 0.6097\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5585 - f1_score: 0.5432 - val_loss: 0.6775 - val_acc: 0.6474 - val_f1_score: 0.6133\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5582 - f1_score: 0.5425 - val_loss: 0.6770 - val_acc: 0.6496 - val_f1_score: 0.6140\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6842 - acc: 0.5586 - f1_score: 0.5426 - val_loss: 0.6765 - val_acc: 0.6496 - val_f1_score: 0.6140\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5589 - f1_score: 0.5429 - val_loss: 0.6760 - val_acc: 0.6496 - val_f1_score: 0.6140\n",
      "Epoch 33/300\n",
      "27/27 - 4s - loss: 0.6838 - acc: 0.5592 - f1_score: 0.5427 - val_loss: 0.6755 - val_acc: 0.6517 - val_f1_score: 0.6158\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5586 - f1_score: 0.5416 - val_loss: 0.6750 - val_acc: 0.6517 - val_f1_score: 0.6158\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5596 - f1_score: 0.5420 - val_loss: 0.6746 - val_acc: 0.6538 - val_f1_score: 0.6176\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5603 - f1_score: 0.5427 - val_loss: 0.6741 - val_acc: 0.6538 - val_f1_score: 0.6176\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5611 - f1_score: 0.5424 - val_loss: 0.6737 - val_acc: 0.6560 - val_f1_score: 0.6195\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5609 - f1_score: 0.5422 - val_loss: 0.6732 - val_acc: 0.6603 - val_f1_score: 0.6231\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5616 - f1_score: 0.5428 - val_loss: 0.6728 - val_acc: 0.6603 - val_f1_score: 0.6208\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5627 - f1_score: 0.5436 - val_loss: 0.6724 - val_acc: 0.6624 - val_f1_score: 0.6226\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6827 - acc: 0.5635 - f1_score: 0.5435 - val_loss: 0.6720 - val_acc: 0.6624 - val_f1_score: 0.6226\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5637 - f1_score: 0.5438 - val_loss: 0.6716 - val_acc: 0.6645 - val_f1_score: 0.6244\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5634 - f1_score: 0.5434 - val_loss: 0.6712 - val_acc: 0.6645 - val_f1_score: 0.6232\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5651 - f1_score: 0.5445 - val_loss: 0.6708 - val_acc: 0.6645 - val_f1_score: 0.6232\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5648 - f1_score: 0.5443 - val_loss: 0.6704 - val_acc: 0.6645 - val_f1_score: 0.6232\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5651 - f1_score: 0.5444 - val_loss: 0.6701 - val_acc: 0.6667 - val_f1_score: 0.6250\n",
      "Epoch 47/300\n",
      "27/27 - 4s - loss: 0.6819 - acc: 0.5648 - f1_score: 0.5438 - val_loss: 0.6697 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5644 - f1_score: 0.5430 - val_loss: 0.6693 - val_acc: 0.6709 - val_f1_score: 0.6286\n",
      "Epoch 49/300\n",
      "27/27 - 4s - loss: 0.6817 - acc: 0.5642 - f1_score: 0.5425 - val_loss: 0.6690 - val_acc: 0.6709 - val_f1_score: 0.6286\n",
      "Epoch 50/300\n",
      "27/27 - 4s - loss: 0.6816 - acc: 0.5637 - f1_score: 0.5414 - val_loss: 0.6687 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 51/300\n",
      "27/27 - 4s - loss: 0.6815 - acc: 0.5644 - f1_score: 0.5424 - val_loss: 0.6683 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 52/300\n",
      "27/27 - 4s - loss: 0.6814 - acc: 0.5642 - f1_score: 0.5411 - val_loss: 0.6680 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 53/300\n",
      "27/27 - 4s - loss: 0.6813 - acc: 0.5638 - f1_score: 0.5411 - val_loss: 0.6677 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 54/300\n",
      "27/27 - 4s - loss: 0.6812 - acc: 0.5647 - f1_score: 0.5413 - val_loss: 0.6674 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 55/300\n",
      "27/27 - 4s - loss: 0.6810 - acc: 0.5644 - f1_score: 0.5415 - val_loss: 0.6671 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 56/300\n",
      "27/27 - 4s - loss: 0.6809 - acc: 0.5644 - f1_score: 0.5410 - val_loss: 0.6668 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 57/300\n",
      "27/27 - 4s - loss: 0.6809 - acc: 0.5645 - f1_score: 0.5411 - val_loss: 0.6665 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 58/300\n",
      "27/27 - 4s - loss: 0.6807 - acc: 0.5656 - f1_score: 0.5415 - val_loss: 0.6663 - val_acc: 0.6731 - val_f1_score: 0.6304\n",
      "Epoch 59/300\n",
      "27/27 - 4s - loss: 0.6806 - acc: 0.5651 - f1_score: 0.5412 - val_loss: 0.6660 - val_acc: 0.6709 - val_f1_score: 0.6274\n",
      "Epoch 60/300\n",
      "27/27 - 4s - loss: 0.6805 - acc: 0.5657 - f1_score: 0.5417 - val_loss: 0.6657 - val_acc: 0.6731 - val_f1_score: 0.6292\n",
      "Epoch 61/300\n",
      "27/27 - 4s - loss: 0.6805 - acc: 0.5654 - f1_score: 0.5409 - val_loss: 0.6655 - val_acc: 0.6752 - val_f1_score: 0.6310\n",
      "Epoch 62/300\n",
      "27/27 - 4s - loss: 0.6803 - acc: 0.5657 - f1_score: 0.5411 - val_loss: 0.6652 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 63/300\n",
      "27/27 - 4s - loss: 0.6802 - acc: 0.5658 - f1_score: 0.5413 - val_loss: 0.6649 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 64/300\n",
      "27/27 - 4s - loss: 0.6802 - acc: 0.5661 - f1_score: 0.5411 - val_loss: 0.6647 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 65/300\n",
      "27/27 - 4s - loss: 0.6801 - acc: 0.5658 - f1_score: 0.5411 - val_loss: 0.6644 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 66/300\n",
      "27/27 - 4s - loss: 0.6800 - acc: 0.5654 - f1_score: 0.5403 - val_loss: 0.6642 - val_acc: 0.6752 - val_f1_score: 0.6297\n",
      "Epoch 67/300\n",
      "27/27 - 4s - loss: 0.6799 - acc: 0.5651 - f1_score: 0.5397 - val_loss: 0.6640 - val_acc: 0.6752 - val_f1_score: 0.6297\n",
      "Epoch 68/300\n",
      "27/27 - 4s - loss: 0.6798 - acc: 0.5648 - f1_score: 0.5394 - val_loss: 0.6637 - val_acc: 0.6752 - val_f1_score: 0.6297\n",
      "Epoch 69/300\n",
      "27/27 - 4s - loss: 0.6797 - acc: 0.5653 - f1_score: 0.5399 - val_loss: 0.6635 - val_acc: 0.6752 - val_f1_score: 0.6297\n",
      "Epoch 70/300\n",
      "27/27 - 4s - loss: 0.6796 - acc: 0.5651 - f1_score: 0.5398 - val_loss: 0.6633 - val_acc: 0.6795 - val_f1_score: 0.6321\n",
      "Epoch 71/300\n",
      "27/27 - 4s - loss: 0.6795 - acc: 0.5653 - f1_score: 0.5395 - val_loss: 0.6630 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 72/300\n",
      "27/27 - 4s - loss: 0.6794 - acc: 0.5651 - f1_score: 0.5391 - val_loss: 0.6629 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 73/300\n",
      "27/27 - 4s - loss: 0.6794 - acc: 0.5656 - f1_score: 0.5394 - val_loss: 0.6627 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 74/300\n",
      "27/27 - 4s - loss: 0.6793 - acc: 0.5654 - f1_score: 0.5394 - val_loss: 0.6625 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 75/300\n",
      "27/27 - 4s - loss: 0.6792 - acc: 0.5651 - f1_score: 0.5392 - val_loss: 0.6622 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 76/300\n",
      "27/27 - 4s - loss: 0.6791 - acc: 0.5653 - f1_score: 0.5385 - val_loss: 0.6621 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 77/300\n",
      "27/27 - 4s - loss: 0.6790 - acc: 0.5658 - f1_score: 0.5392 - val_loss: 0.6619 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 78/300\n",
      "27/27 - 4s - loss: 0.6789 - acc: 0.5657 - f1_score: 0.5393 - val_loss: 0.6617 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 79/300\n",
      "27/27 - 4s - loss: 0.6789 - acc: 0.5654 - f1_score: 0.5385 - val_loss: 0.6616 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 80/300\n",
      "27/27 - 4s - loss: 0.6788 - acc: 0.5666 - f1_score: 0.5394 - val_loss: 0.6614 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 81/300\n",
      "27/27 - 4s - loss: 0.6787 - acc: 0.5666 - f1_score: 0.5393 - val_loss: 0.6613 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 82/300\n",
      "27/27 - 4s - loss: 0.6786 - acc: 0.5670 - f1_score: 0.5395 - val_loss: 0.6612 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 83/300\n",
      "27/27 - 4s - loss: 0.6785 - acc: 0.5661 - f1_score: 0.5387 - val_loss: 0.6610 - val_acc: 0.6880 - val_f1_score: 0.6393\n",
      "Epoch 84/300\n",
      "27/27 - 4s - loss: 0.6785 - acc: 0.5669 - f1_score: 0.5396 - val_loss: 0.6609 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 85/300\n",
      "27/27 - 4s - loss: 0.6784 - acc: 0.5670 - f1_score: 0.5393 - val_loss: 0.6607 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 86/300\n",
      "27/27 - 4s - loss: 0.6783 - acc: 0.5669 - f1_score: 0.5398 - val_loss: 0.6606 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 87/300\n",
      "27/27 - 4s - loss: 0.6782 - acc: 0.5671 - f1_score: 0.5398 - val_loss: 0.6604 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 88/300\n",
      "27/27 - 4s - loss: 0.6781 - acc: 0.5670 - f1_score: 0.5390 - val_loss: 0.6603 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 89/300\n",
      "27/27 - 4s - loss: 0.6781 - acc: 0.5670 - f1_score: 0.5389 - val_loss: 0.6602 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 90/300\n",
      "27/27 - 4s - loss: 0.6780 - acc: 0.5667 - f1_score: 0.5385 - val_loss: 0.6601 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 91/300\n",
      "27/27 - 4s - loss: 0.6779 - acc: 0.5664 - f1_score: 0.5379 - val_loss: 0.6600 - val_acc: 0.6859 - val_f1_score: 0.6362\n",
      "Epoch 92/300\n",
      "27/27 - 4s - loss: 0.6778 - acc: 0.5671 - f1_score: 0.5396 - val_loss: 0.6599 - val_acc: 0.6880 - val_f1_score: 0.6380\n",
      "Epoch 93/300\n",
      "27/27 - 4s - loss: 0.6778 - acc: 0.5674 - f1_score: 0.5383 - val_loss: 0.6598 - val_acc: 0.6880 - val_f1_score: 0.6380\n",
      "Epoch 94/300\n",
      "27/27 - 4s - loss: 0.6777 - acc: 0.5677 - f1_score: 0.5399 - val_loss: 0.6597 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 95/300\n",
      "27/27 - 4s - loss: 0.6776 - acc: 0.5683 - f1_score: 0.5393 - val_loss: 0.6596 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 96/300\n",
      "27/27 - 4s - loss: 0.6775 - acc: 0.5680 - f1_score: 0.5394 - val_loss: 0.6595 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 97/300\n",
      "27/27 - 4s - loss: 0.6775 - acc: 0.5682 - f1_score: 0.5398 - val_loss: 0.6594 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 98/300\n",
      "27/27 - 4s - loss: 0.6774 - acc: 0.5683 - f1_score: 0.5393 - val_loss: 0.6593 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 99/300\n",
      "27/27 - 4s - loss: 0.6773 - acc: 0.5680 - f1_score: 0.5388 - val_loss: 0.6593 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 100/300\n",
      "27/27 - 4s - loss: 0.6772 - acc: 0.5683 - f1_score: 0.5399 - val_loss: 0.6591 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 101/300\n",
      "27/27 - 4s - loss: 0.6771 - acc: 0.5687 - f1_score: 0.5392 - val_loss: 0.6591 - val_acc: 0.6902 - val_f1_score: 0.6398\n",
      "Epoch 102/300\n",
      "27/27 - 4s - loss: 0.6771 - acc: 0.5690 - f1_score: 0.5403 - val_loss: 0.6590 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 103/300\n",
      "27/27 - 4s - loss: 0.6770 - acc: 0.5686 - f1_score: 0.5404 - val_loss: 0.6589 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 104/300\n",
      "27/27 - 4s - loss: 0.6769 - acc: 0.5692 - f1_score: 0.5398 - val_loss: 0.6588 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 105/300\n",
      "27/27 - 4s - loss: 0.6769 - acc: 0.5696 - f1_score: 0.5395 - val_loss: 0.6588 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 106/300\n",
      "27/27 - 4s - loss: 0.6768 - acc: 0.5693 - f1_score: 0.5400 - val_loss: 0.6588 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 107/300\n",
      "27/27 - 4s - loss: 0.6767 - acc: 0.5695 - f1_score: 0.5404 - val_loss: 0.6587 - val_acc: 0.6880 - val_f1_score: 0.6366\n",
      "Epoch 108/300\n",
      "27/27 - 4s - loss: 0.6766 - acc: 0.5690 - f1_score: 0.5397 - val_loss: 0.6586 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 109/300\n",
      "27/27 - 4s - loss: 0.6766 - acc: 0.5690 - f1_score: 0.5399 - val_loss: 0.6585 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 110/300\n",
      "27/27 - 4s - loss: 0.6765 - acc: 0.5698 - f1_score: 0.5400 - val_loss: 0.6585 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 111/300\n",
      "27/27 - 4s - loss: 0.6764 - acc: 0.5696 - f1_score: 0.5402 - val_loss: 0.6584 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 112/300\n",
      "27/27 - 4s - loss: 0.6763 - acc: 0.5692 - f1_score: 0.5390 - val_loss: 0.6584 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 113/300\n",
      "27/27 - 4s - loss: 0.6762 - acc: 0.5689 - f1_score: 0.5387 - val_loss: 0.6584 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Epoch 114/300\n",
      "27/27 - 4s - loss: 0.6762 - acc: 0.5696 - f1_score: 0.5400 - val_loss: 0.6584 - val_acc: 0.6859 - val_f1_score: 0.6335\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00114: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.60      0.51       124\n",
      "           1       0.83      0.72      0.77       344\n",
      "\n",
      "    accuracy                           0.69       468\n",
      "   macro avg       0.64      0.66      0.64       468\n",
      "weighted avg       0.73      0.69      0.70       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1892\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 15s - loss: 0.6951 - acc: 0.4314 - f1_score: 0.3016 - val_loss: 0.6698 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 5s - loss: 0.6943 - acc: 0.4314 - f1_score: 0.3027 - val_loss: 0.6729 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 3/300\n",
      "27/27 - 4s - loss: 0.6936 - acc: 0.4337 - f1_score: 0.3111 - val_loss: 0.6759 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 4/300\n",
      "27/27 - 4s - loss: 0.6929 - acc: 0.4498 - f1_score: 0.3546 - val_loss: 0.6787 - val_acc: 0.9615 - val_f1_score: 0.4902\n",
      "Epoch 5/300\n",
      "27/27 - 4s - loss: 0.6922 - acc: 0.4823 - f1_score: 0.4328 - val_loss: 0.6813 - val_acc: 0.8996 - val_f1_score: 0.5127\n",
      "Epoch 6/300\n",
      "27/27 - 4s - loss: 0.6915 - acc: 0.5237 - f1_score: 0.5180 - val_loss: 0.6837 - val_acc: 0.6731 - val_f1_score: 0.4424\n",
      "Epoch 7/300\n",
      "27/27 - 4s - loss: 0.6909 - acc: 0.5603 - f1_score: 0.5557 - val_loss: 0.6862 - val_acc: 0.4466 - val_f1_score: 0.3441\n",
      "Epoch 8/300\n",
      "27/27 - 4s - loss: 0.6903 - acc: 0.5780 - f1_score: 0.5572 - val_loss: 0.6884 - val_acc: 0.3868 - val_f1_score: 0.3112\n",
      "Epoch 9/300\n",
      "27/27 - 4s - loss: 0.6897 - acc: 0.5819 - f1_score: 0.5531 - val_loss: 0.6906 - val_acc: 0.3376 - val_f1_score: 0.2804\n",
      "Epoch 10/300\n",
      "27/27 - 4s - loss: 0.6891 - acc: 0.5848 - f1_score: 0.5498 - val_loss: 0.6927 - val_acc: 0.3205 - val_f1_score: 0.2692\n",
      "Epoch 11/300\n",
      "27/27 - 4s - loss: 0.6885 - acc: 0.5877 - f1_score: 0.5492 - val_loss: 0.6948 - val_acc: 0.2970 - val_f1_score: 0.2535\n",
      "Epoch 12/300\n",
      "27/27 - 4s - loss: 0.6880 - acc: 0.5874 - f1_score: 0.5465 - val_loss: 0.6967 - val_acc: 0.2821 - val_f1_score: 0.2432\n",
      "Epoch 13/300\n",
      "27/27 - 4s - loss: 0.6875 - acc: 0.5896 - f1_score: 0.5464 - val_loss: 0.6986 - val_acc: 0.2756 - val_f1_score: 0.2388\n",
      "Epoch 14/300\n",
      "27/27 - 4s - loss: 0.6870 - acc: 0.5913 - f1_score: 0.5463 - val_loss: 0.7005 - val_acc: 0.2628 - val_f1_score: 0.2297\n",
      "Epoch 15/300\n",
      "27/27 - 4s - loss: 0.6864 - acc: 0.5905 - f1_score: 0.5436 - val_loss: 0.7022 - val_acc: 0.2585 - val_f1_score: 0.2267\n",
      "Epoch 16/300\n",
      "27/27 - 4s - loss: 0.6860 - acc: 0.5916 - f1_score: 0.5432 - val_loss: 0.7039 - val_acc: 0.2585 - val_f1_score: 0.2267\n",
      "Epoch 17/300\n",
      "27/27 - 4s - loss: 0.6855 - acc: 0.5925 - f1_score: 0.5442 - val_loss: 0.7056 - val_acc: 0.2564 - val_f1_score: 0.2252\n",
      "Epoch 18/300\n",
      "27/27 - 4s - loss: 0.6850 - acc: 0.5929 - f1_score: 0.5420 - val_loss: 0.7072 - val_acc: 0.2543 - val_f1_score: 0.2236\n",
      "Epoch 19/300\n",
      "27/27 - 4s - loss: 0.6845 - acc: 0.5932 - f1_score: 0.5430 - val_loss: 0.7089 - val_acc: 0.2521 - val_f1_score: 0.2221\n",
      "Epoch 20/300\n",
      "27/27 - 4s - loss: 0.6841 - acc: 0.5944 - f1_score: 0.5426 - val_loss: 0.7105 - val_acc: 0.2457 - val_f1_score: 0.2174\n",
      "Epoch 21/300\n",
      "27/27 - 4s - loss: 0.6836 - acc: 0.5947 - f1_score: 0.5428 - val_loss: 0.7121 - val_acc: 0.2436 - val_f1_score: 0.2159\n",
      "Epoch 22/300\n",
      "27/27 - 4s - loss: 0.6832 - acc: 0.5952 - f1_score: 0.5430 - val_loss: 0.7138 - val_acc: 0.2415 - val_f1_score: 0.2143\n",
      "Epoch 23/300\n",
      "27/27 - 4s - loss: 0.6828 - acc: 0.5960 - f1_score: 0.5429 - val_loss: 0.7155 - val_acc: 0.2393 - val_f1_score: 0.2127\n",
      "Epoch 24/300\n",
      "27/27 - 4s - loss: 0.6824 - acc: 0.5963 - f1_score: 0.5435 - val_loss: 0.7173 - val_acc: 0.2372 - val_f1_score: 0.2112\n",
      "Epoch 25/300\n",
      "27/27 - 4s - loss: 0.6820 - acc: 0.5964 - f1_score: 0.5429 - val_loss: 0.7191 - val_acc: 0.2372 - val_f1_score: 0.2112\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       430\n",
      "           1       0.15      0.05      0.08        38\n",
      "\n",
      "    accuracy                           0.90       468\n",
      "   macro avg       0.54      0.51      0.51       468\n",
      "weighted avg       0.86      0.90      0.88       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1929\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 14s - loss: 0.6924 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7092 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7104 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6910 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7115 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.4823 - f1_score: 0.3256 - val_loss: 0.7128 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6897 - acc: 0.4827 - f1_score: 0.3273 - val_loss: 0.7143 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.4853 - f1_score: 0.3357 - val_loss: 0.7157 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.4886 - f1_score: 0.3479 - val_loss: 0.7171 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 8/300\n",
      "27/27 - 4s - loss: 0.6879 - acc: 0.4972 - f1_score: 0.3699 - val_loss: 0.7183 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6874 - acc: 0.5109 - f1_score: 0.4032 - val_loss: 0.7195 - val_acc: 0.2265 - val_f1_score: 0.1879\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5269 - f1_score: 0.4397 - val_loss: 0.7208 - val_acc: 0.2479 - val_f1_score: 0.2190\n",
      "Epoch 11/300\n",
      "27/27 - 4s - loss: 0.6865 - acc: 0.5366 - f1_score: 0.4714 - val_loss: 0.7218 - val_acc: 0.2970 - val_f1_score: 0.2849\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5493 - f1_score: 0.5136 - val_loss: 0.7228 - val_acc: 0.3932 - val_f1_score: 0.3926\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5621 - f1_score: 0.5485 - val_loss: 0.7236 - val_acc: 0.4701 - val_f1_score: 0.4524\n",
      "Epoch 14/300\n",
      "27/27 - 4s - loss: 0.6853 - acc: 0.5656 - f1_score: 0.5621 - val_loss: 0.7244 - val_acc: 0.4850 - val_f1_score: 0.4427\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5632 - f1_score: 0.5630 - val_loss: 0.7251 - val_acc: 0.5000 - val_f1_score: 0.4393\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5598 - f1_score: 0.5597 - val_loss: 0.7258 - val_acc: 0.5043 - val_f1_score: 0.4329\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5621 - f1_score: 0.5614 - val_loss: 0.7262 - val_acc: 0.5043 - val_f1_score: 0.4269\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5609 - f1_score: 0.5592 - val_loss: 0.7267 - val_acc: 0.5043 - val_f1_score: 0.4269\n",
      "Epoch 19/300\n",
      "27/27 - 4s - loss: 0.6838 - acc: 0.5614 - f1_score: 0.5589 - val_loss: 0.7271 - val_acc: 0.5107 - val_f1_score: 0.4311\n",
      "Epoch 20/300\n",
      "27/27 - 4s - loss: 0.6835 - acc: 0.5631 - f1_score: 0.5597 - val_loss: 0.7274 - val_acc: 0.5085 - val_f1_score: 0.4276\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5634 - f1_score: 0.5592 - val_loss: 0.7276 - val_acc: 0.5107 - val_f1_score: 0.4290\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5648 - f1_score: 0.5599 - val_loss: 0.7278 - val_acc: 0.5107 - val_f1_score: 0.4268\n",
      "Epoch 23/300\n",
      "27/27 - 4s - loss: 0.6828 - acc: 0.5647 - f1_score: 0.5591 - val_loss: 0.7279 - val_acc: 0.5128 - val_f1_score: 0.4282\n",
      "Epoch 24/300\n",
      "27/27 - 4s - loss: 0.6825 - acc: 0.5653 - f1_score: 0.5593 - val_loss: 0.7277 - val_acc: 0.5150 - val_f1_score: 0.4296\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5656 - f1_score: 0.5587 - val_loss: 0.7276 - val_acc: 0.5128 - val_f1_score: 0.4260\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5648 - f1_score: 0.5573 - val_loss: 0.7276 - val_acc: 0.5150 - val_f1_score: 0.4274\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5654 - f1_score: 0.5576 - val_loss: 0.7273 - val_acc: 0.5150 - val_f1_score: 0.4251\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5654 - f1_score: 0.5570 - val_loss: 0.7271 - val_acc: 0.5171 - val_f1_score: 0.4265\n",
      "Epoch 29/300\n",
      "27/27 - 4s - loss: 0.6815 - acc: 0.5653 - f1_score: 0.5564 - val_loss: 0.7269 - val_acc: 0.5214 - val_f1_score: 0.4293\n",
      "Epoch 30/300\n",
      "27/27 - 4s - loss: 0.6813 - acc: 0.5640 - f1_score: 0.5543 - val_loss: 0.7269 - val_acc: 0.5214 - val_f1_score: 0.4269\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5642 - f1_score: 0.5542 - val_loss: 0.7270 - val_acc: 0.5192 - val_f1_score: 0.4231\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5634 - f1_score: 0.5531 - val_loss: 0.7265 - val_acc: 0.5214 - val_f1_score: 0.4245\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5635 - f1_score: 0.5524 - val_loss: 0.7263 - val_acc: 0.5235 - val_f1_score: 0.4259\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.24      0.35       280\n",
      "           1       0.42      0.81      0.55       188\n",
      "\n",
      "    accuracy                           0.47       468\n",
      "   macro avg       0.54      0.53      0.45       468\n",
      "weighted avg       0.56      0.47      0.43       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1933\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 14s - loss: 0.6931 - acc: 0.4699 - f1_score: 0.3197 - val_loss: 0.7016 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 4s - loss: 0.6924 - acc: 0.4704 - f1_score: 0.3206 - val_loss: 0.7015 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 3/300\n",
      "28/28 - 4s - loss: 0.6918 - acc: 0.4697 - f1_score: 0.3214 - val_loss: 0.7015 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 4/300\n",
      "28/28 - 4s - loss: 0.6911 - acc: 0.4751 - f1_score: 0.3390 - val_loss: 0.7016 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 5/300\n",
      "28/28 - 4s - loss: 0.6905 - acc: 0.4896 - f1_score: 0.3748 - val_loss: 0.7017 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 6/300\n",
      "28/28 - 4s - loss: 0.6899 - acc: 0.5088 - f1_score: 0.4251 - val_loss: 0.7018 - val_acc: 0.3291 - val_f1_score: 0.2476\n",
      "Epoch 7/300\n",
      "28/28 - 4s - loss: 0.6893 - acc: 0.5341 - f1_score: 0.4864 - val_loss: 0.7021 - val_acc: 0.3547 - val_f1_score: 0.3140\n",
      "Epoch 8/300\n",
      "28/28 - 4s - loss: 0.6888 - acc: 0.5512 - f1_score: 0.5371 - val_loss: 0.7023 - val_acc: 0.4060 - val_f1_score: 0.3967\n",
      "Epoch 9/300\n",
      "28/28 - 4s - loss: 0.6882 - acc: 0.5627 - f1_score: 0.5622 - val_loss: 0.7025 - val_acc: 0.4359 - val_f1_score: 0.4352\n",
      "Epoch 10/300\n",
      "28/28 - 4s - loss: 0.6877 - acc: 0.5620 - f1_score: 0.5613 - val_loss: 0.7028 - val_acc: 0.4316 - val_f1_score: 0.4154\n",
      "Epoch 11/300\n",
      "28/28 - 4s - loss: 0.6873 - acc: 0.5628 - f1_score: 0.5598 - val_loss: 0.7030 - val_acc: 0.4402 - val_f1_score: 0.4187\n",
      "Epoch 12/300\n",
      "28/28 - 4s - loss: 0.6868 - acc: 0.5661 - f1_score: 0.5611 - val_loss: 0.7032 - val_acc: 0.4444 - val_f1_score: 0.4200\n",
      "Epoch 13/300\n",
      "28/28 - 4s - loss: 0.6864 - acc: 0.5656 - f1_score: 0.5585 - val_loss: 0.7033 - val_acc: 0.4530 - val_f1_score: 0.4268\n",
      "Epoch 14/300\n",
      "28/28 - 4s - loss: 0.6860 - acc: 0.5652 - f1_score: 0.5570 - val_loss: 0.7034 - val_acc: 0.4487 - val_f1_score: 0.4212\n",
      "Epoch 15/300\n",
      "28/28 - 4s - loss: 0.6857 - acc: 0.5635 - f1_score: 0.5542 - val_loss: 0.7035 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 16/300\n",
      "28/28 - 4s - loss: 0.6853 - acc: 0.5654 - f1_score: 0.5550 - val_loss: 0.7035 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 17/300\n",
      "28/28 - 4s - loss: 0.6850 - acc: 0.5649 - f1_score: 0.5538 - val_loss: 0.7036 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 18/300\n",
      "28/28 - 4s - loss: 0.6847 - acc: 0.5635 - f1_score: 0.5515 - val_loss: 0.7036 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 19/300\n",
      "28/28 - 4s - loss: 0.6844 - acc: 0.5633 - f1_score: 0.5500 - val_loss: 0.7036 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 20/300\n",
      "28/28 - 4s - loss: 0.6841 - acc: 0.5637 - f1_score: 0.5501 - val_loss: 0.7035 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 21/300\n",
      "28/28 - 4s - loss: 0.6839 - acc: 0.5628 - f1_score: 0.5479 - val_loss: 0.7034 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 22/300\n",
      "28/28 - 4s - loss: 0.6836 - acc: 0.5640 - f1_score: 0.5486 - val_loss: 0.7033 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 23/300\n",
      "28/28 - 4s - loss: 0.6834 - acc: 0.5651 - f1_score: 0.5489 - val_loss: 0.7032 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 24/300\n",
      "28/28 - 4s - loss: 0.6831 - acc: 0.5659 - f1_score: 0.5488 - val_loss: 0.7031 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 25/300\n",
      "28/28 - 4s - loss: 0.6829 - acc: 0.5663 - f1_score: 0.5490 - val_loss: 0.7029 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 26/300\n",
      "28/28 - 4s - loss: 0.6827 - acc: 0.5669 - f1_score: 0.5491 - val_loss: 0.7027 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 27/300\n",
      "28/28 - 4s - loss: 0.6825 - acc: 0.5676 - f1_score: 0.5493 - val_loss: 0.7026 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 28/300\n",
      "28/28 - 4s - loss: 0.6822 - acc: 0.5676 - f1_score: 0.5490 - val_loss: 0.7023 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 29/300\n",
      "28/28 - 4s - loss: 0.6821 - acc: 0.5698 - f1_score: 0.5504 - val_loss: 0.7022 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.32      0.42       148\n",
      "           1       0.35      0.64      0.45        86\n",
      "\n",
      "    accuracy                           0.44       234\n",
      "   macro avg       0.48      0.48      0.44       234\n",
      "weighted avg       0.51      0.44      0.43       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-135244\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Type II, Stage all\n",
      "Average Accuracy:  0.6239316239316239\n",
      "F1 score for Baseline:  0.555526994874659\n",
      "F1 score for Stress:  0.4645490053851992\n",
      "Macro F1:  0.5100380001299292\n",
      "Weighted F1:  0.6106863967729745\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['II']\n",
    "attx_st = ['two_three', 'all']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in ['1818', '1892', '1929', '1933']:\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=20, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_model(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))    \n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecga (Conv1D)       (None, 2560, 32)     2080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edaa (Conv1D)       (None, 2560, 32)     6176        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecga (Activation)    (None, 2560, 32)     0           conv_stage1_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edaa (Activation)    (None, 2560, 32)     0           conv_stage1_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecgb (Conv1D)       (None, 854, 32)      65568       act_stage1_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edab (Conv1D)       (None, 854, 32)      65568       act_stage1_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecgb (Activation)    (None, 854, 32)      0           conv_stage1_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edab (Activation)    (None, 854, 32)      0           conv_stage1_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_ecg (MaxPooling1D)    (None, 427, 32)      0           act_stage1_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_eda (MaxPooling1D)    (None, 427, 32)      0           act_stage1_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 427, 1, 32)   0           mp_stage1_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 427, 1, 32)   0           mp_stage1_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 427, 2, 32)   0           tf.expand_dims[0][0]             \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 427, 32, 2)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 427, 32, 2)   6           tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 427, 32, 2)   0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 427, 2, 32)   0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Attention_laye ((None, 427, 1), (No 32          tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 427, 32)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 attention_layer[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 427, 32)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecga (Conv1D)       (None, 427, 64)      131136      tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edaa (Conv1D)       (None, 427, 64)      131136      tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecga (Activation)    (None, 427, 64)      0           conv_stage2_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edaa (Activation)    (None, 427, 64)      0           conv_stage2_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecgb (Conv1D)       (None, 143, 64)      131136      act_stage2_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edab (Conv1D)       (None, 143, 64)      131136      act_stage2_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecgb (Activation)    (None, 143, 64)      0           conv_stage2_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edab (Activation)    (None, 143, 64)      0           conv_stage2_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_ecg (MaxPooling1D)    (None, 71, 64)       0           act_stage2_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_eda (MaxPooling1D)    (None, 71, 64)       0           act_stage2_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 71, 2, 64)    0           tf.expand_dims_2[0][0]           \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 71, 64, 2)    0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 71, 64, 2)    6           tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 71, 64, 2)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 71, 2, 64)    0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (Attention_la ((None, 71, 1), (Non 64          tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 71, 64)       0           mp_stage2_eda[0][0]              \n",
      "                                                                 attention_layer_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 71, 64)       0           mp_stage2_ecg[0][0]              \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_5 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_eda[0][0]              \n",
      "                                                                 tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecga (Conv1D)       (None, 71, 128)      278656      tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edaa (Conv1D)       (None, 71, 128)      278656      tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecga (Activation)    (None, 71, 128)      0           conv_stage3_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edaa (Activation)    (None, 71, 128)      0           conv_stage3_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecgb (Conv1D)       (None, 24, 128)      278656      act_stage3_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edab (Conv1D)       (None, 24, 128)      278656      act_stage3_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecgb (Activation)    (None, 24, 128)      0           conv_stage3_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edab (Activation)    (None, 24, 128)      0           conv_stage3_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_ecg (MaxPooling1D)    (None, 12, 128)      0           act_stage3_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_eda (MaxPooling1D)    (None, 12, 128)      0           act_stage3_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (None, 12, 2, 128)   0           tf.expand_dims_4[0][0]           \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (None, 12, 128, 2)   0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12, 128, 2)   6           tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 12, 128, 2)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_5 (TFOpL (None, 12, 2, 128)   0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (Attention_la ((None, 12, 1), (Non 128         tf.compat.v1.transpose_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 12, 128)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 attention_layer_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None, 12, 128)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecga (Conv1D)       (None, 12, 256)      459008      tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edaa (Conv1D)       (None, 12, 256)      459008      tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecga (Activation)    (None, 12, 256)      0           conv_stage4_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edaa (Activation)    (None, 12, 256)      0           conv_stage4_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecgb (Conv1D)       (None, 4, 256)       459008      act_stage4_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edab (Conv1D)       (None, 4, 256)       459008      act_stage4_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecgb (Activation)    (None, 4, 256)       0           conv_stage4_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edab (Activation)    (None, 4, 256)       0           conv_stage4_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_ecg (MaxPooling1D)    (None, 2, 256)       0           act_stage4_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_eda (MaxPooling1D)    (None, 2, 256)       0           act_stage4_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           mp_stage4_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           mp_stage4_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 512)          262656      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 512)          262656      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            2050        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,667,508\n",
      "Trainable params: 4,667,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type II, Stage two\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220609-163438\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1105\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 36s - loss: 0.6941 - acc: 0.4689 - f1_score: 0.3192 - val_loss: 0.6932 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6936 - acc: 0.4688 - f1_score: 0.3194 - val_loss: 0.6921 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6931 - acc: 0.4686 - f1_score: 0.3219 - val_loss: 0.6909 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6927 - acc: 0.4744 - f1_score: 0.3400 - val_loss: 0.6897 - val_acc: 0.4188 - val_f1_score: 0.2982\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.4872 - f1_score: 0.3738 - val_loss: 0.6886 - val_acc: 0.4594 - val_f1_score: 0.3908\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.5021 - f1_score: 0.4165 - val_loss: 0.6874 - val_acc: 0.5214 - val_f1_score: 0.5091\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.5172 - f1_score: 0.4645 - val_loss: 0.6862 - val_acc: 0.6239 - val_f1_score: 0.6224\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6910 - acc: 0.5372 - f1_score: 0.5238 - val_loss: 0.6849 - val_acc: 0.6368 - val_f1_score: 0.6241\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6906 - acc: 0.5505 - f1_score: 0.5497 - val_loss: 0.6837 - val_acc: 0.6432 - val_f1_score: 0.6257\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6901 - acc: 0.5560 - f1_score: 0.5549 - val_loss: 0.6825 - val_acc: 0.6560 - val_f1_score: 0.6363\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6897 - acc: 0.5560 - f1_score: 0.5519 - val_loss: 0.6813 - val_acc: 0.6645 - val_f1_score: 0.6430\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6894 - acc: 0.5557 - f1_score: 0.5482 - val_loss: 0.6801 - val_acc: 0.6645 - val_f1_score: 0.6413\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6890 - acc: 0.5580 - f1_score: 0.5488 - val_loss: 0.6790 - val_acc: 0.6645 - val_f1_score: 0.6396\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6886 - acc: 0.5582 - f1_score: 0.5470 - val_loss: 0.6779 - val_acc: 0.6667 - val_f1_score: 0.6406\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5561 - f1_score: 0.5435 - val_loss: 0.6768 - val_acc: 0.6709 - val_f1_score: 0.6434\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5571 - f1_score: 0.5429 - val_loss: 0.6758 - val_acc: 0.6752 - val_f1_score: 0.6471\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5570 - f1_score: 0.5420 - val_loss: 0.6747 - val_acc: 0.6752 - val_f1_score: 0.6462\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5557 - f1_score: 0.5398 - val_loss: 0.6737 - val_acc: 0.6731 - val_f1_score: 0.6434\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6870 - acc: 0.5557 - f1_score: 0.5395 - val_loss: 0.6728 - val_acc: 0.6752 - val_f1_score: 0.6453\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5567 - f1_score: 0.5394 - val_loss: 0.6718 - val_acc: 0.6774 - val_f1_score: 0.6471\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5564 - f1_score: 0.5382 - val_loss: 0.6709 - val_acc: 0.6752 - val_f1_score: 0.6443\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5560 - f1_score: 0.5376 - val_loss: 0.6700 - val_acc: 0.6752 - val_f1_score: 0.6433\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6859 - acc: 0.5574 - f1_score: 0.5385 - val_loss: 0.6691 - val_acc: 0.6752 - val_f1_score: 0.6433\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5598 - f1_score: 0.5397 - val_loss: 0.6682 - val_acc: 0.6752 - val_f1_score: 0.6433\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5593 - f1_score: 0.5392 - val_loss: 0.6674 - val_acc: 0.6752 - val_f1_score: 0.6433\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5600 - f1_score: 0.5397 - val_loss: 0.6666 - val_acc: 0.6731 - val_f1_score: 0.6404\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5608 - f1_score: 0.5395 - val_loss: 0.6659 - val_acc: 0.6731 - val_f1_score: 0.6404\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5608 - f1_score: 0.5391 - val_loss: 0.6651 - val_acc: 0.6752 - val_f1_score: 0.6412\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5614 - f1_score: 0.5393 - val_loss: 0.6644 - val_acc: 0.6774 - val_f1_score: 0.6431\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5621 - f1_score: 0.5394 - val_loss: 0.6637 - val_acc: 0.6816 - val_f1_score: 0.6468\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5627 - f1_score: 0.5399 - val_loss: 0.6630 - val_acc: 0.6816 - val_f1_score: 0.6468\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5629 - f1_score: 0.5399 - val_loss: 0.6624 - val_acc: 0.6838 - val_f1_score: 0.6486\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5631 - f1_score: 0.5394 - val_loss: 0.6618 - val_acc: 0.6838 - val_f1_score: 0.6476\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5635 - f1_score: 0.5395 - val_loss: 0.6612 - val_acc: 0.6838 - val_f1_score: 0.6476\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5635 - f1_score: 0.5388 - val_loss: 0.6606 - val_acc: 0.6838 - val_f1_score: 0.6476\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5638 - f1_score: 0.5389 - val_loss: 0.6601 - val_acc: 0.6859 - val_f1_score: 0.6494\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5641 - f1_score: 0.5388 - val_loss: 0.6595 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5644 - f1_score: 0.5387 - val_loss: 0.6590 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5640 - f1_score: 0.5375 - val_loss: 0.6585 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5647 - f1_score: 0.5382 - val_loss: 0.6580 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5651 - f1_score: 0.5387 - val_loss: 0.6575 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5654 - f1_score: 0.5377 - val_loss: 0.6570 - val_acc: 0.6859 - val_f1_score: 0.6483\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5654 - f1_score: 0.5381 - val_loss: 0.6565 - val_acc: 0.6880 - val_f1_score: 0.6502\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5657 - f1_score: 0.5383 - val_loss: 0.6560 - val_acc: 0.6838 - val_f1_score: 0.6442\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5658 - f1_score: 0.5377 - val_loss: 0.6556 - val_acc: 0.6838 - val_f1_score: 0.6442\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5658 - f1_score: 0.5376 - val_loss: 0.6551 - val_acc: 0.6859 - val_f1_score: 0.6461\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5663 - f1_score: 0.5380 - val_loss: 0.6547 - val_acc: 0.6859 - val_f1_score: 0.6461\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5666 - f1_score: 0.5378 - val_loss: 0.6543 - val_acc: 0.6859 - val_f1_score: 0.6461\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5676 - f1_score: 0.5383 - val_loss: 0.6539 - val_acc: 0.6838 - val_f1_score: 0.6431\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5673 - f1_score: 0.5384 - val_loss: 0.6536 - val_acc: 0.6838 - val_f1_score: 0.6431\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5680 - f1_score: 0.5384 - val_loss: 0.6531 - val_acc: 0.6838 - val_f1_score: 0.6431\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5682 - f1_score: 0.5383 - val_loss: 0.6527 - val_acc: 0.6838 - val_f1_score: 0.6431\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5682 - f1_score: 0.5386 - val_loss: 0.6523 - val_acc: 0.6859 - val_f1_score: 0.6449\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5687 - f1_score: 0.5385 - val_loss: 0.6520 - val_acc: 0.6859 - val_f1_score: 0.6449\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5682 - f1_score: 0.5377 - val_loss: 0.6516 - val_acc: 0.6880 - val_f1_score: 0.6479\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5695 - f1_score: 0.5383 - val_loss: 0.6512 - val_acc: 0.6880 - val_f1_score: 0.6479\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5692 - f1_score: 0.5381 - val_loss: 0.6509 - val_acc: 0.6880 - val_f1_score: 0.6479\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5690 - f1_score: 0.5371 - val_loss: 0.6505 - val_acc: 0.6880 - val_f1_score: 0.6479\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5693 - f1_score: 0.5379 - val_loss: 0.6502 - val_acc: 0.6902 - val_f1_score: 0.6497\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5689 - f1_score: 0.5372 - val_loss: 0.6499 - val_acc: 0.6902 - val_f1_score: 0.6497\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5700 - f1_score: 0.5389 - val_loss: 0.6497 - val_acc: 0.6902 - val_f1_score: 0.6497\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5692 - f1_score: 0.5373 - val_loss: 0.6494 - val_acc: 0.6880 - val_f1_score: 0.6467\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5693 - f1_score: 0.5358 - val_loss: 0.6490 - val_acc: 0.6902 - val_f1_score: 0.6497\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00063: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.71      0.54       119\n",
      "           1       0.87      0.68      0.77       349\n",
      "\n",
      "    accuracy                           0.69       468\n",
      "   macro avg       0.65      0.69      0.65       468\n",
      "weighted avg       0.76      0.69      0.71       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1106\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 11s - loss: 0.6933 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7046 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7046 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.4915 - f1_score: 0.3298 - val_loss: 0.7049 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6916 - acc: 0.4918 - f1_score: 0.3304 - val_loss: 0.7053 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6910 - acc: 0.4917 - f1_score: 0.3306 - val_loss: 0.7057 - val_acc: 0.0897 - val_f1_score: 0.0843\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.4930 - f1_score: 0.3367 - val_loss: 0.7061 - val_acc: 0.1453 - val_f1_score: 0.1450\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.4954 - f1_score: 0.3469 - val_loss: 0.7067 - val_acc: 0.2436 - val_f1_score: 0.2349\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5021 - f1_score: 0.3675 - val_loss: 0.7073 - val_acc: 0.3141 - val_f1_score: 0.2846\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5099 - f1_score: 0.3979 - val_loss: 0.7080 - val_acc: 0.3718 - val_f1_score: 0.3260\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6881 - acc: 0.5208 - f1_score: 0.4405 - val_loss: 0.7086 - val_acc: 0.4402 - val_f1_score: 0.3722\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5408 - f1_score: 0.4942 - val_loss: 0.7091 - val_acc: 0.4893 - val_f1_score: 0.4041\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6871 - acc: 0.5532 - f1_score: 0.5348 - val_loss: 0.7096 - val_acc: 0.5192 - val_f1_score: 0.4231\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5622 - f1_score: 0.5550 - val_loss: 0.7101 - val_acc: 0.5427 - val_f1_score: 0.4380\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5622 - f1_score: 0.5604 - val_loss: 0.7105 - val_acc: 0.5641 - val_f1_score: 0.4516\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5605 - f1_score: 0.5603 - val_loss: 0.7109 - val_acc: 0.5748 - val_f1_score: 0.4583\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5589 - f1_score: 0.5588 - val_loss: 0.7114 - val_acc: 0.5833 - val_f1_score: 0.4638\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5585 - f1_score: 0.5580 - val_loss: 0.7117 - val_acc: 0.5897 - val_f1_score: 0.4678\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5577 - f1_score: 0.5569 - val_loss: 0.7119 - val_acc: 0.5919 - val_f1_score: 0.4692\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5596 - f1_score: 0.5583 - val_loss: 0.7119 - val_acc: 0.5983 - val_f1_score: 0.4733\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5587 - f1_score: 0.5567 - val_loss: 0.7119 - val_acc: 0.6026 - val_f1_score: 0.4760\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5590 - f1_score: 0.5564 - val_loss: 0.7119 - val_acc: 0.6047 - val_f1_score: 0.4774\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5577 - f1_score: 0.5547 - val_loss: 0.7118 - val_acc: 0.6047 - val_f1_score: 0.4774\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5595 - f1_score: 0.5557 - val_loss: 0.7117 - val_acc: 0.6154 - val_f1_score: 0.4842\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5596 - f1_score: 0.5554 - val_loss: 0.7116 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6827 - acc: 0.5599 - f1_score: 0.5553 - val_loss: 0.7114 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5598 - f1_score: 0.5548 - val_loss: 0.7109 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5611 - f1_score: 0.5554 - val_loss: 0.7107 - val_acc: 0.6282 - val_f1_score: 0.4925\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5599 - f1_score: 0.5538 - val_loss: 0.7105 - val_acc: 0.6282 - val_f1_score: 0.4925\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5589 - f1_score: 0.5524 - val_loss: 0.7104 - val_acc: 0.6325 - val_f1_score: 0.4953\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5579 - f1_score: 0.5511 - val_loss: 0.7102 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5576 - f1_score: 0.5506 - val_loss: 0.7099 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5577 - f1_score: 0.5505 - val_loss: 0.7095 - val_acc: 0.6389 - val_f1_score: 0.4995\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5579 - f1_score: 0.5500 - val_loss: 0.7092 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5579 - f1_score: 0.5499 - val_loss: 0.7087 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5582 - f1_score: 0.5496 - val_loss: 0.7084 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5580 - f1_score: 0.5493 - val_loss: 0.7079 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5589 - f1_score: 0.5498 - val_loss: 0.7076 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5586 - f1_score: 0.5491 - val_loss: 0.7073 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5593 - f1_score: 0.5494 - val_loss: 0.7073 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5586 - f1_score: 0.5485 - val_loss: 0.7071 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5590 - f1_score: 0.5487 - val_loss: 0.7067 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5598 - f1_score: 0.5489 - val_loss: 0.7065 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5598 - f1_score: 0.5488 - val_loss: 0.7063 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5602 - f1_score: 0.5490 - val_loss: 0.7059 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5615 - f1_score: 0.5496 - val_loss: 0.7057 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5621 - f1_score: 0.5499 - val_loss: 0.7055 - val_acc: 0.6538 - val_f1_score: 0.5093\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5624 - f1_score: 0.5501 - val_loss: 0.7052 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5627 - f1_score: 0.5501 - val_loss: 0.7050 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5631 - f1_score: 0.5501 - val_loss: 0.7049 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5631 - f1_score: 0.5504 - val_loss: 0.7046 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5641 - f1_score: 0.5510 - val_loss: 0.7047 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5644 - f1_score: 0.5510 - val_loss: 0.7051 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6774 - acc: 0.5641 - f1_score: 0.5509 - val_loss: 0.7050 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5650 - f1_score: 0.5515 - val_loss: 0.7049 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6771 - acc: 0.5656 - f1_score: 0.5516 - val_loss: 0.7051 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6769 - acc: 0.5657 - f1_score: 0.5516 - val_loss: 0.7055 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6768 - acc: 0.5653 - f1_score: 0.5511 - val_loss: 0.7056 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6766 - acc: 0.5663 - f1_score: 0.5516 - val_loss: 0.7064 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6764 - acc: 0.5660 - f1_score: 0.5514 - val_loss: 0.7064 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6763 - acc: 0.5661 - f1_score: 0.5513 - val_loss: 0.7066 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6761 - acc: 0.5657 - f1_score: 0.5512 - val_loss: 0.7059 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6759 - acc: 0.5661 - f1_score: 0.5512 - val_loss: 0.7058 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6758 - acc: 0.5661 - f1_score: 0.5505 - val_loss: 0.7068 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 64/300\n",
      "27/27 - 3s - loss: 0.6756 - acc: 0.5671 - f1_score: 0.5512 - val_loss: 0.7077 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 65/300\n",
      "27/27 - 3s - loss: 0.6754 - acc: 0.5667 - f1_score: 0.5513 - val_loss: 0.7075 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 66/300\n",
      "27/27 - 3s - loss: 0.6753 - acc: 0.5679 - f1_score: 0.5518 - val_loss: 0.7080 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 67/300\n",
      "27/27 - 3s - loss: 0.6751 - acc: 0.5679 - f1_score: 0.5517 - val_loss: 0.7085 - val_acc: 0.6603 - val_f1_score: 0.5136\n",
      "Epoch 68/300\n",
      "27/27 - 3s - loss: 0.6750 - acc: 0.5687 - f1_score: 0.5526 - val_loss: 0.7088 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 69/300\n",
      "27/27 - 3s - loss: 0.6748 - acc: 0.5693 - f1_score: 0.5532 - val_loss: 0.7088 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 70/300\n",
      "27/27 - 3s - loss: 0.6746 - acc: 0.5690 - f1_score: 0.5530 - val_loss: 0.7084 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 71/300\n",
      "27/27 - 3s - loss: 0.6744 - acc: 0.5703 - f1_score: 0.5538 - val_loss: 0.7088 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 72/300\n",
      "27/27 - 3s - loss: 0.6743 - acc: 0.5700 - f1_score: 0.5535 - val_loss: 0.7091 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 73/300\n",
      "27/27 - 3s - loss: 0.6741 - acc: 0.5698 - f1_score: 0.5527 - val_loss: 0.7098 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 74/300\n",
      "27/27 - 3s - loss: 0.6739 - acc: 0.5706 - f1_score: 0.5533 - val_loss: 0.7106 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 75/300\n",
      "27/27 - 3s - loss: 0.6738 - acc: 0.5708 - f1_score: 0.5533 - val_loss: 0.7110 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 76/300\n",
      "27/27 - 3s - loss: 0.6736 - acc: 0.5699 - f1_score: 0.5523 - val_loss: 0.7116 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 77/300\n",
      "27/27 - 3s - loss: 0.6734 - acc: 0.5700 - f1_score: 0.5525 - val_loss: 0.7115 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00077: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.15      0.25       170\n",
      "           1       0.66      0.96      0.78       298\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.67      0.55      0.52       468\n",
      "weighted avg       0.67      0.66      0.59       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1175\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 13s - loss: 0.6947 - acc: 0.4517 - f1_score: 0.3111 - val_loss: 0.6826 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6941 - acc: 0.4518 - f1_score: 0.3117 - val_loss: 0.6832 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6935 - acc: 0.4514 - f1_score: 0.3132 - val_loss: 0.6840 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6929 - acc: 0.4597 - f1_score: 0.3364 - val_loss: 0.6846 - val_acc: 0.7215 - val_f1_score: 0.4282\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4827 - f1_score: 0.3980 - val_loss: 0.6850 - val_acc: 0.7082 - val_f1_score: 0.4397\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.5056 - f1_score: 0.4666 - val_loss: 0.6856 - val_acc: 0.7029 - val_f1_score: 0.5597\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6913 - acc: 0.5377 - f1_score: 0.5328 - val_loss: 0.6861 - val_acc: 0.6313 - val_f1_score: 0.5723\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6907 - acc: 0.5580 - f1_score: 0.5563 - val_loss: 0.6864 - val_acc: 0.5995 - val_f1_score: 0.5796\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6902 - acc: 0.5665 - f1_score: 0.5565 - val_loss: 0.6866 - val_acc: 0.5756 - val_f1_score: 0.5689\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5672 - f1_score: 0.5523 - val_loss: 0.6868 - val_acc: 0.5623 - val_f1_score: 0.5587\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5678 - f1_score: 0.5491 - val_loss: 0.6870 - val_acc: 0.5570 - val_f1_score: 0.5558\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5698 - f1_score: 0.5476 - val_loss: 0.6873 - val_acc: 0.5517 - val_f1_score: 0.5511\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6882 - acc: 0.5713 - f1_score: 0.5461 - val_loss: 0.6875 - val_acc: 0.5597 - val_f1_score: 0.5594\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5715 - f1_score: 0.5450 - val_loss: 0.6877 - val_acc: 0.5650 - val_f1_score: 0.5649\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5729 - f1_score: 0.5443 - val_loss: 0.6878 - val_acc: 0.5517 - val_f1_score: 0.5517\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5721 - f1_score: 0.5425 - val_loss: 0.6882 - val_acc: 0.5491 - val_f1_score: 0.5491\n",
      "Epoch 17/300\n",
      "28/28 - 4s - loss: 0.6863 - acc: 0.5728 - f1_score: 0.5420 - val_loss: 0.6883 - val_acc: 0.5464 - val_f1_score: 0.5464\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5726 - f1_score: 0.5409 - val_loss: 0.6885 - val_acc: 0.5464 - val_f1_score: 0.5464\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5723 - f1_score: 0.5399 - val_loss: 0.6885 - val_acc: 0.5438 - val_f1_score: 0.5437\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5719 - f1_score: 0.5394 - val_loss: 0.6889 - val_acc: 0.5411 - val_f1_score: 0.5411\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6847 - acc: 0.5733 - f1_score: 0.5397 - val_loss: 0.6894 - val_acc: 0.5385 - val_f1_score: 0.5384\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5746 - f1_score: 0.5397 - val_loss: 0.6895 - val_acc: 0.5332 - val_f1_score: 0.5330\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5743 - f1_score: 0.5394 - val_loss: 0.6901 - val_acc: 0.5332 - val_f1_score: 0.5330\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5758 - f1_score: 0.5394 - val_loss: 0.6906 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5762 - f1_score: 0.5395 - val_loss: 0.6910 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5766 - f1_score: 0.5400 - val_loss: 0.6914 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5779 - f1_score: 0.5400 - val_loss: 0.6916 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5778 - f1_score: 0.5396 - val_loss: 0.6921 - val_acc: 0.5225 - val_f1_score: 0.5221\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.83      0.67       186\n",
      "           1       0.69      0.38      0.49       191\n",
      "\n",
      "    accuracy                           0.60       377\n",
      "   macro avg       0.63      0.60      0.58       377\n",
      "weighted avg       0.63      0.60      0.58       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1194\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6930 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7093 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6925 - acc: 0.4829 - f1_score: 0.3257 - val_loss: 0.7096 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6920 - acc: 0.4829 - f1_score: 0.3257 - val_loss: 0.7103 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.4831 - f1_score: 0.3260 - val_loss: 0.7106 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.4831 - f1_score: 0.3262 - val_loss: 0.7113 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6904 - acc: 0.4831 - f1_score: 0.3265 - val_loss: 0.7127 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.4832 - f1_score: 0.3280 - val_loss: 0.7131 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.4858 - f1_score: 0.3364 - val_loss: 0.7138 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.4935 - f1_score: 0.3572 - val_loss: 0.7154 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6884 - acc: 0.4988 - f1_score: 0.3698 - val_loss: 0.7168 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6880 - acc: 0.5025 - f1_score: 0.3781 - val_loss: 0.7172 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5088 - f1_score: 0.3954 - val_loss: 0.7178 - val_acc: 0.2066 - val_f1_score: 0.1758\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5176 - f1_score: 0.4206 - val_loss: 0.7191 - val_acc: 0.2132 - val_f1_score: 0.1861\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5257 - f1_score: 0.4433 - val_loss: 0.7192 - val_acc: 0.2220 - val_f1_score: 0.1990\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5383 - f1_score: 0.4736 - val_loss: 0.7189 - val_acc: 0.2615 - val_f1_score: 0.2514\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5489 - f1_score: 0.5062 - val_loss: 0.7201 - val_acc: 0.2901 - val_f1_score: 0.2856\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5528 - f1_score: 0.5204 - val_loss: 0.7203 - val_acc: 0.3055 - val_f1_score: 0.3045\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5571 - f1_score: 0.5349 - val_loss: 0.7199 - val_acc: 0.3495 - val_f1_score: 0.3468\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5641 - f1_score: 0.5532 - val_loss: 0.7207 - val_acc: 0.3956 - val_f1_score: 0.3826\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5685 - f1_score: 0.5629 - val_loss: 0.7204 - val_acc: 0.4527 - val_f1_score: 0.4213\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5642 - f1_score: 0.5619 - val_loss: 0.7203 - val_acc: 0.5011 - val_f1_score: 0.4522\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5642 - f1_score: 0.5638 - val_loss: 0.7208 - val_acc: 0.5121 - val_f1_score: 0.4535\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5606 - f1_score: 0.5605 - val_loss: 0.7212 - val_acc: 0.5143 - val_f1_score: 0.4514\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5607 - f1_score: 0.5607 - val_loss: 0.7203 - val_acc: 0.5187 - val_f1_score: 0.4468\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5596 - f1_score: 0.5593 - val_loss: 0.7204 - val_acc: 0.5297 - val_f1_score: 0.4544\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5628 - f1_score: 0.5623 - val_loss: 0.7211 - val_acc: 0.5363 - val_f1_score: 0.4589\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5619 - f1_score: 0.5612 - val_loss: 0.7207 - val_acc: 0.5363 - val_f1_score: 0.4525\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5616 - f1_score: 0.5605 - val_loss: 0.7213 - val_acc: 0.5341 - val_f1_score: 0.4487\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5638 - f1_score: 0.5623 - val_loss: 0.7212 - val_acc: 0.5341 - val_f1_score: 0.4441\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5628 - f1_score: 0.5609 - val_loss: 0.7215 - val_acc: 0.5341 - val_f1_score: 0.4441\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5639 - f1_score: 0.5617 - val_loss: 0.7206 - val_acc: 0.5363 - val_f1_score: 0.4432\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5616 - f1_score: 0.5588 - val_loss: 0.7203 - val_acc: 0.5363 - val_f1_score: 0.4408\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5623 - f1_score: 0.5589 - val_loss: 0.7184 - val_acc: 0.5407 - val_f1_score: 0.4411\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5622 - f1_score: 0.5579 - val_loss: 0.7180 - val_acc: 0.5473 - val_f1_score: 0.4454\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5623 - f1_score: 0.5575 - val_loss: 0.7162 - val_acc: 0.5516 - val_f1_score: 0.4482\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5636 - f1_score: 0.5578 - val_loss: 0.7149 - val_acc: 0.5560 - val_f1_score: 0.4510\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5656 - f1_score: 0.5592 - val_loss: 0.7141 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5642 - f1_score: 0.5574 - val_loss: 0.7145 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5643 - f1_score: 0.5575 - val_loss: 0.7149 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5636 - f1_score: 0.5567 - val_loss: 0.7154 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5646 - f1_score: 0.5578 - val_loss: 0.7151 - val_acc: 0.5560 - val_f1_score: 0.4484\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5628 - f1_score: 0.5556 - val_loss: 0.7135 - val_acc: 0.5604 - val_f1_score: 0.4512\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5625 - f1_score: 0.5547 - val_loss: 0.7135 - val_acc: 0.5604 - val_f1_score: 0.4512\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5628 - f1_score: 0.5546 - val_loss: 0.7126 - val_acc: 0.5604 - val_f1_score: 0.4512\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5615 - f1_score: 0.5528 - val_loss: 0.7111 - val_acc: 0.5670 - val_f1_score: 0.4554\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5612 - f1_score: 0.5517 - val_loss: 0.7098 - val_acc: 0.5692 - val_f1_score: 0.4568\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00046: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.19      0.25       192\n",
      "           1       0.57      0.79      0.66       263\n",
      "\n",
      "    accuracy                           0.54       455\n",
      "   macro avg       0.48      0.49      0.46       455\n",
      "weighted avg       0.50      0.54      0.49       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1337\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 11s - loss: 0.6946 - acc: 0.4482 - f1_score: 0.3095 - val_loss: 0.6863 - val_acc: 0.7222 - val_f1_score: 0.4194\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6940 - acc: 0.4484 - f1_score: 0.3102 - val_loss: 0.6869 - val_acc: 0.7115 - val_f1_score: 0.4612\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6934 - acc: 0.4539 - f1_score: 0.3236 - val_loss: 0.6875 - val_acc: 0.7265 - val_f1_score: 0.6244\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4743 - f1_score: 0.3711 - val_loss: 0.6879 - val_acc: 0.6709 - val_f1_score: 0.6355\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.5050 - f1_score: 0.4460 - val_loss: 0.6882 - val_acc: 0.6218 - val_f1_score: 0.6113\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.5332 - f1_score: 0.5237 - val_loss: 0.6885 - val_acc: 0.5940 - val_f1_score: 0.5893\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5580 - f1_score: 0.5572 - val_loss: 0.6887 - val_acc: 0.5791 - val_f1_score: 0.5758\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6907 - acc: 0.5680 - f1_score: 0.5554 - val_loss: 0.6887 - val_acc: 0.5556 - val_f1_score: 0.5537\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6902 - acc: 0.5702 - f1_score: 0.5486 - val_loss: 0.6887 - val_acc: 0.5449 - val_f1_score: 0.5436\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6897 - acc: 0.5706 - f1_score: 0.5437 - val_loss: 0.6887 - val_acc: 0.5385 - val_f1_score: 0.5376\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5725 - f1_score: 0.5414 - val_loss: 0.6885 - val_acc: 0.5363 - val_f1_score: 0.5357\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6888 - acc: 0.5729 - f1_score: 0.5384 - val_loss: 0.6884 - val_acc: 0.5363 - val_f1_score: 0.5358\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5724 - f1_score: 0.5361 - val_loss: 0.6882 - val_acc: 0.5342 - val_f1_score: 0.5339\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5729 - f1_score: 0.5345 - val_loss: 0.6880 - val_acc: 0.5363 - val_f1_score: 0.5360\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6875 - acc: 0.5732 - f1_score: 0.5337 - val_loss: 0.6878 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6871 - acc: 0.5742 - f1_score: 0.5339 - val_loss: 0.6876 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5744 - f1_score: 0.5342 - val_loss: 0.6874 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5753 - f1_score: 0.5339 - val_loss: 0.6872 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6859 - acc: 0.5755 - f1_score: 0.5343 - val_loss: 0.6870 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5764 - f1_score: 0.5344 - val_loss: 0.6868 - val_acc: 0.5342 - val_f1_score: 0.5340\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5769 - f1_score: 0.5345 - val_loss: 0.6865 - val_acc: 0.5342 - val_f1_score: 0.5340\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5766 - f1_score: 0.5338 - val_loss: 0.6863 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5773 - f1_score: 0.5342 - val_loss: 0.6861 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6842 - acc: 0.5777 - f1_score: 0.5344 - val_loss: 0.6858 - val_acc: 0.5363 - val_f1_score: 0.5361\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75       276\n",
      "           1       0.65      0.44      0.52       192\n",
      "\n",
      "    accuracy                           0.67       468\n",
      "   macro avg       0.66      0.64      0.64       468\n",
      "weighted avg       0.67      0.67      0.66       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1390\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 11s - loss: 0.6940 - acc: 0.4670 - f1_score: 0.3184 - val_loss: 0.6943 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6934 - acc: 0.4668 - f1_score: 0.3185 - val_loss: 0.6934 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6929 - acc: 0.4683 - f1_score: 0.3246 - val_loss: 0.6924 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6924 - acc: 0.4788 - f1_score: 0.3537 - val_loss: 0.6915 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6919 - acc: 0.4973 - f1_score: 0.4019 - val_loss: 0.6905 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.5164 - f1_score: 0.4591 - val_loss: 0.6896 - val_acc: 0.4487 - val_f1_score: 0.3399\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6909 - acc: 0.5325 - f1_score: 0.5073 - val_loss: 0.6886 - val_acc: 0.4850 - val_f1_score: 0.4320\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6905 - acc: 0.5501 - f1_score: 0.5485 - val_loss: 0.6877 - val_acc: 0.5128 - val_f1_score: 0.5041\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5628 - f1_score: 0.5619 - val_loss: 0.6868 - val_acc: 0.5641 - val_f1_score: 0.5635\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6895 - acc: 0.5632 - f1_score: 0.5596 - val_loss: 0.6858 - val_acc: 0.5684 - val_f1_score: 0.5607\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5600 - f1_score: 0.5530 - val_loss: 0.6849 - val_acc: 0.6026 - val_f1_score: 0.5887\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5615 - f1_score: 0.5522 - val_loss: 0.6841 - val_acc: 0.6154 - val_f1_score: 0.5977\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5609 - f1_score: 0.5495 - val_loss: 0.6832 - val_acc: 0.6154 - val_f1_score: 0.5954\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5602 - f1_score: 0.5472 - val_loss: 0.6824 - val_acc: 0.6197 - val_f1_score: 0.5983\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6875 - acc: 0.5600 - f1_score: 0.5458 - val_loss: 0.6816 - val_acc: 0.6239 - val_f1_score: 0.6019\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6871 - acc: 0.5608 - f1_score: 0.5452 - val_loss: 0.6808 - val_acc: 0.6197 - val_f1_score: 0.5957\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5599 - f1_score: 0.5439 - val_loss: 0.6800 - val_acc: 0.6175 - val_f1_score: 0.5929\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5605 - f1_score: 0.5431 - val_loss: 0.6792 - val_acc: 0.6154 - val_f1_score: 0.5893\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5598 - f1_score: 0.5424 - val_loss: 0.6785 - val_acc: 0.6154 - val_f1_score: 0.5873\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5596 - f1_score: 0.5413 - val_loss: 0.6778 - val_acc: 0.6197 - val_f1_score: 0.5889\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5614 - f1_score: 0.5424 - val_loss: 0.6771 - val_acc: 0.6175 - val_f1_score: 0.5861\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5615 - f1_score: 0.5422 - val_loss: 0.6764 - val_acc: 0.6197 - val_f1_score: 0.5879\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5631 - f1_score: 0.5430 - val_loss: 0.6757 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5638 - f1_score: 0.5433 - val_loss: 0.6751 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 25/300\n",
      "27/27 - 4s - loss: 0.6844 - acc: 0.5632 - f1_score: 0.5425 - val_loss: 0.6745 - val_acc: 0.6218 - val_f1_score: 0.5896\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5635 - f1_score: 0.5421 - val_loss: 0.6739 - val_acc: 0.6239 - val_f1_score: 0.5914\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5647 - f1_score: 0.5427 - val_loss: 0.6734 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5656 - f1_score: 0.5430 - val_loss: 0.6728 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5671 - f1_score: 0.5438 - val_loss: 0.6723 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5669 - f1_score: 0.5429 - val_loss: 0.6718 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5669 - f1_score: 0.5425 - val_loss: 0.6713 - val_acc: 0.6239 - val_f1_score: 0.5903\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5670 - f1_score: 0.5428 - val_loss: 0.6709 - val_acc: 0.6303 - val_f1_score: 0.5957\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5666 - f1_score: 0.5412 - val_loss: 0.6704 - val_acc: 0.6303 - val_f1_score: 0.5957\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5663 - f1_score: 0.5408 - val_loss: 0.6700 - val_acc: 0.6282 - val_f1_score: 0.5928\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5661 - f1_score: 0.5396 - val_loss: 0.6696 - val_acc: 0.6282 - val_f1_score: 0.5928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00035: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.61      0.51       150\n",
      "           1       0.77      0.63      0.70       318\n",
      "\n",
      "    accuracy                           0.62       468\n",
      "   macro avg       0.61      0.62      0.60       468\n",
      "weighted avg       0.67      0.62      0.64       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1400\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 11s - loss: 0.6928 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7122 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7130 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.4878 - f1_score: 0.3281 - val_loss: 0.7140 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6909 - acc: 0.4876 - f1_score: 0.3283 - val_loss: 0.7150 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6902 - acc: 0.4883 - f1_score: 0.3306 - val_loss: 0.7163 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6895 - acc: 0.4930 - f1_score: 0.3473 - val_loss: 0.7176 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6889 - acc: 0.5018 - f1_score: 0.3689 - val_loss: 0.7189 - val_acc: 0.1453 - val_f1_score: 0.1300\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6882 - acc: 0.5154 - f1_score: 0.4030 - val_loss: 0.7203 - val_acc: 0.1453 - val_f1_score: 0.1310\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5276 - f1_score: 0.4342 - val_loss: 0.7217 - val_acc: 0.1774 - val_f1_score: 0.1722\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6870 - acc: 0.5376 - f1_score: 0.4642 - val_loss: 0.7230 - val_acc: 0.2329 - val_f1_score: 0.2328\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6865 - acc: 0.5473 - f1_score: 0.5024 - val_loss: 0.7243 - val_acc: 0.3077 - val_f1_score: 0.3015\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5544 - f1_score: 0.5327 - val_loss: 0.7255 - val_acc: 0.3803 - val_f1_score: 0.3507\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5635 - f1_score: 0.5561 - val_loss: 0.7267 - val_acc: 0.4573 - val_f1_score: 0.3987\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5654 - f1_score: 0.5636 - val_loss: 0.7279 - val_acc: 0.4808 - val_f1_score: 0.4111\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5645 - f1_score: 0.5645 - val_loss: 0.7288 - val_acc: 0.5021 - val_f1_score: 0.4212\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6842 - acc: 0.5619 - f1_score: 0.5618 - val_loss: 0.7296 - val_acc: 0.5171 - val_f1_score: 0.4288\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5618 - f1_score: 0.5612 - val_loss: 0.7301 - val_acc: 0.5256 - val_f1_score: 0.4320\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5608 - f1_score: 0.5595 - val_loss: 0.7306 - val_acc: 0.5342 - val_f1_score: 0.4351\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5625 - f1_score: 0.5607 - val_loss: 0.7309 - val_acc: 0.5385 - val_f1_score: 0.4353\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5625 - f1_score: 0.5598 - val_loss: 0.7312 - val_acc: 0.5427 - val_f1_score: 0.4355\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5616 - f1_score: 0.5583 - val_loss: 0.7314 - val_acc: 0.5470 - val_f1_score: 0.4355\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5608 - f1_score: 0.5569 - val_loss: 0.7313 - val_acc: 0.5577 - val_f1_score: 0.4394\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5618 - f1_score: 0.5572 - val_loss: 0.7312 - val_acc: 0.5641 - val_f1_score: 0.4433\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5609 - f1_score: 0.5557 - val_loss: 0.7310 - val_acc: 0.5641 - val_f1_score: 0.4433\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5615 - f1_score: 0.5558 - val_loss: 0.7304 - val_acc: 0.5791 - val_f1_score: 0.4525\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5612 - f1_score: 0.5552 - val_loss: 0.7300 - val_acc: 0.5897 - val_f1_score: 0.4561\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5622 - f1_score: 0.5557 - val_loss: 0.7295 - val_acc: 0.5897 - val_f1_score: 0.4530\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5624 - f1_score: 0.5553 - val_loss: 0.7290 - val_acc: 0.5897 - val_f1_score: 0.4530\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5615 - f1_score: 0.5540 - val_loss: 0.7286 - val_acc: 0.5919 - val_f1_score: 0.4511\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5611 - f1_score: 0.5531 - val_loss: 0.7281 - val_acc: 0.5940 - val_f1_score: 0.4524\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5602 - f1_score: 0.5518 - val_loss: 0.7278 - val_acc: 0.5962 - val_f1_score: 0.4504\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5605 - f1_score: 0.5522 - val_loss: 0.7269 - val_acc: 0.6004 - val_f1_score: 0.4529\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5600 - f1_score: 0.5507 - val_loss: 0.7263 - val_acc: 0.6004 - val_f1_score: 0.4529\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5600 - f1_score: 0.5505 - val_loss: 0.7256 - val_acc: 0.6047 - val_f1_score: 0.4555\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5609 - f1_score: 0.5506 - val_loss: 0.7254 - val_acc: 0.6068 - val_f1_score: 0.4567\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5603 - f1_score: 0.5500 - val_loss: 0.7245 - val_acc: 0.6090 - val_f1_score: 0.4580\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5603 - f1_score: 0.5495 - val_loss: 0.7240 - val_acc: 0.6132 - val_f1_score: 0.4605\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5602 - f1_score: 0.5491 - val_loss: 0.7231 - val_acc: 0.6132 - val_f1_score: 0.4605\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6792 - acc: 0.5614 - f1_score: 0.5499 - val_loss: 0.7227 - val_acc: 0.6111 - val_f1_score: 0.4558\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5614 - f1_score: 0.5497 - val_loss: 0.7220 - val_acc: 0.6090 - val_f1_score: 0.4511\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5615 - f1_score: 0.5491 - val_loss: 0.7218 - val_acc: 0.6090 - val_f1_score: 0.4511\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5606 - f1_score: 0.5482 - val_loss: 0.7213 - val_acc: 0.6111 - val_f1_score: 0.4523\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6787 - acc: 0.5612 - f1_score: 0.5485 - val_loss: 0.7208 - val_acc: 0.6111 - val_f1_score: 0.4523\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5612 - f1_score: 0.5482 - val_loss: 0.7204 - val_acc: 0.6111 - val_f1_score: 0.4523\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5608 - f1_score: 0.5476 - val_loss: 0.7197 - val_acc: 0.6111 - val_f1_score: 0.4523\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5605 - f1_score: 0.5472 - val_loss: 0.7187 - val_acc: 0.6132 - val_f1_score: 0.4535\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5615 - f1_score: 0.5477 - val_loss: 0.7180 - val_acc: 0.6132 - val_f1_score: 0.4535\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6782 - acc: 0.5619 - f1_score: 0.5478 - val_loss: 0.7176 - val_acc: 0.6132 - val_f1_score: 0.4535\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5624 - f1_score: 0.5479 - val_loss: 0.7172 - val_acc: 0.6132 - val_f1_score: 0.4535\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5622 - f1_score: 0.5477 - val_loss: 0.7167 - val_acc: 0.6154 - val_f1_score: 0.4548\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5640 - f1_score: 0.5493 - val_loss: 0.7162 - val_acc: 0.6175 - val_f1_score: 0.4560\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5638 - f1_score: 0.5483 - val_loss: 0.7163 - val_acc: 0.6154 - val_f1_score: 0.4512\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5637 - f1_score: 0.5487 - val_loss: 0.7154 - val_acc: 0.6154 - val_f1_score: 0.4512\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5661 - f1_score: 0.5505 - val_loss: 0.7152 - val_acc: 0.6154 - val_f1_score: 0.4512\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5651 - f1_score: 0.5497 - val_loss: 0.7145 - val_acc: 0.6154 - val_f1_score: 0.4512\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6774 - acc: 0.5669 - f1_score: 0.5506 - val_loss: 0.7146 - val_acc: 0.6175 - val_f1_score: 0.4524\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5657 - f1_score: 0.5497 - val_loss: 0.7140 - val_acc: 0.6197 - val_f1_score: 0.4536\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00057: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.12      0.17       154\n",
      "           1       0.67      0.85      0.75       314\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.48      0.49      0.46       468\n",
      "weighted avg       0.54      0.61      0.56       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1419\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 11s - loss: 0.6937 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.6997 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4754 - f1_score: 0.3222 - val_loss: 0.6986 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6929 - acc: 0.4756 - f1_score: 0.3225 - val_loss: 0.6973 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6925 - acc: 0.4756 - f1_score: 0.3237 - val_loss: 0.6963 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6922 - acc: 0.4759 - f1_score: 0.3265 - val_loss: 0.6955 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4789 - f1_score: 0.3344 - val_loss: 0.6947 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6916 - acc: 0.4868 - f1_score: 0.3554 - val_loss: 0.6937 - val_acc: 0.3121 - val_f1_score: 0.2379\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4986 - f1_score: 0.3833 - val_loss: 0.6926 - val_acc: 0.3165 - val_f1_score: 0.2450\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5084 - f1_score: 0.4130 - val_loss: 0.6919 - val_acc: 0.3297 - val_f1_score: 0.2660\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6906 - acc: 0.5185 - f1_score: 0.4388 - val_loss: 0.6912 - val_acc: 0.3429 - val_f1_score: 0.2883\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5250 - f1_score: 0.4607 - val_loss: 0.6904 - val_acc: 0.3714 - val_f1_score: 0.3389\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.5348 - f1_score: 0.4956 - val_loss: 0.6894 - val_acc: 0.4440 - val_f1_score: 0.4396\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5486 - f1_score: 0.5311 - val_loss: 0.6886 - val_acc: 0.5143 - val_f1_score: 0.5133\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.5561 - f1_score: 0.5513 - val_loss: 0.6880 - val_acc: 0.5538 - val_f1_score: 0.5488\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5570 - f1_score: 0.5552 - val_loss: 0.6872 - val_acc: 0.5934 - val_f1_score: 0.5824\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.5573 - f1_score: 0.5571 - val_loss: 0.6866 - val_acc: 0.6000 - val_f1_score: 0.5848\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5578 - f1_score: 0.5578 - val_loss: 0.6861 - val_acc: 0.6198 - val_f1_score: 0.6005\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5588 - f1_score: 0.5588 - val_loss: 0.6855 - val_acc: 0.6198 - val_f1_score: 0.5997\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5578 - f1_score: 0.5573 - val_loss: 0.6850 - val_acc: 0.6330 - val_f1_score: 0.6093\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5548 - f1_score: 0.5539 - val_loss: 0.6841 - val_acc: 0.6352 - val_f1_score: 0.6103\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5555 - f1_score: 0.5533 - val_loss: 0.6835 - val_acc: 0.6352 - val_f1_score: 0.6084\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5561 - f1_score: 0.5529 - val_loss: 0.6830 - val_acc: 0.6352 - val_f1_score: 0.6084\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5549 - f1_score: 0.5514 - val_loss: 0.6824 - val_acc: 0.6374 - val_f1_score: 0.6103\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5552 - f1_score: 0.5515 - val_loss: 0.6816 - val_acc: 0.6374 - val_f1_score: 0.6093\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6871 - acc: 0.5574 - f1_score: 0.5521 - val_loss: 0.6811 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5575 - f1_score: 0.5520 - val_loss: 0.6806 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5573 - f1_score: 0.5513 - val_loss: 0.6800 - val_acc: 0.6440 - val_f1_score: 0.6139\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5571 - f1_score: 0.5507 - val_loss: 0.6795 - val_acc: 0.6462 - val_f1_score: 0.6157\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5564 - f1_score: 0.5496 - val_loss: 0.6788 - val_acc: 0.6505 - val_f1_score: 0.6195\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5575 - f1_score: 0.5503 - val_loss: 0.6782 - val_acc: 0.6571 - val_f1_score: 0.6251\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5568 - f1_score: 0.5492 - val_loss: 0.6774 - val_acc: 0.6571 - val_f1_score: 0.6230\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5587 - f1_score: 0.5505 - val_loss: 0.6768 - val_acc: 0.6571 - val_f1_score: 0.6230\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5586 - f1_score: 0.5498 - val_loss: 0.6760 - val_acc: 0.6593 - val_f1_score: 0.6237\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5578 - f1_score: 0.5480 - val_loss: 0.6752 - val_acc: 0.6593 - val_f1_score: 0.6214\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5571 - f1_score: 0.5468 - val_loss: 0.6744 - val_acc: 0.6593 - val_f1_score: 0.6214\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5584 - f1_score: 0.5467 - val_loss: 0.6740 - val_acc: 0.6593 - val_f1_score: 0.6214\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5574 - f1_score: 0.5460 - val_loss: 0.6735 - val_acc: 0.6571 - val_f1_score: 0.6172\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5590 - f1_score: 0.5467 - val_loss: 0.6731 - val_acc: 0.6571 - val_f1_score: 0.6172\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5583 - f1_score: 0.5462 - val_loss: 0.6725 - val_acc: 0.6615 - val_f1_score: 0.6209\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6847 - acc: 0.5594 - f1_score: 0.5464 - val_loss: 0.6721 - val_acc: 0.6637 - val_f1_score: 0.6227\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5593 - f1_score: 0.5466 - val_loss: 0.6717 - val_acc: 0.6637 - val_f1_score: 0.6227\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5596 - f1_score: 0.5467 - val_loss: 0.6709 - val_acc: 0.6659 - val_f1_score: 0.6233\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5590 - f1_score: 0.5452 - val_loss: 0.6705 - val_acc: 0.6659 - val_f1_score: 0.6233\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5597 - f1_score: 0.5458 - val_loss: 0.6699 - val_acc: 0.6659 - val_f1_score: 0.6233\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5587 - f1_score: 0.5445 - val_loss: 0.6692 - val_acc: 0.6681 - val_f1_score: 0.6252\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5587 - f1_score: 0.5439 - val_loss: 0.6685 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 47/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5583 - f1_score: 0.5425 - val_loss: 0.6680 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 48/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5586 - f1_score: 0.5421 - val_loss: 0.6677 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 49/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5584 - f1_score: 0.5423 - val_loss: 0.6672 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 50/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5591 - f1_score: 0.5430 - val_loss: 0.6667 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 51/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5588 - f1_score: 0.5424 - val_loss: 0.6662 - val_acc: 0.6725 - val_f1_score: 0.6276\n",
      "Epoch 52/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5597 - f1_score: 0.5429 - val_loss: 0.6659 - val_acc: 0.6703 - val_f1_score: 0.6258\n",
      "Epoch 53/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5591 - f1_score: 0.5427 - val_loss: 0.6652 - val_acc: 0.6747 - val_f1_score: 0.6295\n",
      "Epoch 54/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5593 - f1_score: 0.5420 - val_loss: 0.6644 - val_acc: 0.6791 - val_f1_score: 0.6332\n",
      "Epoch 55/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5600 - f1_score: 0.5420 - val_loss: 0.6642 - val_acc: 0.6769 - val_f1_score: 0.6313\n",
      "Epoch 56/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5600 - f1_score: 0.5423 - val_loss: 0.6636 - val_acc: 0.6813 - val_f1_score: 0.6351\n",
      "Epoch 57/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5600 - f1_score: 0.5416 - val_loss: 0.6634 - val_acc: 0.6791 - val_f1_score: 0.6332\n",
      "Epoch 58/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5597 - f1_score: 0.5418 - val_loss: 0.6629 - val_acc: 0.6835 - val_f1_score: 0.6369\n",
      "Epoch 59/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5599 - f1_score: 0.5413 - val_loss: 0.6624 - val_acc: 0.6857 - val_f1_score: 0.6388\n",
      "Epoch 60/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5606 - f1_score: 0.5413 - val_loss: 0.6620 - val_acc: 0.6879 - val_f1_score: 0.6407\n",
      "Epoch 61/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5615 - f1_score: 0.5421 - val_loss: 0.6614 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 62/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5625 - f1_score: 0.5422 - val_loss: 0.6607 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 63/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5626 - f1_score: 0.5419 - val_loss: 0.6604 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 64/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5622 - f1_score: 0.5420 - val_loss: 0.6599 - val_acc: 0.7033 - val_f1_score: 0.6538\n",
      "Epoch 65/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5623 - f1_score: 0.5409 - val_loss: 0.6597 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 66/300\n",
      "28/28 - 3s - loss: 0.6815 - acc: 0.5625 - f1_score: 0.5419 - val_loss: 0.6593 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 67/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5633 - f1_score: 0.5427 - val_loss: 0.6590 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 68/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5633 - f1_score: 0.5430 - val_loss: 0.6585 - val_acc: 0.6945 - val_f1_score: 0.6463\n",
      "Epoch 69/300\n",
      "28/28 - 3s - loss: 0.6812 - acc: 0.5638 - f1_score: 0.5433 - val_loss: 0.6578 - val_acc: 0.6989 - val_f1_score: 0.6473\n",
      "Epoch 70/300\n",
      "28/28 - 3s - loss: 0.6811 - acc: 0.5632 - f1_score: 0.5415 - val_loss: 0.6572 - val_acc: 0.6967 - val_f1_score: 0.6440\n",
      "Epoch 71/300\n",
      "28/28 - 3s - loss: 0.6810 - acc: 0.5638 - f1_score: 0.5409 - val_loss: 0.6569 - val_acc: 0.6989 - val_f1_score: 0.6473\n",
      "Epoch 72/300\n",
      "28/28 - 3s - loss: 0.6809 - acc: 0.5641 - f1_score: 0.5414 - val_loss: 0.6562 - val_acc: 0.6989 - val_f1_score: 0.6459\n",
      "Epoch 73/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5638 - f1_score: 0.5402 - val_loss: 0.6556 - val_acc: 0.7011 - val_f1_score: 0.6478\n",
      "Epoch 74/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5641 - f1_score: 0.5393 - val_loss: 0.6556 - val_acc: 0.6989 - val_f1_score: 0.6459\n",
      "Epoch 75/300\n",
      "28/28 - 3s - loss: 0.6806 - acc: 0.5643 - f1_score: 0.5406 - val_loss: 0.6555 - val_acc: 0.6967 - val_f1_score: 0.6440\n",
      "Epoch 76/300\n",
      "28/28 - 3s - loss: 0.6805 - acc: 0.5645 - f1_score: 0.5409 - val_loss: 0.6552 - val_acc: 0.6989 - val_f1_score: 0.6459\n",
      "Epoch 77/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5643 - f1_score: 0.5402 - val_loss: 0.6545 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 78/300\n",
      "28/28 - 3s - loss: 0.6803 - acc: 0.5652 - f1_score: 0.5401 - val_loss: 0.6543 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 79/300\n",
      "28/28 - 3s - loss: 0.6802 - acc: 0.5652 - f1_score: 0.5404 - val_loss: 0.6542 - val_acc: 0.6967 - val_f1_score: 0.6440\n",
      "Epoch 80/300\n",
      "28/28 - 3s - loss: 0.6801 - acc: 0.5652 - f1_score: 0.5407 - val_loss: 0.6540 - val_acc: 0.6989 - val_f1_score: 0.6459\n",
      "Epoch 81/300\n",
      "28/28 - 3s - loss: 0.6800 - acc: 0.5646 - f1_score: 0.5400 - val_loss: 0.6536 - val_acc: 0.6989 - val_f1_score: 0.6459\n",
      "Epoch 82/300\n",
      "28/28 - 3s - loss: 0.6799 - acc: 0.5654 - f1_score: 0.5406 - val_loss: 0.6531 - val_acc: 0.7055 - val_f1_score: 0.6516\n",
      "Epoch 83/300\n",
      "28/28 - 3s - loss: 0.6798 - acc: 0.5658 - f1_score: 0.5404 - val_loss: 0.6525 - val_acc: 0.7099 - val_f1_score: 0.6554\n",
      "Epoch 84/300\n",
      "28/28 - 3s - loss: 0.6797 - acc: 0.5649 - f1_score: 0.5389 - val_loss: 0.6521 - val_acc: 0.7077 - val_f1_score: 0.6520\n",
      "Epoch 85/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5658 - f1_score: 0.5395 - val_loss: 0.6520 - val_acc: 0.7099 - val_f1_score: 0.6554\n",
      "Epoch 86/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5655 - f1_score: 0.5394 - val_loss: 0.6519 - val_acc: 0.7077 - val_f1_score: 0.6535\n",
      "Epoch 87/300\n",
      "28/28 - 3s - loss: 0.6795 - acc: 0.5661 - f1_score: 0.5408 - val_loss: 0.6519 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 88/300\n",
      "28/28 - 3s - loss: 0.6794 - acc: 0.5655 - f1_score: 0.5408 - val_loss: 0.6516 - val_acc: 0.7033 - val_f1_score: 0.6497\n",
      "Epoch 89/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5659 - f1_score: 0.5404 - val_loss: 0.6511 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 90/300\n",
      "28/28 - 3s - loss: 0.6792 - acc: 0.5659 - f1_score: 0.5397 - val_loss: 0.6505 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 91/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5669 - f1_score: 0.5395 - val_loss: 0.6506 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 92/300\n",
      "28/28 - 3s - loss: 0.6790 - acc: 0.5655 - f1_score: 0.5394 - val_loss: 0.6503 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 93/300\n",
      "28/28 - 3s - loss: 0.6789 - acc: 0.5664 - f1_score: 0.5400 - val_loss: 0.6504 - val_acc: 0.7077 - val_f1_score: 0.6535\n",
      "Epoch 94/300\n",
      "28/28 - 3s - loss: 0.6789 - acc: 0.5655 - f1_score: 0.5399 - val_loss: 0.6505 - val_acc: 0.7055 - val_f1_score: 0.6516\n",
      "Epoch 95/300\n",
      "28/28 - 3s - loss: 0.6788 - acc: 0.5659 - f1_score: 0.5403 - val_loss: 0.6502 - val_acc: 0.7077 - val_f1_score: 0.6535\n",
      "Epoch 96/300\n",
      "28/28 - 3s - loss: 0.6787 - acc: 0.5656 - f1_score: 0.5397 - val_loss: 0.6503 - val_acc: 0.7011 - val_f1_score: 0.6478\n",
      "Epoch 97/300\n",
      "28/28 - 3s - loss: 0.6787 - acc: 0.5662 - f1_score: 0.5415 - val_loss: 0.6497 - val_acc: 0.7055 - val_f1_score: 0.6501\n",
      "Epoch 98/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5669 - f1_score: 0.5409 - val_loss: 0.6491 - val_acc: 0.7099 - val_f1_score: 0.6539\n",
      "Epoch 99/300\n",
      "28/28 - 3s - loss: 0.6785 - acc: 0.5678 - f1_score: 0.5404 - val_loss: 0.6489 - val_acc: 0.7077 - val_f1_score: 0.6520\n",
      "Epoch 100/300\n",
      "28/28 - 3s - loss: 0.6784 - acc: 0.5678 - f1_score: 0.5406 - val_loss: 0.6484 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 101/300\n",
      "28/28 - 3s - loss: 0.6783 - acc: 0.5678 - f1_score: 0.5402 - val_loss: 0.6482 - val_acc: 0.7099 - val_f1_score: 0.6539\n",
      "Epoch 102/300\n",
      "28/28 - 3s - loss: 0.6782 - acc: 0.5684 - f1_score: 0.5412 - val_loss: 0.6478 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 103/300\n",
      "28/28 - 3s - loss: 0.6781 - acc: 0.5688 - f1_score: 0.5405 - val_loss: 0.6476 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 104/300\n",
      "28/28 - 3s - loss: 0.6780 - acc: 0.5691 - f1_score: 0.5412 - val_loss: 0.6476 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 105/300\n",
      "28/28 - 3s - loss: 0.6779 - acc: 0.5680 - f1_score: 0.5411 - val_loss: 0.6475 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 106/300\n",
      "28/28 - 3s - loss: 0.6779 - acc: 0.5693 - f1_score: 0.5418 - val_loss: 0.6475 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 107/300\n",
      "28/28 - 3s - loss: 0.6778 - acc: 0.5684 - f1_score: 0.5414 - val_loss: 0.6470 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 108/300\n",
      "28/28 - 3s - loss: 0.6777 - acc: 0.5691 - f1_score: 0.5417 - val_loss: 0.6469 - val_acc: 0.7143 - val_f1_score: 0.6577\n",
      "Epoch 109/300\n",
      "28/28 - 3s - loss: 0.6776 - acc: 0.5691 - f1_score: 0.5417 - val_loss: 0.6460 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 110/300\n",
      "28/28 - 3s - loss: 0.6775 - acc: 0.5701 - f1_score: 0.5408 - val_loss: 0.6457 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 111/300\n",
      "28/28 - 3s - loss: 0.6774 - acc: 0.5698 - f1_score: 0.5397 - val_loss: 0.6457 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 112/300\n",
      "28/28 - 3s - loss: 0.6774 - acc: 0.5706 - f1_score: 0.5420 - val_loss: 0.6453 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 113/300\n",
      "28/28 - 3s - loss: 0.6773 - acc: 0.5706 - f1_score: 0.5408 - val_loss: 0.6455 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 114/300\n",
      "28/28 - 3s - loss: 0.6772 - acc: 0.5709 - f1_score: 0.5419 - val_loss: 0.6450 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 115/300\n",
      "28/28 - 3s - loss: 0.6771 - acc: 0.5707 - f1_score: 0.5407 - val_loss: 0.6449 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 116/300\n",
      "28/28 - 3s - loss: 0.6770 - acc: 0.5703 - f1_score: 0.5389 - val_loss: 0.6446 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 117/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5706 - f1_score: 0.5405 - val_loss: 0.6445 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 118/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5711 - f1_score: 0.5402 - val_loss: 0.6446 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 119/300\n",
      "28/28 - 3s - loss: 0.6768 - acc: 0.5707 - f1_score: 0.5407 - val_loss: 0.6443 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Epoch 120/300\n",
      "28/28 - 3s - loss: 0.6767 - acc: 0.5706 - f1_score: 0.5403 - val_loss: 0.6441 - val_acc: 0.7165 - val_f1_score: 0.6596\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00120: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.56      0.52       126\n",
      "           1       0.82      0.78      0.80       329\n",
      "\n",
      "    accuracy                           0.72       455\n",
      "   macro avg       0.66      0.67      0.66       455\n",
      "weighted avg       0.73      0.72      0.72       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1517\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6950 - acc: 0.4426 - f1_score: 0.3068 - val_loss: 0.6749 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6942 - acc: 0.4422 - f1_score: 0.3079 - val_loss: 0.6783 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6934 - acc: 0.4503 - f1_score: 0.3331 - val_loss: 0.6813 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4771 - f1_score: 0.4041 - val_loss: 0.6842 - val_acc: 0.8632 - val_f1_score: 0.4633\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.5177 - f1_score: 0.5004 - val_loss: 0.6870 - val_acc: 0.6353 - val_f1_score: 0.4029\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.5605 - f1_score: 0.5590 - val_loss: 0.6895 - val_acc: 0.4245 - val_f1_score: 0.3138\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5768 - f1_score: 0.5635 - val_loss: 0.6920 - val_acc: 0.3305 - val_f1_score: 0.2687\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5833 - f1_score: 0.5602 - val_loss: 0.6942 - val_acc: 0.3077 - val_f1_score: 0.2590\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6891 - acc: 0.5842 - f1_score: 0.5552 - val_loss: 0.6963 - val_acc: 0.2906 - val_f1_score: 0.2517\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5843 - f1_score: 0.5515 - val_loss: 0.6982 - val_acc: 0.2821 - val_f1_score: 0.2458\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5859 - f1_score: 0.5506 - val_loss: 0.7000 - val_acc: 0.2707 - val_f1_score: 0.2377\n",
      "Epoch 12/300\n",
      "28/28 - 4s - loss: 0.6873 - acc: 0.5856 - f1_score: 0.5490 - val_loss: 0.7020 - val_acc: 0.2707 - val_f1_score: 0.2395\n",
      "Epoch 13/300\n",
      "28/28 - 4s - loss: 0.6867 - acc: 0.5862 - f1_score: 0.5484 - val_loss: 0.7037 - val_acc: 0.2707 - val_f1_score: 0.2395\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5870 - f1_score: 0.5482 - val_loss: 0.7052 - val_acc: 0.2707 - val_f1_score: 0.2395\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5875 - f1_score: 0.5486 - val_loss: 0.7071 - val_acc: 0.2678 - val_f1_score: 0.2375\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5885 - f1_score: 0.5477 - val_loss: 0.7091 - val_acc: 0.2678 - val_f1_score: 0.2375\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5885 - f1_score: 0.5481 - val_loss: 0.7108 - val_acc: 0.2678 - val_f1_score: 0.2375\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5889 - f1_score: 0.5480 - val_loss: 0.7126 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5893 - f1_score: 0.5484 - val_loss: 0.7142 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5890 - f1_score: 0.5485 - val_loss: 0.7160 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5909 - f1_score: 0.5487 - val_loss: 0.7177 - val_acc: 0.2707 - val_f1_score: 0.2413\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       351\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       351\n",
      "   macro avg       0.50      0.46      0.48       351\n",
      "weighted avg       1.00      0.93      0.96       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1544\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "29/29 - 13s - loss: 0.6940 - acc: 0.4669 - f1_score: 0.3183 - val_loss: 0.6929 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/300\n",
      "29/29 - 3s - loss: 0.6934 - acc: 0.4669 - f1_score: 0.3187 - val_loss: 0.6908 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 3/300\n",
      "29/29 - 3s - loss: 0.6929 - acc: 0.4676 - f1_score: 0.3244 - val_loss: 0.6887 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 4/300\n",
      "29/29 - 3s - loss: 0.6923 - acc: 0.4793 - f1_score: 0.3559 - val_loss: 0.6865 - val_acc: 0.4038 - val_f1_score: 0.3227\n",
      "Epoch 5/300\n",
      "29/29 - 3s - loss: 0.6917 - acc: 0.4983 - f1_score: 0.4076 - val_loss: 0.6842 - val_acc: 0.5481 - val_f1_score: 0.5385\n",
      "Epoch 6/300\n",
      "29/29 - 3s - loss: 0.6912 - acc: 0.5145 - f1_score: 0.4635 - val_loss: 0.6819 - val_acc: 0.6827 - val_f1_score: 0.6820\n",
      "Epoch 7/300\n",
      "29/29 - 3s - loss: 0.6907 - acc: 0.5387 - f1_score: 0.5269 - val_loss: 0.6796 - val_acc: 0.7596 - val_f1_score: 0.7545\n",
      "Epoch 8/300\n",
      "29/29 - 3s - loss: 0.6901 - acc: 0.5530 - f1_score: 0.5528 - val_loss: 0.6773 - val_acc: 0.7788 - val_f1_score: 0.7712\n",
      "Epoch 9/300\n",
      "29/29 - 3s - loss: 0.6896 - acc: 0.5595 - f1_score: 0.5575 - val_loss: 0.6750 - val_acc: 0.7981 - val_f1_score: 0.7895\n",
      "Epoch 10/300\n",
      "29/29 - 3s - loss: 0.6891 - acc: 0.5579 - f1_score: 0.5523 - val_loss: 0.6727 - val_acc: 0.7885 - val_f1_score: 0.7786\n",
      "Epoch 11/300\n",
      "29/29 - 3s - loss: 0.6887 - acc: 0.5605 - f1_score: 0.5522 - val_loss: 0.6705 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 12/300\n",
      "29/29 - 3s - loss: 0.6882 - acc: 0.5606 - f1_score: 0.5494 - val_loss: 0.6683 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 13/300\n",
      "29/29 - 3s - loss: 0.6878 - acc: 0.5599 - f1_score: 0.5476 - val_loss: 0.6662 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 14/300\n",
      "29/29 - 3s - loss: 0.6874 - acc: 0.5601 - f1_score: 0.5458 - val_loss: 0.6641 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 15/300\n",
      "29/29 - 3s - loss: 0.6870 - acc: 0.5614 - f1_score: 0.5463 - val_loss: 0.6620 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 16/300\n",
      "29/29 - 3s - loss: 0.6866 - acc: 0.5613 - f1_score: 0.5453 - val_loss: 0.6602 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 17/300\n",
      "29/29 - 3s - loss: 0.6862 - acc: 0.5598 - f1_score: 0.5429 - val_loss: 0.6584 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 18/300\n",
      "29/29 - 3s - loss: 0.6859 - acc: 0.5587 - f1_score: 0.5411 - val_loss: 0.6565 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 19/300\n",
      "29/29 - 3s - loss: 0.6855 - acc: 0.5595 - f1_score: 0.5411 - val_loss: 0.6547 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 20/300\n",
      "29/29 - 3s - loss: 0.6852 - acc: 0.5603 - f1_score: 0.5412 - val_loss: 0.6530 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 21/300\n",
      "29/29 - 3s - loss: 0.6849 - acc: 0.5608 - f1_score: 0.5413 - val_loss: 0.6513 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 22/300\n",
      "29/29 - 3s - loss: 0.6846 - acc: 0.5617 - f1_score: 0.5414 - val_loss: 0.6497 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 23/300\n",
      "29/29 - 3s - loss: 0.6844 - acc: 0.5630 - f1_score: 0.5421 - val_loss: 0.6481 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 24/300\n",
      "29/29 - 3s - loss: 0.6841 - acc: 0.5641 - f1_score: 0.5426 - val_loss: 0.6467 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 25/300\n",
      "29/29 - 3s - loss: 0.6838 - acc: 0.5639 - f1_score: 0.5419 - val_loss: 0.6453 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 26/300\n",
      "29/29 - 3s - loss: 0.6836 - acc: 0.5653 - f1_score: 0.5426 - val_loss: 0.6439 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 27/300\n",
      "29/29 - 3s - loss: 0.6833 - acc: 0.5668 - f1_score: 0.5434 - val_loss: 0.6425 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 28/300\n",
      "29/29 - 3s - loss: 0.6831 - acc: 0.5664 - f1_score: 0.5423 - val_loss: 0.6412 - val_acc: 0.7885 - val_f1_score: 0.7719\n",
      "Epoch 29/300\n",
      "29/29 - 3s - loss: 0.6829 - acc: 0.5668 - f1_score: 0.5426 - val_loss: 0.6400 - val_acc: 0.7981 - val_f1_score: 0.7811\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.75        44\n",
      "           1       0.80      0.87      0.83        60\n",
      "\n",
      "    accuracy                           0.80       104\n",
      "   macro avg       0.80      0.79      0.79       104\n",
      "weighted avg       0.80      0.80      0.80       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1624\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 11s - loss: 0.6945 - acc: 0.4556 - f1_score: 0.3130 - val_loss: 0.6851 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6939 - acc: 0.4550 - f1_score: 0.3132 - val_loss: 0.6857 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4561 - f1_score: 0.3186 - val_loss: 0.6862 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4687 - f1_score: 0.3517 - val_loss: 0.6866 - val_acc: 0.6667 - val_f1_score: 0.4081\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6920 - acc: 0.4927 - f1_score: 0.4179 - val_loss: 0.6871 - val_acc: 0.6268 - val_f1_score: 0.4124\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5214 - f1_score: 0.4943 - val_loss: 0.6875 - val_acc: 0.5726 - val_f1_score: 0.4818\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5494 - f1_score: 0.5478 - val_loss: 0.6878 - val_acc: 0.5071 - val_f1_score: 0.4879\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5678 - f1_score: 0.5645 - val_loss: 0.6881 - val_acc: 0.4587 - val_f1_score: 0.4574\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5691 - f1_score: 0.5600 - val_loss: 0.6883 - val_acc: 0.4786 - val_f1_score: 0.4786\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5717 - f1_score: 0.5580 - val_loss: 0.6884 - val_acc: 0.4644 - val_f1_score: 0.4640\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5709 - f1_score: 0.5548 - val_loss: 0.6886 - val_acc: 0.4701 - val_f1_score: 0.4691\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6882 - acc: 0.5702 - f1_score: 0.5514 - val_loss: 0.6887 - val_acc: 0.4672 - val_f1_score: 0.4658\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5707 - f1_score: 0.5504 - val_loss: 0.6888 - val_acc: 0.4672 - val_f1_score: 0.4655\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5712 - f1_score: 0.5498 - val_loss: 0.6889 - val_acc: 0.4644 - val_f1_score: 0.4621\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5718 - f1_score: 0.5493 - val_loss: 0.6891 - val_acc: 0.4672 - val_f1_score: 0.4633\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5739 - f1_score: 0.5497 - val_loss: 0.6893 - val_acc: 0.4615 - val_f1_score: 0.4570\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5746 - f1_score: 0.5500 - val_loss: 0.6894 - val_acc: 0.4644 - val_f1_score: 0.4596\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5736 - f1_score: 0.5486 - val_loss: 0.6896 - val_acc: 0.4672 - val_f1_score: 0.4622\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5762 - f1_score: 0.5502 - val_loss: 0.6897 - val_acc: 0.4558 - val_f1_score: 0.4494\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5756 - f1_score: 0.5496 - val_loss: 0.6900 - val_acc: 0.4587 - val_f1_score: 0.4519\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5768 - f1_score: 0.5494 - val_loss: 0.6902 - val_acc: 0.4587 - val_f1_score: 0.4519\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5771 - f1_score: 0.5495 - val_loss: 0.6904 - val_acc: 0.4587 - val_f1_score: 0.4519\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5765 - f1_score: 0.5483 - val_loss: 0.6907 - val_acc: 0.4558 - val_f1_score: 0.4487\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5762 - f1_score: 0.5478 - val_loss: 0.6911 - val_acc: 0.4530 - val_f1_score: 0.4447\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5765 - f1_score: 0.5472 - val_loss: 0.6914 - val_acc: 0.4530 - val_f1_score: 0.4447\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5765 - f1_score: 0.5472 - val_loss: 0.6919 - val_acc: 0.4558 - val_f1_score: 0.4463\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5772 - f1_score: 0.5469 - val_loss: 0.6922 - val_acc: 0.4558 - val_f1_score: 0.4463\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029387887940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59       185\n",
      "           1       0.47      0.33      0.39       166\n",
      "\n",
      "    accuracy                           0.51       351\n",
      "   macro avg       0.50      0.50      0.49       351\n",
      "weighted avg       0.50      0.51      0.49       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1674\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6929 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7191 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7192 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.4852 - f1_score: 0.3270 - val_loss: 0.7194 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6913 - acc: 0.4852 - f1_score: 0.3270 - val_loss: 0.7197 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6908 - acc: 0.4851 - f1_score: 0.3276 - val_loss: 0.7200 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6902 - acc: 0.4872 - f1_score: 0.3360 - val_loss: 0.7204 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.4959 - f1_score: 0.3581 - val_loss: 0.7209 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6891 - acc: 0.5081 - f1_score: 0.3893 - val_loss: 0.7214 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5206 - f1_score: 0.4218 - val_loss: 0.7218 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5326 - f1_score: 0.4557 - val_loss: 0.7222 - val_acc: 0.0492 - val_f1_score: 0.0483\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5480 - f1_score: 0.5004 - val_loss: 0.7226 - val_acc: 0.0708 - val_f1_score: 0.0707\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5599 - f1_score: 0.5341 - val_loss: 0.7228 - val_acc: 0.1108 - val_f1_score: 0.1099\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5714 - f1_score: 0.5630 - val_loss: 0.7233 - val_acc: 0.1631 - val_f1_score: 0.1573\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5724 - f1_score: 0.5705 - val_loss: 0.7236 - val_acc: 0.2462 - val_f1_score: 0.2248\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5714 - f1_score: 0.5711 - val_loss: 0.7235 - val_acc: 0.3446 - val_f1_score: 0.2953\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5683 - f1_score: 0.5682 - val_loss: 0.7237 - val_acc: 0.4000 - val_f1_score: 0.3316\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5644 - f1_score: 0.5640 - val_loss: 0.7235 - val_acc: 0.4338 - val_f1_score: 0.3528\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5644 - f1_score: 0.5632 - val_loss: 0.7233 - val_acc: 0.4431 - val_f1_score: 0.3585\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5654 - f1_score: 0.5636 - val_loss: 0.7231 - val_acc: 0.4585 - val_f1_score: 0.3679\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5666 - f1_score: 0.5642 - val_loss: 0.7227 - val_acc: 0.4800 - val_f1_score: 0.3810\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5651 - f1_score: 0.5622 - val_loss: 0.7225 - val_acc: 0.4923 - val_f1_score: 0.3883\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5634 - f1_score: 0.5599 - val_loss: 0.7219 - val_acc: 0.5108 - val_f1_score: 0.3993\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5619 - f1_score: 0.5576 - val_loss: 0.7212 - val_acc: 0.5292 - val_f1_score: 0.4102\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5619 - f1_score: 0.5570 - val_loss: 0.7205 - val_acc: 0.5323 - val_f1_score: 0.4120\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5613 - f1_score: 0.5560 - val_loss: 0.7196 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5630 - f1_score: 0.5572 - val_loss: 0.7190 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5624 - f1_score: 0.5560 - val_loss: 0.7183 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5644 - f1_score: 0.5577 - val_loss: 0.7175 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5659 - f1_score: 0.5587 - val_loss: 0.7166 - val_acc: 0.5662 - val_f1_score: 0.4319\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5661 - f1_score: 0.5587 - val_loss: 0.7161 - val_acc: 0.5662 - val_f1_score: 0.4319\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5657 - f1_score: 0.5579 - val_loss: 0.7153 - val_acc: 0.5662 - val_f1_score: 0.4319\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5650 - f1_score: 0.5568 - val_loss: 0.7143 - val_acc: 0.5662 - val_f1_score: 0.4319\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5653 - f1_score: 0.5569 - val_loss: 0.7133 - val_acc: 0.5692 - val_f1_score: 0.4337\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5644 - f1_score: 0.5556 - val_loss: 0.7118 - val_acc: 0.5723 - val_f1_score: 0.4355\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5640 - f1_score: 0.5546 - val_loss: 0.7105 - val_acc: 0.5877 - val_f1_score: 0.4445\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6811 - acc: 0.5637 - f1_score: 0.5539 - val_loss: 0.7098 - val_acc: 0.5908 - val_f1_score: 0.4463\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6810 - acc: 0.5632 - f1_score: 0.5531 - val_loss: 0.7086 - val_acc: 0.5908 - val_f1_score: 0.4463\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6808 - acc: 0.5627 - f1_score: 0.5522 - val_loss: 0.7077 - val_acc: 0.6000 - val_f1_score: 0.4518\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5626 - f1_score: 0.5515 - val_loss: 0.7073 - val_acc: 0.6031 - val_f1_score: 0.4536\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6805 - acc: 0.5626 - f1_score: 0.5513 - val_loss: 0.7070 - val_acc: 0.6031 - val_f1_score: 0.4536\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5626 - f1_score: 0.5514 - val_loss: 0.7063 - val_acc: 0.6062 - val_f1_score: 0.4554\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6803 - acc: 0.5630 - f1_score: 0.5513 - val_loss: 0.7058 - val_acc: 0.6123 - val_f1_score: 0.4590\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6801 - acc: 0.5624 - f1_score: 0.5506 - val_loss: 0.7056 - val_acc: 0.6123 - val_f1_score: 0.4590\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6800 - acc: 0.5634 - f1_score: 0.5513 - val_loss: 0.7055 - val_acc: 0.6123 - val_f1_score: 0.4590\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6799 - acc: 0.5630 - f1_score: 0.5510 - val_loss: 0.7041 - val_acc: 0.6123 - val_f1_score: 0.4590\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6797 - acc: 0.5640 - f1_score: 0.5516 - val_loss: 0.7035 - val_acc: 0.6185 - val_f1_score: 0.4627\n",
      "Epoch 47/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5649 - f1_score: 0.5523 - val_loss: 0.7027 - val_acc: 0.6215 - val_f1_score: 0.4645\n",
      "Epoch 48/300\n",
      "28/28 - 3s - loss: 0.6795 - acc: 0.5643 - f1_score: 0.5516 - val_loss: 0.7020 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 49/300\n",
      "28/28 - 3s - loss: 0.6794 - acc: 0.5649 - f1_score: 0.5520 - val_loss: 0.7014 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 50/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5654 - f1_score: 0.5525 - val_loss: 0.7007 - val_acc: 0.6277 - val_f1_score: 0.4682\n",
      "Epoch 51/300\n",
      "28/28 - 3s - loss: 0.6792 - acc: 0.5664 - f1_score: 0.5529 - val_loss: 0.7006 - val_acc: 0.6277 - val_f1_score: 0.4682\n",
      "Epoch 52/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5666 - f1_score: 0.5532 - val_loss: 0.6998 - val_acc: 0.6277 - val_f1_score: 0.4682\n",
      "Epoch 53/300\n",
      "28/28 - 3s - loss: 0.6789 - acc: 0.5673 - f1_score: 0.5537 - val_loss: 0.6990 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 54/300\n",
      "28/28 - 3s - loss: 0.6788 - acc: 0.5673 - f1_score: 0.5537 - val_loss: 0.6975 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 55/300\n",
      "28/28 - 3s - loss: 0.6787 - acc: 0.5680 - f1_score: 0.5538 - val_loss: 0.6965 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 56/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5687 - f1_score: 0.5541 - val_loss: 0.6962 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 57/300\n",
      "28/28 - 3s - loss: 0.6785 - acc: 0.5687 - f1_score: 0.5543 - val_loss: 0.6952 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 58/300\n",
      "28/28 - 3s - loss: 0.6784 - acc: 0.5701 - f1_score: 0.5551 - val_loss: 0.6953 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 59/300\n",
      "28/28 - 3s - loss: 0.6783 - acc: 0.5694 - f1_score: 0.5544 - val_loss: 0.6955 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 60/300\n",
      "28/28 - 3s - loss: 0.6782 - acc: 0.5697 - f1_score: 0.5548 - val_loss: 0.6955 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 61/300\n",
      "28/28 - 3s - loss: 0.6781 - acc: 0.5701 - f1_score: 0.5553 - val_loss: 0.6947 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 62/300\n",
      "28/28 - 3s - loss: 0.6780 - acc: 0.5705 - f1_score: 0.5554 - val_loss: 0.6949 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 63/300\n",
      "28/28 - 3s - loss: 0.6779 - acc: 0.5704 - f1_score: 0.5555 - val_loss: 0.6948 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 64/300\n",
      "28/28 - 3s - loss: 0.6778 - acc: 0.5715 - f1_score: 0.5562 - val_loss: 0.6952 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 65/300\n",
      "28/28 - 3s - loss: 0.6777 - acc: 0.5703 - f1_score: 0.5554 - val_loss: 0.6941 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 66/300\n",
      "28/28 - 3s - loss: 0.6776 - acc: 0.5717 - f1_score: 0.5564 - val_loss: 0.6939 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 67/300\n",
      "28/28 - 3s - loss: 0.6775 - acc: 0.5718 - f1_score: 0.5562 - val_loss: 0.6940 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 68/300\n",
      "28/28 - 3s - loss: 0.6775 - acc: 0.5728 - f1_score: 0.5573 - val_loss: 0.6938 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 69/300\n",
      "28/28 - 3s - loss: 0.6774 - acc: 0.5725 - f1_score: 0.5569 - val_loss: 0.6929 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 70/300\n",
      "28/28 - 3s - loss: 0.6773 - acc: 0.5728 - f1_score: 0.5571 - val_loss: 0.6927 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 71/300\n",
      "28/28 - 3s - loss: 0.6772 - acc: 0.5727 - f1_score: 0.5570 - val_loss: 0.6931 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 72/300\n",
      "28/28 - 3s - loss: 0.6771 - acc: 0.5732 - f1_score: 0.5578 - val_loss: 0.6913 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 73/300\n",
      "28/28 - 3s - loss: 0.6770 - acc: 0.5728 - f1_score: 0.5569 - val_loss: 0.6918 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 74/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5728 - f1_score: 0.5569 - val_loss: 0.6923 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 75/300\n",
      "28/28 - 3s - loss: 0.6768 - acc: 0.5729 - f1_score: 0.5573 - val_loss: 0.6913 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 76/300\n",
      "28/28 - 3s - loss: 0.6767 - acc: 0.5734 - f1_score: 0.5573 - val_loss: 0.6916 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 77/300\n",
      "28/28 - 4s - loss: 0.6766 - acc: 0.5742 - f1_score: 0.5587 - val_loss: 0.6908 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 78/300\n",
      "28/28 - 4s - loss: 0.6766 - acc: 0.5739 - f1_score: 0.5580 - val_loss: 0.6910 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 79/300\n",
      "28/28 - 3s - loss: 0.6765 - acc: 0.5747 - f1_score: 0.5587 - val_loss: 0.6919 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 80/300\n",
      "28/28 - 3s - loss: 0.6764 - acc: 0.5741 - f1_score: 0.5583 - val_loss: 0.6917 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 81/300\n",
      "28/28 - 3s - loss: 0.6763 - acc: 0.5741 - f1_score: 0.5581 - val_loss: 0.6921 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 82/300\n",
      "28/28 - 3s - loss: 0.6762 - acc: 0.5742 - f1_score: 0.5585 - val_loss: 0.6915 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 83/300\n",
      "28/28 - 3s - loss: 0.6761 - acc: 0.5748 - f1_score: 0.5590 - val_loss: 0.6916 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 84/300\n",
      "28/28 - 3s - loss: 0.6761 - acc: 0.5754 - f1_score: 0.5593 - val_loss: 0.6919 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Epoch 85/300\n",
      "28/28 - 3s - loss: 0.6760 - acc: 0.5749 - f1_score: 0.5590 - val_loss: 0.6912 - val_acc: 0.6369 - val_f1_score: 0.4737\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00085: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000293BDEF7CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18       131\n",
      "           1       0.62      1.00      0.77       194\n",
      "\n",
      "    accuracy                           0.64       325\n",
      "   macro avg       0.81      0.55      0.47       325\n",
      "weighted avg       0.77      0.64      0.53       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1688\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 11s - loss: 0.6942 - acc: 0.4618 - f1_score: 0.3159 - val_loss: 0.6897 - val_acc: 0.5333 - val_f1_score: 0.3478\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6937 - acc: 0.4615 - f1_score: 0.3158 - val_loss: 0.6891 - val_acc: 0.5385 - val_f1_score: 0.3692\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4620 - f1_score: 0.3208 - val_loss: 0.6883 - val_acc: 0.5641 - val_f1_score: 0.4494\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4719 - f1_score: 0.3460 - val_loss: 0.6874 - val_acc: 0.6128 - val_f1_score: 0.5787\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6922 - acc: 0.4896 - f1_score: 0.3946 - val_loss: 0.6865 - val_acc: 0.6692 - val_f1_score: 0.6625\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6917 - acc: 0.5094 - f1_score: 0.4579 - val_loss: 0.6856 - val_acc: 0.6897 - val_f1_score: 0.6888\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.5312 - f1_score: 0.5198 - val_loss: 0.6847 - val_acc: 0.6923 - val_f1_score: 0.6923\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6908 - acc: 0.5504 - f1_score: 0.5503 - val_loss: 0.6837 - val_acc: 0.6872 - val_f1_score: 0.6868\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5591 - f1_score: 0.5538 - val_loss: 0.6828 - val_acc: 0.6897 - val_f1_score: 0.6885\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5562 - f1_score: 0.5459 - val_loss: 0.6818 - val_acc: 0.6897 - val_f1_score: 0.6872\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6895 - acc: 0.5608 - f1_score: 0.5463 - val_loss: 0.6806 - val_acc: 0.6923 - val_f1_score: 0.6894\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6890 - acc: 0.5589 - f1_score: 0.5424 - val_loss: 0.6799 - val_acc: 0.6949 - val_f1_score: 0.6908\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5611 - f1_score: 0.5409 - val_loss: 0.6790 - val_acc: 0.6923 - val_f1_score: 0.6876\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5615 - f1_score: 0.5404 - val_loss: 0.6780 - val_acc: 0.6923 - val_f1_score: 0.6872\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5615 - f1_score: 0.5392 - val_loss: 0.6768 - val_acc: 0.6923 - val_f1_score: 0.6867\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5612 - f1_score: 0.5381 - val_loss: 0.6760 - val_acc: 0.6949 - val_f1_score: 0.6891\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5631 - f1_score: 0.5383 - val_loss: 0.6749 - val_acc: 0.6949 - val_f1_score: 0.6891\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5640 - f1_score: 0.5393 - val_loss: 0.6739 - val_acc: 0.6949 - val_f1_score: 0.6891\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6865 - acc: 0.5645 - f1_score: 0.5391 - val_loss: 0.6728 - val_acc: 0.6923 - val_f1_score: 0.6863\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5642 - f1_score: 0.5390 - val_loss: 0.6722 - val_acc: 0.6923 - val_f1_score: 0.6858\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5642 - f1_score: 0.5376 - val_loss: 0.6716 - val_acc: 0.6923 - val_f1_score: 0.6858\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5650 - f1_score: 0.5369 - val_loss: 0.6708 - val_acc: 0.6923 - val_f1_score: 0.6853\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5655 - f1_score: 0.5379 - val_loss: 0.6701 - val_acc: 0.6923 - val_f1_score: 0.6853\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5638 - f1_score: 0.5350 - val_loss: 0.6693 - val_acc: 0.6949 - val_f1_score: 0.6877\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5634 - f1_score: 0.5341 - val_loss: 0.6688 - val_acc: 0.6923 - val_f1_score: 0.6848\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5640 - f1_score: 0.5339 - val_loss: 0.6683 - val_acc: 0.6923 - val_f1_score: 0.6848\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5640 - f1_score: 0.5331 - val_loss: 0.6678 - val_acc: 0.6923 - val_f1_score: 0.6848\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000293C1E87550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69       180\n",
      "           1       0.75      0.65      0.69       210\n",
      "\n",
      "    accuracy                           0.69       390\n",
      "   macro avg       0.70      0.70      0.69       390\n",
      "weighted avg       0.70      0.69      0.69       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1717\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6939 - acc: 0.4663 - f1_score: 0.3180 - val_loss: 0.6965 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4660 - f1_score: 0.3181 - val_loss: 0.6969 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6926 - acc: 0.4677 - f1_score: 0.3243 - val_loss: 0.6974 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.4796 - f1_score: 0.3589 - val_loss: 0.6980 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4998 - f1_score: 0.4122 - val_loss: 0.6986 - val_acc: 0.4530 - val_f1_score: 0.3308\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6906 - acc: 0.5152 - f1_score: 0.4660 - val_loss: 0.6994 - val_acc: 0.5000 - val_f1_score: 0.4493\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5401 - f1_score: 0.5273 - val_loss: 0.7002 - val_acc: 0.5556 - val_f1_score: 0.5448\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6893 - acc: 0.5547 - f1_score: 0.5545 - val_loss: 0.7011 - val_acc: 0.5598 - val_f1_score: 0.5585\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5617 - f1_score: 0.5594 - val_loss: 0.7020 - val_acc: 0.5256 - val_f1_score: 0.5254\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5616 - f1_score: 0.5564 - val_loss: 0.7028 - val_acc: 0.4957 - val_f1_score: 0.4934\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5641 - f1_score: 0.5566 - val_loss: 0.7037 - val_acc: 0.4872 - val_f1_score: 0.4834\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5652 - f1_score: 0.5559 - val_loss: 0.7045 - val_acc: 0.4829 - val_f1_score: 0.4769\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5651 - f1_score: 0.5538 - val_loss: 0.7054 - val_acc: 0.4872 - val_f1_score: 0.4786\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5658 - f1_score: 0.5537 - val_loss: 0.7062 - val_acc: 0.4872 - val_f1_score: 0.4774\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5645 - f1_score: 0.5511 - val_loss: 0.7070 - val_acc: 0.4786 - val_f1_score: 0.4660\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5647 - f1_score: 0.5506 - val_loss: 0.7078 - val_acc: 0.4701 - val_f1_score: 0.4557\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5649 - f1_score: 0.5503 - val_loss: 0.7086 - val_acc: 0.4658 - val_f1_score: 0.4489\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5659 - f1_score: 0.5500 - val_loss: 0.7093 - val_acc: 0.4573 - val_f1_score: 0.4364\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5656 - f1_score: 0.5493 - val_loss: 0.7099 - val_acc: 0.4573 - val_f1_score: 0.4344\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5654 - f1_score: 0.5484 - val_loss: 0.7106 - val_acc: 0.4615 - val_f1_score: 0.4379\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5641 - f1_score: 0.5461 - val_loss: 0.7112 - val_acc: 0.4658 - val_f1_score: 0.4413\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5645 - f1_score: 0.5462 - val_loss: 0.7117 - val_acc: 0.4744 - val_f1_score: 0.4481\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5658 - f1_score: 0.5466 - val_loss: 0.7123 - val_acc: 0.4615 - val_f1_score: 0.4313\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5662 - f1_score: 0.5466 - val_loss: 0.7127 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5680 - f1_score: 0.5480 - val_loss: 0.7132 - val_acc: 0.4487 - val_f1_score: 0.4139\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6815 - acc: 0.5686 - f1_score: 0.5481 - val_loss: 0.7135 - val_acc: 0.4444 - val_f1_score: 0.4081\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6812 - acc: 0.5700 - f1_score: 0.5487 - val_loss: 0.7140 - val_acc: 0.4444 - val_f1_score: 0.4053\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6809 - acc: 0.5698 - f1_score: 0.5484 - val_loss: 0.7143 - val_acc: 0.4402 - val_f1_score: 0.3994\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.50      0.58       143\n",
      "           1       0.45      0.65      0.53        91\n",
      "\n",
      "    accuracy                           0.56       234\n",
      "   macro avg       0.57      0.58      0.56       234\n",
      "weighted avg       0.60      0.56      0.56       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1818\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 11s - loss: 0.6937 - acc: 0.4727 - f1_score: 0.3210 - val_loss: 0.6987 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6932 - acc: 0.4725 - f1_score: 0.3209 - val_loss: 0.6977 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6927 - acc: 0.4728 - f1_score: 0.3218 - val_loss: 0.6966 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4739 - f1_score: 0.3258 - val_loss: 0.6955 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.4818 - f1_score: 0.3499 - val_loss: 0.6945 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6913 - acc: 0.4967 - f1_score: 0.3883 - val_loss: 0.6935 - val_acc: 0.3675 - val_f1_score: 0.2765\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6909 - acc: 0.5101 - f1_score: 0.4243 - val_loss: 0.6925 - val_acc: 0.3718 - val_f1_score: 0.2933\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.5264 - f1_score: 0.4756 - val_loss: 0.6915 - val_acc: 0.3932 - val_f1_score: 0.3534\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6899 - acc: 0.5424 - f1_score: 0.5209 - val_loss: 0.6905 - val_acc: 0.4487 - val_f1_score: 0.4442\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6895 - acc: 0.5553 - f1_score: 0.5503 - val_loss: 0.6896 - val_acc: 0.5235 - val_f1_score: 0.5225\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5608 - f1_score: 0.5607 - val_loss: 0.6887 - val_acc: 0.5534 - val_f1_score: 0.5462\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5583 - f1_score: 0.5574 - val_loss: 0.6878 - val_acc: 0.5940 - val_f1_score: 0.5812\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5577 - f1_score: 0.5549 - val_loss: 0.6869 - val_acc: 0.6218 - val_f1_score: 0.6033\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5574 - f1_score: 0.5529 - val_loss: 0.6861 - val_acc: 0.6239 - val_f1_score: 0.6019\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6875 - acc: 0.5602 - f1_score: 0.5541 - val_loss: 0.6852 - val_acc: 0.6303 - val_f1_score: 0.6057\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5589 - f1_score: 0.5513 - val_loss: 0.6845 - val_acc: 0.6346 - val_f1_score: 0.6084\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5587 - f1_score: 0.5505 - val_loss: 0.6837 - val_acc: 0.6303 - val_f1_score: 0.6019\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5593 - f1_score: 0.5498 - val_loss: 0.6829 - val_acc: 0.6346 - val_f1_score: 0.6056\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5593 - f1_score: 0.5494 - val_loss: 0.6821 - val_acc: 0.6368 - val_f1_score: 0.6064\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5586 - f1_score: 0.5477 - val_loss: 0.6814 - val_acc: 0.6368 - val_f1_score: 0.6054\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5576 - f1_score: 0.5460 - val_loss: 0.6807 - val_acc: 0.6368 - val_f1_score: 0.6043\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5570 - f1_score: 0.5448 - val_loss: 0.6800 - val_acc: 0.6389 - val_f1_score: 0.6061\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6851 - acc: 0.5571 - f1_score: 0.5444 - val_loss: 0.6793 - val_acc: 0.6410 - val_f1_score: 0.6079\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5583 - f1_score: 0.5452 - val_loss: 0.6785 - val_acc: 0.6410 - val_f1_score: 0.6068\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5586 - f1_score: 0.5449 - val_loss: 0.6779 - val_acc: 0.6432 - val_f1_score: 0.6086\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5577 - f1_score: 0.5432 - val_loss: 0.6772 - val_acc: 0.6432 - val_f1_score: 0.6075\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6842 - acc: 0.5583 - f1_score: 0.5436 - val_loss: 0.6765 - val_acc: 0.6453 - val_f1_score: 0.6093\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5582 - f1_score: 0.5424 - val_loss: 0.6759 - val_acc: 0.6474 - val_f1_score: 0.6111\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5590 - f1_score: 0.5426 - val_loss: 0.6752 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5587 - f1_score: 0.5416 - val_loss: 0.6746 - val_acc: 0.6517 - val_f1_score: 0.6147\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5583 - f1_score: 0.5407 - val_loss: 0.6741 - val_acc: 0.6581 - val_f1_score: 0.6201\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5585 - f1_score: 0.5408 - val_loss: 0.6735 - val_acc: 0.6603 - val_f1_score: 0.6219\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5589 - f1_score: 0.5405 - val_loss: 0.6729 - val_acc: 0.6624 - val_f1_score: 0.6237\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5598 - f1_score: 0.5409 - val_loss: 0.6723 - val_acc: 0.6667 - val_f1_score: 0.6274\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5608 - f1_score: 0.5413 - val_loss: 0.6718 - val_acc: 0.6667 - val_f1_score: 0.6274\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5611 - f1_score: 0.5415 - val_loss: 0.6712 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5618 - f1_score: 0.5412 - val_loss: 0.6707 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5622 - f1_score: 0.5415 - val_loss: 0.6702 - val_acc: 0.6645 - val_f1_score: 0.6232\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5627 - f1_score: 0.5416 - val_loss: 0.6697 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5629 - f1_score: 0.5414 - val_loss: 0.6692 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5644 - f1_score: 0.5419 - val_loss: 0.6688 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5640 - f1_score: 0.5418 - val_loss: 0.6683 - val_acc: 0.6709 - val_f1_score: 0.6286\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5648 - f1_score: 0.5424 - val_loss: 0.6678 - val_acc: 0.6709 - val_f1_score: 0.6286\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5654 - f1_score: 0.5421 - val_loss: 0.6674 - val_acc: 0.6774 - val_f1_score: 0.6340\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5648 - f1_score: 0.5417 - val_loss: 0.6670 - val_acc: 0.6774 - val_f1_score: 0.6340\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5654 - f1_score: 0.5420 - val_loss: 0.6666 - val_acc: 0.6774 - val_f1_score: 0.6328\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5656 - f1_score: 0.5417 - val_loss: 0.6661 - val_acc: 0.6774 - val_f1_score: 0.6328\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5657 - f1_score: 0.5418 - val_loss: 0.6657 - val_acc: 0.6774 - val_f1_score: 0.6328\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5661 - f1_score: 0.5418 - val_loss: 0.6654 - val_acc: 0.6752 - val_f1_score: 0.6285\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5654 - f1_score: 0.5404 - val_loss: 0.6650 - val_acc: 0.6774 - val_f1_score: 0.6303\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5658 - f1_score: 0.5411 - val_loss: 0.6646 - val_acc: 0.6774 - val_f1_score: 0.6303\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5660 - f1_score: 0.5401 - val_loss: 0.6643 - val_acc: 0.6774 - val_f1_score: 0.6303\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5661 - f1_score: 0.5408 - val_loss: 0.6639 - val_acc: 0.6816 - val_f1_score: 0.6339\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5664 - f1_score: 0.5401 - val_loss: 0.6636 - val_acc: 0.6816 - val_f1_score: 0.6339\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5664 - f1_score: 0.5408 - val_loss: 0.6632 - val_acc: 0.6816 - val_f1_score: 0.6339\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5666 - f1_score: 0.5405 - val_loss: 0.6629 - val_acc: 0.6838 - val_f1_score: 0.6357\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5658 - f1_score: 0.5395 - val_loss: 0.6626 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5674 - f1_score: 0.5402 - val_loss: 0.6623 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6792 - acc: 0.5666 - f1_score: 0.5397 - val_loss: 0.6620 - val_acc: 0.6859 - val_f1_score: 0.6375\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5667 - f1_score: 0.5398 - val_loss: 0.6617 - val_acc: 0.6838 - val_f1_score: 0.6344\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5679 - f1_score: 0.5402 - val_loss: 0.6614 - val_acc: 0.6816 - val_f1_score: 0.6312\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5674 - f1_score: 0.5402 - val_loss: 0.6611 - val_acc: 0.6816 - val_f1_score: 0.6299\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6787 - acc: 0.5682 - f1_score: 0.5404 - val_loss: 0.6608 - val_acc: 0.6838 - val_f1_score: 0.6317\n",
      "Epoch 64/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5689 - f1_score: 0.5405 - val_loss: 0.6606 - val_acc: 0.6838 - val_f1_score: 0.6317\n",
      "Epoch 65/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5683 - f1_score: 0.5405 - val_loss: 0.6603 - val_acc: 0.6838 - val_f1_score: 0.6317\n",
      "Epoch 66/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5689 - f1_score: 0.5399 - val_loss: 0.6600 - val_acc: 0.6838 - val_f1_score: 0.6317\n",
      "Epoch 67/300\n",
      "27/27 - 3s - loss: 0.6782 - acc: 0.5689 - f1_score: 0.5399 - val_loss: 0.6597 - val_acc: 0.6816 - val_f1_score: 0.6285\n",
      "Epoch 68/300\n",
      "27/27 - 3s - loss: 0.6781 - acc: 0.5689 - f1_score: 0.5397 - val_loss: 0.6595 - val_acc: 0.6816 - val_f1_score: 0.6285\n",
      "Epoch 69/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5682 - f1_score: 0.5391 - val_loss: 0.6592 - val_acc: 0.6838 - val_f1_score: 0.6289\n",
      "Epoch 70/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5679 - f1_score: 0.5388 - val_loss: 0.6590 - val_acc: 0.6902 - val_f1_score: 0.6342\n",
      "Epoch 71/300\n",
      "27/27 - 3s - loss: 0.6778 - acc: 0.5683 - f1_score: 0.5385 - val_loss: 0.6587 - val_acc: 0.6902 - val_f1_score: 0.6342\n",
      "Epoch 72/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5689 - f1_score: 0.5385 - val_loss: 0.6585 - val_acc: 0.6902 - val_f1_score: 0.6342\n",
      "Epoch 73/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5684 - f1_score: 0.5382 - val_loss: 0.6583 - val_acc: 0.6902 - val_f1_score: 0.6342\n",
      "Epoch 74/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5686 - f1_score: 0.5385 - val_loss: 0.6581 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 75/300\n",
      "27/27 - 3s - loss: 0.6774 - acc: 0.5687 - f1_score: 0.5386 - val_loss: 0.6578 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 76/300\n",
      "27/27 - 3s - loss: 0.6773 - acc: 0.5693 - f1_score: 0.5383 - val_loss: 0.6576 - val_acc: 0.6923 - val_f1_score: 0.6360\n",
      "Epoch 77/300\n",
      "27/27 - 3s - loss: 0.6772 - acc: 0.5703 - f1_score: 0.5393 - val_loss: 0.6575 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 78/300\n",
      "27/27 - 3s - loss: 0.6771 - acc: 0.5692 - f1_score: 0.5384 - val_loss: 0.6573 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 79/300\n",
      "27/27 - 3s - loss: 0.6770 - acc: 0.5695 - f1_score: 0.5382 - val_loss: 0.6571 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 80/300\n",
      "27/27 - 3s - loss: 0.6769 - acc: 0.5703 - f1_score: 0.5385 - val_loss: 0.6569 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 81/300\n",
      "27/27 - 3s - loss: 0.6768 - acc: 0.5698 - f1_score: 0.5377 - val_loss: 0.6568 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 82/300\n",
      "27/27 - 3s - loss: 0.6767 - acc: 0.5699 - f1_score: 0.5375 - val_loss: 0.6567 - val_acc: 0.6944 - val_f1_score: 0.6379\n",
      "Epoch 83/300\n",
      "27/27 - 3s - loss: 0.6766 - acc: 0.5700 - f1_score: 0.5379 - val_loss: 0.6565 - val_acc: 0.6987 - val_f1_score: 0.6415\n",
      "Epoch 84/300\n",
      "27/27 - 3s - loss: 0.6765 - acc: 0.5696 - f1_score: 0.5375 - val_loss: 0.6564 - val_acc: 0.6987 - val_f1_score: 0.6415\n",
      "Epoch 85/300\n",
      "27/27 - 3s - loss: 0.6764 - acc: 0.5699 - f1_score: 0.5370 - val_loss: 0.6562 - val_acc: 0.6987 - val_f1_score: 0.6415\n",
      "Epoch 86/300\n",
      "27/27 - 3s - loss: 0.6763 - acc: 0.5702 - f1_score: 0.5385 - val_loss: 0.6560 - val_acc: 0.7009 - val_f1_score: 0.6418\n",
      "Epoch 87/300\n",
      "27/27 - 3s - loss: 0.6762 - acc: 0.5699 - f1_score: 0.5375 - val_loss: 0.6559 - val_acc: 0.7030 - val_f1_score: 0.6436\n",
      "Epoch 88/300\n",
      "27/27 - 3s - loss: 0.6761 - acc: 0.5711 - f1_score: 0.5382 - val_loss: 0.6558 - val_acc: 0.7030 - val_f1_score: 0.6436\n",
      "Epoch 89/300\n",
      "27/27 - 3s - loss: 0.6760 - acc: 0.5718 - f1_score: 0.5383 - val_loss: 0.6557 - val_acc: 0.7030 - val_f1_score: 0.6436\n",
      "Epoch 90/300\n",
      "27/27 - 3s - loss: 0.6759 - acc: 0.5716 - f1_score: 0.5384 - val_loss: 0.6556 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 91/300\n",
      "27/27 - 3s - loss: 0.6758 - acc: 0.5726 - f1_score: 0.5392 - val_loss: 0.6555 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 92/300\n",
      "27/27 - 3s - loss: 0.6757 - acc: 0.5716 - f1_score: 0.5388 - val_loss: 0.6554 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 93/300\n",
      "27/27 - 3s - loss: 0.6756 - acc: 0.5726 - f1_score: 0.5381 - val_loss: 0.6554 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 94/300\n",
      "27/27 - 3s - loss: 0.6755 - acc: 0.5718 - f1_score: 0.5390 - val_loss: 0.6552 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 95/300\n",
      "27/27 - 3s - loss: 0.6755 - acc: 0.5738 - f1_score: 0.5395 - val_loss: 0.6551 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 96/300\n",
      "27/27 - 3s - loss: 0.6754 - acc: 0.5737 - f1_score: 0.5393 - val_loss: 0.6551 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 97/300\n",
      "27/27 - 3s - loss: 0.6753 - acc: 0.5729 - f1_score: 0.5390 - val_loss: 0.6550 - val_acc: 0.6987 - val_f1_score: 0.6370\n",
      "Epoch 98/300\n",
      "27/27 - 3s - loss: 0.6752 - acc: 0.5740 - f1_score: 0.5393 - val_loss: 0.6550 - val_acc: 0.6987 - val_f1_score: 0.6370\n",
      "Epoch 99/300\n",
      "27/27 - 3s - loss: 0.6751 - acc: 0.5745 - f1_score: 0.5397 - val_loss: 0.6549 - val_acc: 0.7009 - val_f1_score: 0.6403\n",
      "Epoch 100/300\n",
      "27/27 - 3s - loss: 0.6750 - acc: 0.5734 - f1_score: 0.5395 - val_loss: 0.6548 - val_acc: 0.7009 - val_f1_score: 0.6388\n",
      "Epoch 101/300\n",
      "27/27 - 3s - loss: 0.6749 - acc: 0.5751 - f1_score: 0.5401 - val_loss: 0.6548 - val_acc: 0.6987 - val_f1_score: 0.6370\n",
      "Epoch 102/300\n",
      "27/27 - 3s - loss: 0.6748 - acc: 0.5753 - f1_score: 0.5408 - val_loss: 0.6547 - val_acc: 0.7009 - val_f1_score: 0.6388\n",
      "Epoch 103/300\n",
      "27/27 - 3s - loss: 0.6747 - acc: 0.5741 - f1_score: 0.5400 - val_loss: 0.6545 - val_acc: 0.7030 - val_f1_score: 0.6406\n",
      "Epoch 104/300\n",
      "27/27 - 3s - loss: 0.6746 - acc: 0.5755 - f1_score: 0.5404 - val_loss: 0.6545 - val_acc: 0.7030 - val_f1_score: 0.6406\n",
      "Epoch 105/300\n",
      "27/27 - 3s - loss: 0.6745 - acc: 0.5758 - f1_score: 0.5397 - val_loss: 0.6545 - val_acc: 0.7030 - val_f1_score: 0.6406\n",
      "Epoch 106/300\n",
      "27/27 - 3s - loss: 0.6744 - acc: 0.5764 - f1_score: 0.5412 - val_loss: 0.6545 - val_acc: 0.7030 - val_f1_score: 0.6406\n",
      "Epoch 107/300\n",
      "27/27 - 3s - loss: 0.6743 - acc: 0.5761 - f1_score: 0.5413 - val_loss: 0.6544 - val_acc: 0.7030 - val_f1_score: 0.6406\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00107: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000293C258E550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.64      0.50       108\n",
      "           1       0.87      0.72      0.79       360\n",
      "\n",
      "    accuracy                           0.70       468\n",
      "   macro avg       0.64      0.68      0.64       468\n",
      "weighted avg       0.76      0.70      0.72       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1892\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 12s - loss: 0.6956 - acc: 0.4311 - f1_score: 0.3012 - val_loss: 0.6732 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6947 - acc: 0.4317 - f1_score: 0.3051 - val_loss: 0.6774 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6938 - acc: 0.4405 - f1_score: 0.3314 - val_loss: 0.6813 - val_acc: 0.9658 - val_f1_score: 0.4913\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6930 - acc: 0.4725 - f1_score: 0.4170 - val_loss: 0.6849 - val_acc: 0.8675 - val_f1_score: 0.5084\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.5261 - f1_score: 0.5232 - val_loss: 0.6884 - val_acc: 0.6026 - val_f1_score: 0.4086\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.5718 - f1_score: 0.5598 - val_loss: 0.6916 - val_acc: 0.3910 - val_f1_score: 0.3116\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6907 - acc: 0.5861 - f1_score: 0.5559 - val_loss: 0.6948 - val_acc: 0.2991 - val_f1_score: 0.2536\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5910 - f1_score: 0.5473 - val_loss: 0.6977 - val_acc: 0.2671 - val_f1_score: 0.2328\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6893 - acc: 0.5926 - f1_score: 0.5406 - val_loss: 0.7005 - val_acc: 0.2393 - val_f1_score: 0.2127\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5954 - f1_score: 0.5386 - val_loss: 0.7031 - val_acc: 0.2201 - val_f1_score: 0.1984\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5977 - f1_score: 0.5374 - val_loss: 0.7056 - val_acc: 0.2073 - val_f1_score: 0.1886\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6874 - acc: 0.5977 - f1_score: 0.5356 - val_loss: 0.7079 - val_acc: 0.2073 - val_f1_score: 0.1886\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5981 - f1_score: 0.5346 - val_loss: 0.7102 - val_acc: 0.2051 - val_f1_score: 0.1869\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5967 - f1_score: 0.5315 - val_loss: 0.7125 - val_acc: 0.2009 - val_f1_score: 0.1836\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5977 - f1_score: 0.5320 - val_loss: 0.7147 - val_acc: 0.1987 - val_f1_score: 0.1820\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5966 - f1_score: 0.5300 - val_loss: 0.7167 - val_acc: 0.1987 - val_f1_score: 0.1820\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5977 - f1_score: 0.5323 - val_loss: 0.7188 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5986 - f1_score: 0.5309 - val_loss: 0.7208 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6833 - acc: 0.5981 - f1_score: 0.5319 - val_loss: 0.7228 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6827 - acc: 0.5980 - f1_score: 0.5307 - val_loss: 0.7247 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5987 - f1_score: 0.5324 - val_loss: 0.7266 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5994 - f1_score: 0.5336 - val_loss: 0.7285 - val_acc: 0.1944 - val_f1_score: 0.1786\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5994 - f1_score: 0.5335 - val_loss: 0.7304 - val_acc: 0.1966 - val_f1_score: 0.1803\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5990 - f1_score: 0.5348 - val_loss: 0.7324 - val_acc: 0.1944 - val_f1_score: 0.1786\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       413\n",
      "           1       0.23      0.05      0.09        55\n",
      "\n",
      "    accuracy                           0.87       468\n",
      "   macro avg       0.56      0.52      0.51       468\n",
      "weighted avg       0.81      0.87      0.83       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1929\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 12s - loss: 0.6930 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7088 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.4820 - f1_score: 0.3252 - val_loss: 0.7092 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6919 - acc: 0.4820 - f1_score: 0.3252 - val_loss: 0.7097 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.4821 - f1_score: 0.3260 - val_loss: 0.7104 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6906 - acc: 0.4827 - f1_score: 0.3290 - val_loss: 0.7112 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.4879 - f1_score: 0.3457 - val_loss: 0.7122 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6894 - acc: 0.4989 - f1_score: 0.3725 - val_loss: 0.7130 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5106 - f1_score: 0.4019 - val_loss: 0.7140 - val_acc: 0.2265 - val_f1_score: 0.1879\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6882 - acc: 0.5244 - f1_score: 0.4367 - val_loss: 0.7149 - val_acc: 0.2479 - val_f1_score: 0.2216\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5319 - f1_score: 0.4701 - val_loss: 0.7159 - val_acc: 0.3013 - val_f1_score: 0.2948\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6871 - acc: 0.5447 - f1_score: 0.5100 - val_loss: 0.7168 - val_acc: 0.3675 - val_f1_score: 0.3642\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5495 - f1_score: 0.5364 - val_loss: 0.7178 - val_acc: 0.4701 - val_f1_score: 0.4379\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5590 - f1_score: 0.5560 - val_loss: 0.7187 - val_acc: 0.4829 - val_f1_score: 0.4288\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6856 - acc: 0.5612 - f1_score: 0.5610 - val_loss: 0.7196 - val_acc: 0.4957 - val_f1_score: 0.4327\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5614 - f1_score: 0.5611 - val_loss: 0.7204 - val_acc: 0.4979 - val_f1_score: 0.4285\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6848 - acc: 0.5618 - f1_score: 0.5609 - val_loss: 0.7212 - val_acc: 0.5000 - val_f1_score: 0.4260\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5628 - f1_score: 0.5613 - val_loss: 0.7218 - val_acc: 0.5043 - val_f1_score: 0.4289\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5625 - f1_score: 0.5600 - val_loss: 0.7225 - val_acc: 0.5064 - val_f1_score: 0.4283\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5625 - f1_score: 0.5591 - val_loss: 0.7230 - val_acc: 0.5085 - val_f1_score: 0.4297\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5634 - f1_score: 0.5590 - val_loss: 0.7235 - val_acc: 0.5064 - val_f1_score: 0.4262\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5635 - f1_score: 0.5585 - val_loss: 0.7238 - val_acc: 0.5085 - val_f1_score: 0.4276\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5640 - f1_score: 0.5581 - val_loss: 0.7242 - val_acc: 0.5107 - val_f1_score: 0.4290\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5647 - f1_score: 0.5582 - val_loss: 0.7243 - val_acc: 0.5150 - val_f1_score: 0.4318\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5664 - f1_score: 0.5596 - val_loss: 0.7242 - val_acc: 0.5128 - val_f1_score: 0.4260\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5660 - f1_score: 0.5584 - val_loss: 0.7241 - val_acc: 0.5192 - val_f1_score: 0.4302\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5661 - f1_score: 0.5579 - val_loss: 0.7241 - val_acc: 0.5192 - val_f1_score: 0.4302\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5663 - f1_score: 0.5579 - val_loss: 0.7237 - val_acc: 0.5235 - val_f1_score: 0.4330\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5651 - f1_score: 0.5559 - val_loss: 0.7234 - val_acc: 0.5235 - val_f1_score: 0.4306\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5637 - f1_score: 0.5538 - val_loss: 0.7232 - val_acc: 0.5214 - val_f1_score: 0.4269\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5629 - f1_score: 0.5523 - val_loss: 0.7231 - val_acc: 0.5235 - val_f1_score: 0.4283\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5645 - f1_score: 0.5534 - val_loss: 0.7232 - val_acc: 0.5256 - val_f1_score: 0.4296\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5647 - f1_score: 0.5534 - val_loss: 0.7226 - val_acc: 0.5278 - val_f1_score: 0.4310\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.21      0.30       252\n",
      "           1       0.46      0.77      0.57       216\n",
      "\n",
      "    accuracy                           0.47       468\n",
      "   macro avg       0.49      0.49      0.44       468\n",
      "weighted avg       0.49      0.47      0.43       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1933\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 13s - loss: 0.6937 - acc: 0.4699 - f1_score: 0.3197 - val_loss: 0.7019 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6930 - acc: 0.4697 - f1_score: 0.3198 - val_loss: 0.7014 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4725 - f1_score: 0.3289 - val_loss: 0.7009 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4854 - f1_score: 0.3661 - val_loss: 0.7006 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6911 - acc: 0.5034 - f1_score: 0.4150 - val_loss: 0.7004 - val_acc: 0.3419 - val_f1_score: 0.2643\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5228 - f1_score: 0.4760 - val_loss: 0.7003 - val_acc: 0.3376 - val_f1_score: 0.2785\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5435 - f1_score: 0.5300 - val_loss: 0.7003 - val_acc: 0.3761 - val_f1_score: 0.3573\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6893 - acc: 0.5598 - f1_score: 0.5594 - val_loss: 0.7003 - val_acc: 0.4359 - val_f1_score: 0.4355\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5662 - f1_score: 0.5643 - val_loss: 0.7004 - val_acc: 0.4274 - val_f1_score: 0.4135\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5627 - f1_score: 0.5585 - val_loss: 0.7005 - val_acc: 0.4316 - val_f1_score: 0.4098\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5656 - f1_score: 0.5590 - val_loss: 0.7006 - val_acc: 0.4359 - val_f1_score: 0.4111\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6871 - acc: 0.5669 - f1_score: 0.5587 - val_loss: 0.7007 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5668 - f1_score: 0.5569 - val_loss: 0.7008 - val_acc: 0.4487 - val_f1_score: 0.4212\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5665 - f1_score: 0.5555 - val_loss: 0.7010 - val_acc: 0.4487 - val_f1_score: 0.4212\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5663 - f1_score: 0.5542 - val_loss: 0.7011 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5661 - f1_score: 0.5531 - val_loss: 0.7012 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5662 - f1_score: 0.5524 - val_loss: 0.7013 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5658 - f1_score: 0.5512 - val_loss: 0.7014 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6841 - acc: 0.5658 - f1_score: 0.5504 - val_loss: 0.7015 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5665 - f1_score: 0.5507 - val_loss: 0.7015 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5661 - f1_score: 0.5496 - val_loss: 0.7015 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5663 - f1_score: 0.5496 - val_loss: 0.7013 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5672 - f1_score: 0.5493 - val_loss: 0.7013 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5666 - f1_score: 0.5483 - val_loss: 0.7012 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5669 - f1_score: 0.5483 - val_loss: 0.7010 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6820 - acc: 0.5669 - f1_score: 0.5477 - val_loss: 0.7008 - val_acc: 0.4615 - val_f1_score: 0.4288\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5683 - f1_score: 0.5485 - val_loss: 0.7006 - val_acc: 0.4615 - val_f1_score: 0.4288\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5689 - f1_score: 0.5487 - val_loss: 0.7003 - val_acc: 0.4658 - val_f1_score: 0.4321\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.32      0.42       150\n",
      "           1       0.35      0.64      0.45        84\n",
      "\n",
      "    accuracy                           0.44       234\n",
      "   macro avg       0.48      0.48      0.44       234\n",
      "weighted avg       0.52      0.44      0.43       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220609-163438\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Type II, Stage two\n",
      "Average Accuracy:  0.6505323011740637\n",
      "F1 score for Baseline:  0.531216987206858\n",
      "F1 score for Stress:  0.5877505846880917\n",
      "Macro F1:  0.5594837859474748\n",
      "Weighted F1:  0.6325420668295167\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['II']\n",
    "attx_st = ['two']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in sub_dict_ecg.keys():\n",
    "\n",
    "            if i in ['1765']:\n",
    "                continue\n",
    "\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=20, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_model(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))\n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecga (Conv1D)       (None, 2560, 32)     2080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edaa (Conv1D)       (None, 2560, 32)     6176        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecga (Activation)    (None, 2560, 32)     0           conv_stage1_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edaa (Activation)    (None, 2560, 32)     0           conv_stage1_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_ecgb (Conv1D)       (None, 854, 32)      65568       act_stage1_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage1_edab (Conv1D)       (None, 854, 32)      65568       act_stage1_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_ecgb (Activation)    (None, 854, 32)      0           conv_stage1_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage1_edab (Activation)    (None, 854, 32)      0           conv_stage1_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_ecg (MaxPooling1D)    (None, 427, 32)      0           act_stage1_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage1_eda (MaxPooling1D)    (None, 427, 32)      0           act_stage1_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 427, 1, 32)   0           mp_stage1_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 427, 1, 32)   0           mp_stage1_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 427, 2, 32)   0           tf.expand_dims[0][0]             \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 427, 32, 2)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 427, 32, 2)   6           tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 427, 32, 2)   0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 427, 2, 32)   0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Attention_laye ((None, 427, 1), (No 32          tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 427, 32)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 attention_layer[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 427, 32)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 427, 64)      0           mp_stage1_eda[0][0]              \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecga (Conv1D)       (None, 427, 64)      131136      tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edaa (Conv1D)       (None, 427, 64)      131136      tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecga (Activation)    (None, 427, 64)      0           conv_stage2_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edaa (Activation)    (None, 427, 64)      0           conv_stage2_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_ecgb (Conv1D)       (None, 143, 64)      131136      act_stage2_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage2_edab (Conv1D)       (None, 143, 64)      131136      act_stage2_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_ecgb (Activation)    (None, 143, 64)      0           conv_stage2_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage2_edab (Activation)    (None, 143, 64)      0           conv_stage2_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_ecg (MaxPooling1D)    (None, 71, 64)       0           act_stage2_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage2_eda (MaxPooling1D)    (None, 71, 64)       0           act_stage2_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 71, 1, 64)    0           mp_stage2_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 71, 2, 64)    0           tf.expand_dims_2[0][0]           \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 71, 64, 2)    0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 71, 64, 2)    6           tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 71, 64, 2)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 71, 2, 64)    0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (Attention_la ((None, 71, 1), (Non 64          tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 71, 64)       0           mp_stage2_eda[0][0]              \n",
      "                                                                 attention_layer_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 71, 64)       0           mp_stage2_ecg[0][0]              \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_5 (TFOpLambda)        (None, 71, 128)      0           mp_stage2_eda[0][0]              \n",
      "                                                                 tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecga (Conv1D)       (None, 71, 128)      278656      tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edaa (Conv1D)       (None, 71, 128)      278656      tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecga (Activation)    (None, 71, 128)      0           conv_stage3_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edaa (Activation)    (None, 71, 128)      0           conv_stage3_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_ecgb (Conv1D)       (None, 24, 128)      278656      act_stage3_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage3_edab (Conv1D)       (None, 24, 128)      278656      act_stage3_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_ecgb (Activation)    (None, 24, 128)      0           conv_stage3_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage3_edab (Activation)    (None, 24, 128)      0           conv_stage3_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_ecg (MaxPooling1D)    (None, 12, 128)      0           act_stage3_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage3_eda (MaxPooling1D)    (None, 12, 128)      0           act_stage3_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (None, 12, 1, 128)   0           mp_stage3_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (None, 12, 2, 128)   0           tf.expand_dims_4[0][0]           \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (None, 12, 128, 2)   0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12, 128, 2)   6           tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 12, 128, 2)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_5 (TFOpL (None, 12, 2, 128)   0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (Attention_la ((None, 12, 1), (Non 128         tf.compat.v1.transpose_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 12, 128)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 attention_layer_2[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None, 12, 128)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_ecg[0][0]              \n",
      "                                                                 tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (None, 12, 256)      0           mp_stage3_eda[0][0]              \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecga (Conv1D)       (None, 12, 256)      459008      tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edaa (Conv1D)       (None, 12, 256)      459008      tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecga (Activation)    (None, 12, 256)      0           conv_stage4_ecga[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edaa (Activation)    (None, 12, 256)      0           conv_stage4_edaa[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_ecgb (Conv1D)       (None, 4, 256)       459008      act_stage4_ecga[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_stage4_edab (Conv1D)       (None, 4, 256)       459008      act_stage4_edaa[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_ecgb (Activation)    (None, 4, 256)       0           conv_stage4_ecgb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "act_stage4_edab (Activation)    (None, 4, 256)       0           conv_stage4_edab[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_ecg (MaxPooling1D)    (None, 2, 256)       0           act_stage4_ecgb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mp_stage4_eda (MaxPooling1D)    (None, 2, 256)       0           act_stage4_edab[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           mp_stage4_ecg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           mp_stage4_eda[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 512)          262656      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 512)          262656      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            2050        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,667,508\n",
      "Trainable params: 4,667,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type FF, Stage FF\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-141100\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1105\n",
      "Epoch 1/300\n",
      "27/27 - 18s - loss: 0.6940 - acc: 0.4689 - f1_score: 0.3195 - val_loss: 0.6937 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6936 - acc: 0.4685 - f1_score: 0.3197 - val_loss: 0.6926 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6932 - acc: 0.4708 - f1_score: 0.3303 - val_loss: 0.6916 - val_acc: 0.4188 - val_f1_score: 0.3012\n",
      "Epoch 4/300\n",
      "27/27 - 4s - loss: 0.6928 - acc: 0.4804 - f1_score: 0.3648 - val_loss: 0.6905 - val_acc: 0.4402 - val_f1_score: 0.3618\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4982 - f1_score: 0.4211 - val_loss: 0.6895 - val_acc: 0.5406 - val_f1_score: 0.5332\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6919 - acc: 0.5261 - f1_score: 0.4968 - val_loss: 0.6884 - val_acc: 0.6004 - val_f1_score: 0.5970\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.5486 - f1_score: 0.5435 - val_loss: 0.6873 - val_acc: 0.6389 - val_f1_score: 0.6253\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5485 - f1_score: 0.5480 - val_loss: 0.6862 - val_acc: 0.6624 - val_f1_score: 0.6441\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.5532 - f1_score: 0.5487 - val_loss: 0.6850 - val_acc: 0.6752 - val_f1_score: 0.6524\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.5585 - f1_score: 0.5492 - val_loss: 0.6839 - val_acc: 0.6774 - val_f1_score: 0.6526\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5571 - f1_score: 0.5454 - val_loss: 0.6827 - val_acc: 0.6752 - val_f1_score: 0.6490\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6896 - acc: 0.5570 - f1_score: 0.5426 - val_loss: 0.6816 - val_acc: 0.6752 - val_f1_score: 0.6462\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5570 - f1_score: 0.5414 - val_loss: 0.6805 - val_acc: 0.6752 - val_f1_score: 0.6462\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6889 - acc: 0.5573 - f1_score: 0.5404 - val_loss: 0.6794 - val_acc: 0.6752 - val_f1_score: 0.6453\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5579 - f1_score: 0.5401 - val_loss: 0.6782 - val_acc: 0.6795 - val_f1_score: 0.6490\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6882 - acc: 0.5595 - f1_score: 0.5405 - val_loss: 0.6771 - val_acc: 0.6795 - val_f1_score: 0.6490\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6878 - acc: 0.5580 - f1_score: 0.5386 - val_loss: 0.6760 - val_acc: 0.6795 - val_f1_score: 0.6490\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6875 - acc: 0.5579 - f1_score: 0.5382 - val_loss: 0.6749 - val_acc: 0.6795 - val_f1_score: 0.6480\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5579 - f1_score: 0.5383 - val_loss: 0.6738 - val_acc: 0.6816 - val_f1_score: 0.6498\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6869 - acc: 0.5576 - f1_score: 0.5373 - val_loss: 0.6728 - val_acc: 0.6816 - val_f1_score: 0.6498\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5585 - f1_score: 0.5379 - val_loss: 0.6718 - val_acc: 0.6816 - val_f1_score: 0.6498\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5571 - f1_score: 0.5369 - val_loss: 0.6707 - val_acc: 0.6795 - val_f1_score: 0.6470\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5585 - f1_score: 0.5378 - val_loss: 0.6698 - val_acc: 0.6795 - val_f1_score: 0.6470\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5599 - f1_score: 0.5384 - val_loss: 0.6688 - val_acc: 0.6795 - val_f1_score: 0.6470\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5586 - f1_score: 0.5374 - val_loss: 0.6679 - val_acc: 0.6795 - val_f1_score: 0.6470\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5593 - f1_score: 0.5379 - val_loss: 0.6670 - val_acc: 0.6774 - val_f1_score: 0.6441\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5596 - f1_score: 0.5378 - val_loss: 0.6661 - val_acc: 0.6774 - val_f1_score: 0.6441\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6848 - acc: 0.5599 - f1_score: 0.5379 - val_loss: 0.6653 - val_acc: 0.6774 - val_f1_score: 0.6441\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5600 - f1_score: 0.5379 - val_loss: 0.6645 - val_acc: 0.6774 - val_f1_score: 0.6441\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5602 - f1_score: 0.5376 - val_loss: 0.6637 - val_acc: 0.6795 - val_f1_score: 0.6460\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.65      0.56       148\n",
      "           1       0.81      0.69      0.75       320\n",
      "\n",
      "    accuracy                           0.68       468\n",
      "   macro avg       0.65      0.67      0.65       468\n",
      "weighted avg       0.71      0.68      0.69       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1106\n",
      "Epoch 1/300\n",
      "27/27 - 14s - loss: 0.6935 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7019 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6930 - acc: 0.4915 - f1_score: 0.3298 - val_loss: 0.7018 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.4912 - f1_score: 0.3299 - val_loss: 0.7019 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6920 - acc: 0.4914 - f1_score: 0.3310 - val_loss: 0.7021 - val_acc: 0.0919 - val_f1_score: 0.0868\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.4922 - f1_score: 0.3348 - val_loss: 0.7023 - val_acc: 0.1111 - val_f1_score: 0.1084\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6909 - acc: 0.4965 - f1_score: 0.3511 - val_loss: 0.7026 - val_acc: 0.1688 - val_f1_score: 0.1685\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6903 - acc: 0.5021 - f1_score: 0.3753 - val_loss: 0.7030 - val_acc: 0.2585 - val_f1_score: 0.2449\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.5164 - f1_score: 0.4325 - val_loss: 0.7035 - val_acc: 0.3590 - val_f1_score: 0.3185\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5311 - f1_score: 0.4831 - val_loss: 0.7040 - val_acc: 0.4274 - val_f1_score: 0.3656\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5515 - f1_score: 0.5300 - val_loss: 0.7045 - val_acc: 0.4915 - val_f1_score: 0.4077\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6882 - acc: 0.5580 - f1_score: 0.5494 - val_loss: 0.7051 - val_acc: 0.5256 - val_f1_score: 0.4272\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6877 - acc: 0.5606 - f1_score: 0.5584 - val_loss: 0.7057 - val_acc: 0.5470 - val_f1_score: 0.4407\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5605 - f1_score: 0.5601 - val_loss: 0.7063 - val_acc: 0.5662 - val_f1_score: 0.4529\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5580 - f1_score: 0.5580 - val_loss: 0.7069 - val_acc: 0.5726 - val_f1_score: 0.4570\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5563 - f1_score: 0.5559 - val_loss: 0.7074 - val_acc: 0.5855 - val_f1_score: 0.4651\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5560 - f1_score: 0.5549 - val_loss: 0.7081 - val_acc: 0.5897 - val_f1_score: 0.4678\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6854 - acc: 0.5574 - f1_score: 0.5558 - val_loss: 0.7086 - val_acc: 0.5919 - val_f1_score: 0.4692\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5602 - f1_score: 0.5580 - val_loss: 0.7091 - val_acc: 0.6004 - val_f1_score: 0.4746\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5598 - f1_score: 0.5572 - val_loss: 0.7094 - val_acc: 0.6026 - val_f1_score: 0.4760\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5592 - f1_score: 0.5559 - val_loss: 0.7097 - val_acc: 0.6068 - val_f1_score: 0.4787\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5596 - f1_score: 0.5559 - val_loss: 0.7100 - val_acc: 0.6090 - val_f1_score: 0.4801\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5596 - f1_score: 0.5556 - val_loss: 0.7100 - val_acc: 0.6111 - val_f1_score: 0.4815\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6833 - acc: 0.5590 - f1_score: 0.5545 - val_loss: 0.7102 - val_acc: 0.6111 - val_f1_score: 0.4815\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5580 - f1_score: 0.5529 - val_loss: 0.7104 - val_acc: 0.6175 - val_f1_score: 0.4856\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5577 - f1_score: 0.5524 - val_loss: 0.7103 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5569 - f1_score: 0.5512 - val_loss: 0.7101 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5570 - f1_score: 0.5506 - val_loss: 0.7100 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5563 - f1_score: 0.5496 - val_loss: 0.7099 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5563 - f1_score: 0.5493 - val_loss: 0.7098 - val_acc: 0.6261 - val_f1_score: 0.4911\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5563 - f1_score: 0.5491 - val_loss: 0.7097 - val_acc: 0.6282 - val_f1_score: 0.4925\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5564 - f1_score: 0.5490 - val_loss: 0.7095 - val_acc: 0.6303 - val_f1_score: 0.4939\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5561 - f1_score: 0.5484 - val_loss: 0.7091 - val_acc: 0.6325 - val_f1_score: 0.4953\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5554 - f1_score: 0.5472 - val_loss: 0.7089 - val_acc: 0.6389 - val_f1_score: 0.4995\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5561 - f1_score: 0.5478 - val_loss: 0.7084 - val_acc: 0.6389 - val_f1_score: 0.4995\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5558 - f1_score: 0.5469 - val_loss: 0.7081 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5566 - f1_score: 0.5475 - val_loss: 0.7075 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5583 - f1_score: 0.5487 - val_loss: 0.7072 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5585 - f1_score: 0.5483 - val_loss: 0.7069 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5587 - f1_score: 0.5481 - val_loss: 0.7069 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5585 - f1_score: 0.5478 - val_loss: 0.7067 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5589 - f1_score: 0.5481 - val_loss: 0.7063 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5599 - f1_score: 0.5488 - val_loss: 0.7061 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5599 - f1_score: 0.5487 - val_loss: 0.7058 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6792 - acc: 0.5595 - f1_score: 0.5481 - val_loss: 0.7055 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5589 - f1_score: 0.5469 - val_loss: 0.7053 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6789 - acc: 0.5598 - f1_score: 0.5475 - val_loss: 0.7051 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6787 - acc: 0.5599 - f1_score: 0.5476 - val_loss: 0.7048 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5608 - f1_score: 0.5483 - val_loss: 0.7046 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6784 - acc: 0.5618 - f1_score: 0.5491 - val_loss: 0.7045 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6783 - acc: 0.5614 - f1_score: 0.5487 - val_loss: 0.7042 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6782 - acc: 0.5628 - f1_score: 0.5498 - val_loss: 0.7043 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6780 - acc: 0.5627 - f1_score: 0.5494 - val_loss: 0.7046 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6779 - acc: 0.5634 - f1_score: 0.5502 - val_loss: 0.7045 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6777 - acc: 0.5635 - f1_score: 0.5502 - val_loss: 0.7045 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6776 - acc: 0.5648 - f1_score: 0.5513 - val_loss: 0.7046 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6775 - acc: 0.5650 - f1_score: 0.5510 - val_loss: 0.7049 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6774 - acc: 0.5645 - f1_score: 0.5508 - val_loss: 0.7050 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6772 - acc: 0.5654 - f1_score: 0.5511 - val_loss: 0.7057 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6771 - acc: 0.5663 - f1_score: 0.5523 - val_loss: 0.7057 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 60/300\n",
      "27/27 - 3s - loss: 0.6770 - acc: 0.5664 - f1_score: 0.5523 - val_loss: 0.7058 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 61/300\n",
      "27/27 - 3s - loss: 0.6768 - acc: 0.5660 - f1_score: 0.5519 - val_loss: 0.7054 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 62/300\n",
      "27/27 - 3s - loss: 0.6767 - acc: 0.5666 - f1_score: 0.5522 - val_loss: 0.7053 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 63/300\n",
      "27/27 - 3s - loss: 0.6766 - acc: 0.5669 - f1_score: 0.5520 - val_loss: 0.7060 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 64/300\n",
      "27/27 - 3s - loss: 0.6765 - acc: 0.5670 - f1_score: 0.5522 - val_loss: 0.7067 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 65/300\n",
      "27/27 - 3s - loss: 0.6764 - acc: 0.5676 - f1_score: 0.5532 - val_loss: 0.7066 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 66/300\n",
      "27/27 - 3s - loss: 0.6762 - acc: 0.5679 - f1_score: 0.5531 - val_loss: 0.7071 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 67/300\n",
      "27/27 - 3s - loss: 0.6761 - acc: 0.5674 - f1_score: 0.5527 - val_loss: 0.7076 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 68/300\n",
      "27/27 - 3s - loss: 0.6760 - acc: 0.5683 - f1_score: 0.5535 - val_loss: 0.7079 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 69/300\n",
      "27/27 - 3s - loss: 0.6759 - acc: 0.5684 - f1_score: 0.5538 - val_loss: 0.7080 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 70/300\n",
      "27/27 - 3s - loss: 0.6758 - acc: 0.5684 - f1_score: 0.5538 - val_loss: 0.7078 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 71/300\n",
      "27/27 - 3s - loss: 0.6756 - acc: 0.5686 - f1_score: 0.5539 - val_loss: 0.7081 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 72/300\n",
      "27/27 - 3s - loss: 0.6755 - acc: 0.5686 - f1_score: 0.5539 - val_loss: 0.7083 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 73/300\n",
      "27/27 - 3s - loss: 0.6754 - acc: 0.5683 - f1_score: 0.5534 - val_loss: 0.7089 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 74/300\n",
      "27/27 - 3s - loss: 0.6753 - acc: 0.5689 - f1_score: 0.5538 - val_loss: 0.7095 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00074: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.15      0.24       176\n",
      "           1       0.65      0.96      0.77       292\n",
      "\n",
      "    accuracy                           0.65       468\n",
      "   macro avg       0.66      0.55      0.51       468\n",
      "weighted avg       0.66      0.65      0.57       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1175\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6945 - acc: 0.4518 - f1_score: 0.3119 - val_loss: 0.6855 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6940 - acc: 0.4515 - f1_score: 0.3122 - val_loss: 0.6861 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6934 - acc: 0.4535 - f1_score: 0.3218 - val_loss: 0.6869 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6929 - acc: 0.4690 - f1_score: 0.3774 - val_loss: 0.6876 - val_acc: 0.7109 - val_f1_score: 0.4562\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6923 - acc: 0.5149 - f1_score: 0.4874 - val_loss: 0.6881 - val_acc: 0.6658 - val_f1_score: 0.5255\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.5446 - f1_score: 0.5440 - val_loss: 0.6887 - val_acc: 0.6021 - val_f1_score: 0.5613\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6913 - acc: 0.5595 - f1_score: 0.5547 - val_loss: 0.6893 - val_acc: 0.5703 - val_f1_score: 0.5629\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6908 - acc: 0.5678 - f1_score: 0.5534 - val_loss: 0.6897 - val_acc: 0.5570 - val_f1_score: 0.5558\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5699 - f1_score: 0.5485 - val_loss: 0.6900 - val_acc: 0.5544 - val_f1_score: 0.5540\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5695 - f1_score: 0.5449 - val_loss: 0.6902 - val_acc: 0.5438 - val_f1_score: 0.5437\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6893 - acc: 0.5708 - f1_score: 0.5438 - val_loss: 0.6904 - val_acc: 0.5385 - val_f1_score: 0.5385\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6888 - acc: 0.5716 - f1_score: 0.5425 - val_loss: 0.6908 - val_acc: 0.5411 - val_f1_score: 0.5411\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5715 - f1_score: 0.5391 - val_loss: 0.6910 - val_acc: 0.5385 - val_f1_score: 0.5384\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6878 - acc: 0.5722 - f1_score: 0.5391 - val_loss: 0.6911 - val_acc: 0.5411 - val_f1_score: 0.5411\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5726 - f1_score: 0.5383 - val_loss: 0.6912 - val_acc: 0.5358 - val_f1_score: 0.5357\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5726 - f1_score: 0.5382 - val_loss: 0.6915 - val_acc: 0.5305 - val_f1_score: 0.5303\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5742 - f1_score: 0.5392 - val_loss: 0.6914 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5745 - f1_score: 0.5393 - val_loss: 0.6915 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5748 - f1_score: 0.5398 - val_loss: 0.6914 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5748 - f1_score: 0.5397 - val_loss: 0.6916 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6847 - acc: 0.5752 - f1_score: 0.5397 - val_loss: 0.6919 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5764 - f1_score: 0.5397 - val_loss: 0.6919 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6839 - acc: 0.5748 - f1_score: 0.5385 - val_loss: 0.6923 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5758 - f1_score: 0.5389 - val_loss: 0.6926 - val_acc: 0.5279 - val_f1_score: 0.5276\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5756 - f1_score: 0.5385 - val_loss: 0.6929 - val_acc: 0.5252 - val_f1_score: 0.5249\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5755 - f1_score: 0.5387 - val_loss: 0.6931 - val_acc: 0.5252 - val_f1_score: 0.5249\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5751 - f1_score: 0.5369 - val_loss: 0.6932 - val_acc: 0.5252 - val_f1_score: 0.5249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.86      0.62       153\n",
      "           1       0.80      0.37      0.51       224\n",
      "\n",
      "    accuracy                           0.57       377\n",
      "   macro avg       0.64      0.62      0.56       377\n",
      "weighted avg       0.67      0.57      0.55       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1194\n",
      "Epoch 1/300\n",
      "28/28 - 10s - loss: 0.6933 - acc: 0.4832 - f1_score: 0.3260 - val_loss: 0.7052 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6929 - acc: 0.4831 - f1_score: 0.3260 - val_loss: 0.7053 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4831 - f1_score: 0.3262 - val_loss: 0.7058 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6919 - acc: 0.4828 - f1_score: 0.3261 - val_loss: 0.7059 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6914 - acc: 0.4834 - f1_score: 0.3281 - val_loss: 0.7064 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.4841 - f1_score: 0.3318 - val_loss: 0.7075 - val_acc: 0.2022 - val_f1_score: 0.1697\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6904 - acc: 0.4857 - f1_score: 0.3361 - val_loss: 0.7077 - val_acc: 0.2044 - val_f1_score: 0.1727\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.4932 - f1_score: 0.3600 - val_loss: 0.7083 - val_acc: 0.2110 - val_f1_score: 0.1818\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6895 - acc: 0.5019 - f1_score: 0.3916 - val_loss: 0.7097 - val_acc: 0.2176 - val_f1_score: 0.1920\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6890 - acc: 0.5110 - f1_score: 0.4169 - val_loss: 0.7110 - val_acc: 0.2242 - val_f1_score: 0.2031\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5165 - f1_score: 0.4325 - val_loss: 0.7115 - val_acc: 0.2374 - val_f1_score: 0.2220\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5281 - f1_score: 0.4675 - val_loss: 0.7121 - val_acc: 0.2725 - val_f1_score: 0.2694\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5431 - f1_score: 0.5075 - val_loss: 0.7134 - val_acc: 0.3055 - val_f1_score: 0.3047\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5512 - f1_score: 0.5258 - val_loss: 0.7136 - val_acc: 0.3626 - val_f1_score: 0.3597\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5586 - f1_score: 0.5437 - val_loss: 0.7135 - val_acc: 0.4286 - val_f1_score: 0.4143\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5635 - f1_score: 0.5561 - val_loss: 0.7146 - val_acc: 0.4637 - val_f1_score: 0.4359\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6867 - acc: 0.5612 - f1_score: 0.5566 - val_loss: 0.7149 - val_acc: 0.4901 - val_f1_score: 0.4540\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6865 - acc: 0.5626 - f1_score: 0.5598 - val_loss: 0.7147 - val_acc: 0.4989 - val_f1_score: 0.4473\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5633 - f1_score: 0.5626 - val_loss: 0.7155 - val_acc: 0.4989 - val_f1_score: 0.4440\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5620 - f1_score: 0.5618 - val_loss: 0.7154 - val_acc: 0.5077 - val_f1_score: 0.4392\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5612 - f1_score: 0.5612 - val_loss: 0.7154 - val_acc: 0.5143 - val_f1_score: 0.4397\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5586 - f1_score: 0.5583 - val_loss: 0.7161 - val_acc: 0.5231 - val_f1_score: 0.4436\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5596 - f1_score: 0.5591 - val_loss: 0.7168 - val_acc: 0.5231 - val_f1_score: 0.4414\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5602 - f1_score: 0.5595 - val_loss: 0.7162 - val_acc: 0.5319 - val_f1_score: 0.4450\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5622 - f1_score: 0.5609 - val_loss: 0.7165 - val_acc: 0.5385 - val_f1_score: 0.4494\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5613 - f1_score: 0.5597 - val_loss: 0.7173 - val_acc: 0.5407 - val_f1_score: 0.4509\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5622 - f1_score: 0.5605 - val_loss: 0.7171 - val_acc: 0.5385 - val_f1_score: 0.4447\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5617 - f1_score: 0.5595 - val_loss: 0.7178 - val_acc: 0.5407 - val_f1_score: 0.4461\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5617 - f1_score: 0.5591 - val_loss: 0.7177 - val_acc: 0.5451 - val_f1_score: 0.4465\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5623 - f1_score: 0.5593 - val_loss: 0.7180 - val_acc: 0.5473 - val_f1_score: 0.4479\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5616 - f1_score: 0.5584 - val_loss: 0.7172 - val_acc: 0.5538 - val_f1_score: 0.4496\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5612 - f1_score: 0.5572 - val_loss: 0.7170 - val_acc: 0.5538 - val_f1_score: 0.4496\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5617 - f1_score: 0.5573 - val_loss: 0.7153 - val_acc: 0.5560 - val_f1_score: 0.4510\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5616 - f1_score: 0.5563 - val_loss: 0.7149 - val_acc: 0.5560 - val_f1_score: 0.4510\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6828 - acc: 0.5626 - f1_score: 0.5569 - val_loss: 0.7132 - val_acc: 0.5582 - val_f1_score: 0.4524\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5617 - f1_score: 0.5552 - val_loss: 0.7120 - val_acc: 0.5582 - val_f1_score: 0.4498\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6825 - acc: 0.5599 - f1_score: 0.5526 - val_loss: 0.7113 - val_acc: 0.5582 - val_f1_score: 0.4498\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00037: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.21      0.31       247\n",
      "           1       0.47      0.82      0.59       208\n",
      "\n",
      "    accuracy                           0.49       455\n",
      "   macro avg       0.52      0.52      0.45       455\n",
      "weighted avg       0.53      0.49      0.44       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1337\n",
      "Epoch 1/300\n",
      "27/27 - 15s - loss: 0.6944 - acc: 0.4484 - f1_score: 0.3100 - val_loss: 0.6889 - val_acc: 0.7158 - val_f1_score: 0.4314\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6938 - acc: 0.4497 - f1_score: 0.3126 - val_loss: 0.6895 - val_acc: 0.7115 - val_f1_score: 0.5560\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6933 - acc: 0.4617 - f1_score: 0.3493 - val_loss: 0.6900 - val_acc: 0.6624 - val_f1_score: 0.6138\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6927 - acc: 0.4965 - f1_score: 0.4362 - val_loss: 0.6905 - val_acc: 0.6026 - val_f1_score: 0.5935\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.5360 - f1_score: 0.5293 - val_loss: 0.6909 - val_acc: 0.5641 - val_f1_score: 0.5621\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.5537 - f1_score: 0.5499 - val_loss: 0.6912 - val_acc: 0.5342 - val_f1_score: 0.5336\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5674 - f1_score: 0.5501 - val_loss: 0.6914 - val_acc: 0.5107 - val_f1_score: 0.5106\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6907 - acc: 0.5721 - f1_score: 0.5428 - val_loss: 0.6916 - val_acc: 0.5085 - val_f1_score: 0.5085\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6903 - acc: 0.5734 - f1_score: 0.5375 - val_loss: 0.6916 - val_acc: 0.5064 - val_f1_score: 0.5064\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6898 - acc: 0.5726 - f1_score: 0.5337 - val_loss: 0.6916 - val_acc: 0.5085 - val_f1_score: 0.5085\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6893 - acc: 0.5750 - f1_score: 0.5328 - val_loss: 0.6915 - val_acc: 0.5085 - val_f1_score: 0.5085\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6889 - acc: 0.5767 - f1_score: 0.5326 - val_loss: 0.6913 - val_acc: 0.5107 - val_f1_score: 0.5107\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5767 - f1_score: 0.5317 - val_loss: 0.6911 - val_acc: 0.5192 - val_f1_score: 0.5192\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5769 - f1_score: 0.5305 - val_loss: 0.6909 - val_acc: 0.5192 - val_f1_score: 0.5192\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6876 - acc: 0.5755 - f1_score: 0.5290 - val_loss: 0.6907 - val_acc: 0.5214 - val_f1_score: 0.5214\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5761 - f1_score: 0.5286 - val_loss: 0.6904 - val_acc: 0.5214 - val_f1_score: 0.5214\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5763 - f1_score: 0.5298 - val_loss: 0.6901 - val_acc: 0.5214 - val_f1_score: 0.5214\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5764 - f1_score: 0.5288 - val_loss: 0.6898 - val_acc: 0.5235 - val_f1_score: 0.5235\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5758 - f1_score: 0.5290 - val_loss: 0.6894 - val_acc: 0.5235 - val_f1_score: 0.5235\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5770 - f1_score: 0.5297 - val_loss: 0.6890 - val_acc: 0.5278 - val_f1_score: 0.5277\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5766 - f1_score: 0.5294 - val_loss: 0.6886 - val_acc: 0.5321 - val_f1_score: 0.5319\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6850 - acc: 0.5771 - f1_score: 0.5304 - val_loss: 0.6882 - val_acc: 0.5299 - val_f1_score: 0.5298\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5766 - f1_score: 0.5302 - val_loss: 0.6877 - val_acc: 0.5321 - val_f1_score: 0.5319\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75       296\n",
      "           1       0.55      0.42      0.48       172\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.63      0.61      0.61       468\n",
      "weighted avg       0.65      0.66      0.65       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1390\n",
      "Epoch 1/300\n",
      "27/27 - 14s - loss: 0.6940 - acc: 0.4669 - f1_score: 0.3188 - val_loss: 0.6941 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6935 - acc: 0.4666 - f1_score: 0.3193 - val_loss: 0.6933 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6930 - acc: 0.4707 - f1_score: 0.3358 - val_loss: 0.6924 - val_acc: 0.4423 - val_f1_score: 0.3067\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6926 - acc: 0.4853 - f1_score: 0.3860 - val_loss: 0.6915 - val_acc: 0.4466 - val_f1_score: 0.3303\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6921 - acc: 0.5135 - f1_score: 0.4634 - val_loss: 0.6907 - val_acc: 0.4701 - val_f1_score: 0.4057\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.5443 - f1_score: 0.5353 - val_loss: 0.6898 - val_acc: 0.5278 - val_f1_score: 0.5160\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5550 - f1_score: 0.5547 - val_loss: 0.6889 - val_acc: 0.5684 - val_f1_score: 0.5683\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.5582 - f1_score: 0.5547 - val_loss: 0.6880 - val_acc: 0.5791 - val_f1_score: 0.5723\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.5608 - f1_score: 0.5525 - val_loss: 0.6871 - val_acc: 0.6004 - val_f1_score: 0.5854\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6899 - acc: 0.5609 - f1_score: 0.5493 - val_loss: 0.6863 - val_acc: 0.6111 - val_f1_score: 0.5917\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6895 - acc: 0.5608 - f1_score: 0.5468 - val_loss: 0.6854 - val_acc: 0.6068 - val_f1_score: 0.5838\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5586 - f1_score: 0.5427 - val_loss: 0.6845 - val_acc: 0.6175 - val_f1_score: 0.5911\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6887 - acc: 0.5608 - f1_score: 0.5438 - val_loss: 0.6836 - val_acc: 0.6218 - val_f1_score: 0.5937\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5622 - f1_score: 0.5441 - val_loss: 0.6828 - val_acc: 0.6218 - val_f1_score: 0.5937\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5618 - f1_score: 0.5430 - val_loss: 0.6819 - val_acc: 0.6197 - val_f1_score: 0.5899\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6875 - acc: 0.5616 - f1_score: 0.5421 - val_loss: 0.6811 - val_acc: 0.6197 - val_f1_score: 0.5889\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5616 - f1_score: 0.5418 - val_loss: 0.6802 - val_acc: 0.6218 - val_f1_score: 0.5907\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6868 - acc: 0.5615 - f1_score: 0.5411 - val_loss: 0.6794 - val_acc: 0.6239 - val_f1_score: 0.5914\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6864 - acc: 0.5614 - f1_score: 0.5411 - val_loss: 0.6786 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6861 - acc: 0.5615 - f1_score: 0.5407 - val_loss: 0.6778 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6858 - acc: 0.5624 - f1_score: 0.5413 - val_loss: 0.6770 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5622 - f1_score: 0.5412 - val_loss: 0.6763 - val_acc: 0.6261 - val_f1_score: 0.5932\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5629 - f1_score: 0.5416 - val_loss: 0.6755 - val_acc: 0.6239 - val_f1_score: 0.5914\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5632 - f1_score: 0.5418 - val_loss: 0.6748 - val_acc: 0.6282 - val_f1_score: 0.5950\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6846 - acc: 0.5625 - f1_score: 0.5410 - val_loss: 0.6742 - val_acc: 0.6282 - val_f1_score: 0.5950\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5624 - f1_score: 0.5406 - val_loss: 0.6735 - val_acc: 0.6282 - val_f1_score: 0.5950\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5627 - f1_score: 0.5406 - val_loss: 0.6729 - val_acc: 0.6303 - val_f1_score: 0.5968\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5632 - f1_score: 0.5408 - val_loss: 0.6723 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5640 - f1_score: 0.5412 - val_loss: 0.6718 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6834 - acc: 0.5640 - f1_score: 0.5411 - val_loss: 0.6712 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5640 - f1_score: 0.5405 - val_loss: 0.6707 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5638 - f1_score: 0.5407 - val_loss: 0.6703 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5653 - f1_score: 0.5414 - val_loss: 0.6698 - val_acc: 0.6282 - val_f1_score: 0.5939\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5657 - f1_score: 0.5417 - val_loss: 0.6693 - val_acc: 0.6303 - val_f1_score: 0.5957\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5669 - f1_score: 0.5422 - val_loss: 0.6689 - val_acc: 0.6303 - val_f1_score: 0.5946\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5664 - f1_score: 0.5421 - val_loss: 0.6685 - val_acc: 0.6325 - val_f1_score: 0.5964\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5666 - f1_score: 0.5418 - val_loss: 0.6681 - val_acc: 0.6325 - val_f1_score: 0.5964\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5664 - f1_score: 0.5416 - val_loss: 0.6677 - val_acc: 0.6346 - val_f1_score: 0.5981\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5664 - f1_score: 0.5411 - val_loss: 0.6674 - val_acc: 0.6368 - val_f1_score: 0.5999\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5674 - f1_score: 0.5420 - val_loss: 0.6670 - val_acc: 0.6346 - val_f1_score: 0.5970\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5683 - f1_score: 0.5422 - val_loss: 0.6667 - val_acc: 0.6346 - val_f1_score: 0.5970\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5684 - f1_score: 0.5426 - val_loss: 0.6663 - val_acc: 0.6346 - val_f1_score: 0.5970\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5682 - f1_score: 0.5421 - val_loss: 0.6660 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5684 - f1_score: 0.5419 - val_loss: 0.6657 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5690 - f1_score: 0.5426 - val_loss: 0.6654 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5683 - f1_score: 0.5415 - val_loss: 0.6651 - val_acc: 0.6325 - val_f1_score: 0.5940\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5690 - f1_score: 0.5420 - val_loss: 0.6649 - val_acc: 0.6346 - val_f1_score: 0.5958\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5695 - f1_score: 0.5422 - val_loss: 0.6646 - val_acc: 0.6346 - val_f1_score: 0.5958\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5692 - f1_score: 0.5415 - val_loss: 0.6644 - val_acc: 0.6346 - val_f1_score: 0.5958\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5690 - f1_score: 0.5416 - val_loss: 0.6642 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5686 - f1_score: 0.5406 - val_loss: 0.6639 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5696 - f1_score: 0.5410 - val_loss: 0.6636 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5690 - f1_score: 0.5413 - val_loss: 0.6634 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5689 - f1_score: 0.5407 - val_loss: 0.6632 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6798 - acc: 0.5692 - f1_score: 0.5408 - val_loss: 0.6630 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5709 - f1_score: 0.5418 - val_loss: 0.6628 - val_acc: 0.6303 - val_f1_score: 0.5899\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5696 - f1_score: 0.5411 - val_loss: 0.6626 - val_acc: 0.6282 - val_f1_score: 0.5869\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5716 - f1_score: 0.5421 - val_loss: 0.6623 - val_acc: 0.6282 - val_f1_score: 0.5869\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5700 - f1_score: 0.5415 - val_loss: 0.6622 - val_acc: 0.6303 - val_f1_score: 0.5886\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00059: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.66      0.48       118\n",
      "           1       0.85      0.63      0.72       350\n",
      "\n",
      "    accuracy                           0.64       468\n",
      "   macro avg       0.61      0.64      0.60       468\n",
      "weighted avg       0.73      0.64      0.66       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1400\n",
      "Epoch 1/300\n",
      "27/27 - 16s - loss: 0.6931 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7078 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6926 - acc: 0.4876 - f1_score: 0.3280 - val_loss: 0.7082 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6920 - acc: 0.4875 - f1_score: 0.3285 - val_loss: 0.7088 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.4885 - f1_score: 0.3311 - val_loss: 0.7094 - val_acc: 0.1410 - val_f1_score: 0.1247\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6909 - acc: 0.4904 - f1_score: 0.3383 - val_loss: 0.7101 - val_acc: 0.1410 - val_f1_score: 0.1247\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6903 - acc: 0.4991 - f1_score: 0.3678 - val_loss: 0.7111 - val_acc: 0.1410 - val_f1_score: 0.1267\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6897 - acc: 0.5067 - f1_score: 0.3917 - val_loss: 0.7120 - val_acc: 0.1581 - val_f1_score: 0.1492\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6891 - acc: 0.5264 - f1_score: 0.4504 - val_loss: 0.7131 - val_acc: 0.1944 - val_f1_score: 0.1931\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5415 - f1_score: 0.4947 - val_loss: 0.7143 - val_acc: 0.2628 - val_f1_score: 0.2610\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5569 - f1_score: 0.5318 - val_loss: 0.7156 - val_acc: 0.3611 - val_f1_score: 0.3443\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5642 - f1_score: 0.5549 - val_loss: 0.7170 - val_acc: 0.4124 - val_f1_score: 0.3787\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5676 - f1_score: 0.5649 - val_loss: 0.7183 - val_acc: 0.4509 - val_f1_score: 0.4026\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5602 - f1_score: 0.5598 - val_loss: 0.7196 - val_acc: 0.4872 - val_f1_score: 0.4212\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5566 - f1_score: 0.5565 - val_loss: 0.7210 - val_acc: 0.5171 - val_f1_score: 0.4354\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5598 - f1_score: 0.5593 - val_loss: 0.7222 - val_acc: 0.5278 - val_f1_score: 0.4403\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5586 - f1_score: 0.5576 - val_loss: 0.7233 - val_acc: 0.5385 - val_f1_score: 0.4427\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5598 - f1_score: 0.5582 - val_loss: 0.7243 - val_acc: 0.5427 - val_f1_score: 0.4406\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6839 - acc: 0.5608 - f1_score: 0.5584 - val_loss: 0.7252 - val_acc: 0.5449 - val_f1_score: 0.4394\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5593 - f1_score: 0.5565 - val_loss: 0.7259 - val_acc: 0.5577 - val_f1_score: 0.4475\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6832 - acc: 0.5615 - f1_score: 0.5579 - val_loss: 0.7265 - val_acc: 0.5556 - val_f1_score: 0.4435\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5614 - f1_score: 0.5572 - val_loss: 0.7272 - val_acc: 0.5598 - val_f1_score: 0.4435\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5608 - f1_score: 0.5562 - val_loss: 0.7274 - val_acc: 0.5684 - val_f1_score: 0.4459\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.5602 - f1_score: 0.5550 - val_loss: 0.7276 - val_acc: 0.5748 - val_f1_score: 0.4499\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6821 - acc: 0.5599 - f1_score: 0.5544 - val_loss: 0.7277 - val_acc: 0.5812 - val_f1_score: 0.4538\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5600 - f1_score: 0.5543 - val_loss: 0.7274 - val_acc: 0.5833 - val_f1_score: 0.4551\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5602 - f1_score: 0.5540 - val_loss: 0.7272 - val_acc: 0.5897 - val_f1_score: 0.4591\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5608 - f1_score: 0.5542 - val_loss: 0.7268 - val_acc: 0.5919 - val_f1_score: 0.4574\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6812 - acc: 0.5592 - f1_score: 0.5521 - val_loss: 0.7265 - val_acc: 0.5919 - val_f1_score: 0.4574\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5590 - f1_score: 0.5516 - val_loss: 0.7262 - val_acc: 0.6004 - val_f1_score: 0.4626\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5593 - f1_score: 0.5516 - val_loss: 0.7257 - val_acc: 0.6026 - val_f1_score: 0.4607\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5599 - f1_score: 0.5516 - val_loss: 0.7254 - val_acc: 0.6068 - val_f1_score: 0.4633\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5598 - f1_score: 0.5515 - val_loss: 0.7246 - val_acc: 0.6068 - val_f1_score: 0.4601\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5587 - f1_score: 0.5499 - val_loss: 0.7240 - val_acc: 0.6047 - val_f1_score: 0.4555\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5593 - f1_score: 0.5503 - val_loss: 0.7233 - val_acc: 0.6047 - val_f1_score: 0.4521\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5586 - f1_score: 0.5490 - val_loss: 0.7230 - val_acc: 0.6047 - val_f1_score: 0.4521\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5579 - f1_score: 0.5483 - val_loss: 0.7221 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6799 - acc: 0.5582 - f1_score: 0.5480 - val_loss: 0.7216 - val_acc: 0.6090 - val_f1_score: 0.4546\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6797 - acc: 0.5577 - f1_score: 0.5474 - val_loss: 0.7208 - val_acc: 0.6111 - val_f1_score: 0.4558\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6796 - acc: 0.5580 - f1_score: 0.5472 - val_loss: 0.7203 - val_acc: 0.6111 - val_f1_score: 0.4558\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6795 - acc: 0.5574 - f1_score: 0.5465 - val_loss: 0.7196 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6794 - acc: 0.5590 - f1_score: 0.5473 - val_loss: 0.7192 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6793 - acc: 0.5587 - f1_score: 0.5471 - val_loss: 0.7187 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6792 - acc: 0.5590 - f1_score: 0.5472 - val_loss: 0.7182 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6791 - acc: 0.5589 - f1_score: 0.5467 - val_loss: 0.7177 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6790 - acc: 0.5587 - f1_score: 0.5465 - val_loss: 0.7170 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6789 - acc: 0.5592 - f1_score: 0.5469 - val_loss: 0.7161 - val_acc: 0.6111 - val_f1_score: 0.4523\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6788 - acc: 0.5603 - f1_score: 0.5477 - val_loss: 0.7153 - val_acc: 0.6132 - val_f1_score: 0.4535\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6787 - acc: 0.5615 - f1_score: 0.5484 - val_loss: 0.7149 - val_acc: 0.6154 - val_f1_score: 0.4548\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6786 - acc: 0.5616 - f1_score: 0.5483 - val_loss: 0.7145 - val_acc: 0.6154 - val_f1_score: 0.4548\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5618 - f1_score: 0.5485 - val_loss: 0.7140 - val_acc: 0.6154 - val_f1_score: 0.4548\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6785 - acc: 0.5625 - f1_score: 0.5490 - val_loss: 0.7135 - val_acc: 0.6175 - val_f1_score: 0.4560\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.13      0.19       161\n",
      "           1       0.65      0.86      0.74       307\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.49      0.49      0.46       468\n",
      "weighted avg       0.54      0.61      0.55       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1419\n",
      "Epoch 1/300\n",
      "28/28 - 9s - loss: 0.6937 - acc: 0.4756 - f1_score: 0.3225 - val_loss: 0.6988 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 2/300\n",
      "28/28 - 5s - loss: 0.6934 - acc: 0.4756 - f1_score: 0.3228 - val_loss: 0.6977 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6930 - acc: 0.4751 - f1_score: 0.3228 - val_loss: 0.6965 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4766 - f1_score: 0.3275 - val_loss: 0.6955 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6924 - acc: 0.4796 - f1_score: 0.3389 - val_loss: 0.6947 - val_acc: 0.3187 - val_f1_score: 0.2463\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4848 - f1_score: 0.3593 - val_loss: 0.6940 - val_acc: 0.3231 - val_f1_score: 0.2534\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.4957 - f1_score: 0.3942 - val_loss: 0.6931 - val_acc: 0.3275 - val_f1_score: 0.2763\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5117 - f1_score: 0.4405 - val_loss: 0.6920 - val_acc: 0.3582 - val_f1_score: 0.3327\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.5328 - f1_score: 0.4981 - val_loss: 0.6913 - val_acc: 0.4330 - val_f1_score: 0.4258\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5447 - f1_score: 0.5247 - val_loss: 0.6907 - val_acc: 0.4769 - val_f1_score: 0.4769\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6906 - acc: 0.5535 - f1_score: 0.5420 - val_loss: 0.6899 - val_acc: 0.5253 - val_f1_score: 0.5221\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5552 - f1_score: 0.5524 - val_loss: 0.6889 - val_acc: 0.5714 - val_f1_score: 0.5610\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.5519 - f1_score: 0.5518 - val_loss: 0.6881 - val_acc: 0.6000 - val_f1_score: 0.5826\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5495 - f1_score: 0.5490 - val_loss: 0.6874 - val_acc: 0.6110 - val_f1_score: 0.5904\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.5510 - f1_score: 0.5500 - val_loss: 0.6867 - val_acc: 0.6286 - val_f1_score: 0.6055\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5523 - f1_score: 0.5502 - val_loss: 0.6861 - val_acc: 0.6330 - val_f1_score: 0.6093\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.5542 - f1_score: 0.5518 - val_loss: 0.6856 - val_acc: 0.6330 - val_f1_score: 0.6093\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6888 - acc: 0.5539 - f1_score: 0.5513 - val_loss: 0.6850 - val_acc: 0.6374 - val_f1_score: 0.6122\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5555 - f1_score: 0.5520 - val_loss: 0.6844 - val_acc: 0.6396 - val_f1_score: 0.6140\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6883 - acc: 0.5568 - f1_score: 0.5530 - val_loss: 0.6836 - val_acc: 0.6549 - val_f1_score: 0.6243\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5573 - f1_score: 0.5520 - val_loss: 0.6829 - val_acc: 0.6505 - val_f1_score: 0.6184\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5558 - f1_score: 0.5497 - val_loss: 0.6823 - val_acc: 0.6505 - val_f1_score: 0.6184\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5567 - f1_score: 0.5502 - val_loss: 0.6817 - val_acc: 0.6505 - val_f1_score: 0.6184\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6875 - acc: 0.5558 - f1_score: 0.5492 - val_loss: 0.6809 - val_acc: 0.6549 - val_f1_score: 0.6211\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5554 - f1_score: 0.5474 - val_loss: 0.6804 - val_acc: 0.6549 - val_f1_score: 0.6211\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6872 - acc: 0.5554 - f1_score: 0.5472 - val_loss: 0.6798 - val_acc: 0.6549 - val_f1_score: 0.6211\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5551 - f1_score: 0.5469 - val_loss: 0.6791 - val_acc: 0.6527 - val_f1_score: 0.6181\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5555 - f1_score: 0.5468 - val_loss: 0.6785 - val_acc: 0.6505 - val_f1_score: 0.6151\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5558 - f1_score: 0.5469 - val_loss: 0.6778 - val_acc: 0.6505 - val_f1_score: 0.6151\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5560 - f1_score: 0.5468 - val_loss: 0.6771 - val_acc: 0.6505 - val_f1_score: 0.6151\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6862 - acc: 0.5552 - f1_score: 0.5458 - val_loss: 0.6763 - val_acc: 0.6527 - val_f1_score: 0.6159\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5538 - f1_score: 0.5437 - val_loss: 0.6756 - val_acc: 0.6527 - val_f1_score: 0.6147\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6858 - acc: 0.5536 - f1_score: 0.5433 - val_loss: 0.6748 - val_acc: 0.6505 - val_f1_score: 0.6117\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5542 - f1_score: 0.5428 - val_loss: 0.6740 - val_acc: 0.6549 - val_f1_score: 0.6153\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5557 - f1_score: 0.5438 - val_loss: 0.6730 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5565 - f1_score: 0.5433 - val_loss: 0.6726 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5568 - f1_score: 0.5439 - val_loss: 0.6721 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5570 - f1_score: 0.5435 - val_loss: 0.6716 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6850 - acc: 0.5565 - f1_score: 0.5430 - val_loss: 0.6710 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5562 - f1_score: 0.5423 - val_loss: 0.6706 - val_acc: 0.6615 - val_f1_score: 0.6196\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00040: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.46      0.52       182\n",
      "           1       0.69      0.78      0.73       273\n",
      "\n",
      "    accuracy                           0.65       455\n",
      "   macro avg       0.64      0.62      0.62       455\n",
      "weighted avg       0.65      0.65      0.65       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1517\n",
      "Epoch 1/300\n",
      "28/28 - 16s - loss: 0.6948 - acc: 0.4426 - f1_score: 0.3072 - val_loss: 0.6802 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6940 - acc: 0.4422 - f1_score: 0.3117 - val_loss: 0.6834 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4598 - f1_score: 0.3716 - val_loss: 0.6864 - val_acc: 0.8519 - val_f1_score: 0.4600\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6925 - acc: 0.5110 - f1_score: 0.4879 - val_loss: 0.6894 - val_acc: 0.5698 - val_f1_score: 0.3749\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6918 - acc: 0.5516 - f1_score: 0.5501 - val_loss: 0.6923 - val_acc: 0.3903 - val_f1_score: 0.3050\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6911 - acc: 0.5765 - f1_score: 0.5573 - val_loss: 0.6949 - val_acc: 0.3162 - val_f1_score: 0.2646\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5811 - f1_score: 0.5512 - val_loss: 0.6976 - val_acc: 0.2877 - val_f1_score: 0.2497\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5825 - f1_score: 0.5446 - val_loss: 0.7000 - val_acc: 0.2792 - val_f1_score: 0.2475\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6891 - acc: 0.5873 - f1_score: 0.5456 - val_loss: 0.7022 - val_acc: 0.2764 - val_f1_score: 0.2473\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5892 - f1_score: 0.5451 - val_loss: 0.7043 - val_acc: 0.2678 - val_f1_score: 0.2410\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5897 - f1_score: 0.5450 - val_loss: 0.7062 - val_acc: 0.2678 - val_f1_score: 0.2410\n",
      "Epoch 12/300\n",
      "28/28 - 5s - loss: 0.6873 - acc: 0.5889 - f1_score: 0.5434 - val_loss: 0.7084 - val_acc: 0.2678 - val_f1_score: 0.2410\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5890 - f1_score: 0.5425 - val_loss: 0.7102 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5896 - f1_score: 0.5430 - val_loss: 0.7119 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5895 - f1_score: 0.5433 - val_loss: 0.7138 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5893 - f1_score: 0.5417 - val_loss: 0.7158 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5890 - f1_score: 0.5425 - val_loss: 0.7175 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5883 - f1_score: 0.5418 - val_loss: 0.7193 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6832 - acc: 0.5892 - f1_score: 0.5432 - val_loss: 0.7209 - val_acc: 0.2650 - val_f1_score: 0.2389\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5886 - f1_score: 0.5437 - val_loss: 0.7227 - val_acc: 0.2678 - val_f1_score: 0.2410\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5895 - f1_score: 0.5435 - val_loss: 0.7245 - val_acc: 0.2678 - val_f1_score: 0.2410\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       351\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       351\n",
      "   macro avg       0.50      0.46      0.48       351\n",
      "weighted avg       1.00      0.93      0.96       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1544\n",
      "Epoch 1/300\n",
      "29/29 - 22s - loss: 0.6940 - acc: 0.4669 - f1_score: 0.3185 - val_loss: 0.6932 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/300\n",
      "29/29 - 3s - loss: 0.6935 - acc: 0.4669 - f1_score: 0.3196 - val_loss: 0.6913 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 3/300\n",
      "29/29 - 3s - loss: 0.6930 - acc: 0.4717 - f1_score: 0.3353 - val_loss: 0.6894 - val_acc: 0.3846 - val_f1_score: 0.3110\n",
      "Epoch 4/300\n",
      "29/29 - 3s - loss: 0.6925 - acc: 0.4844 - f1_score: 0.3845 - val_loss: 0.6875 - val_acc: 0.5288 - val_f1_score: 0.5159\n",
      "Epoch 5/300\n",
      "29/29 - 3s - loss: 0.6920 - acc: 0.5140 - f1_score: 0.4705 - val_loss: 0.6854 - val_acc: 0.6731 - val_f1_score: 0.6730\n",
      "Epoch 6/300\n",
      "29/29 - 3s - loss: 0.6915 - acc: 0.5423 - f1_score: 0.5356 - val_loss: 0.6833 - val_acc: 0.7500 - val_f1_score: 0.7454\n",
      "Epoch 7/300\n",
      "29/29 - 3s - loss: 0.6910 - acc: 0.5517 - f1_score: 0.5514 - val_loss: 0.6812 - val_acc: 0.7692 - val_f1_score: 0.7621\n",
      "Epoch 8/300\n",
      "29/29 - 3s - loss: 0.6905 - acc: 0.5548 - f1_score: 0.5503 - val_loss: 0.6791 - val_acc: 0.8077 - val_f1_score: 0.7987\n",
      "Epoch 9/300\n",
      "29/29 - 3s - loss: 0.6900 - acc: 0.5601 - f1_score: 0.5503 - val_loss: 0.6769 - val_acc: 0.8077 - val_f1_score: 0.7987\n",
      "Epoch 10/300\n",
      "29/29 - 3s - loss: 0.6896 - acc: 0.5592 - f1_score: 0.5468 - val_loss: 0.6748 - val_acc: 0.7981 - val_f1_score: 0.7877\n",
      "Epoch 11/300\n",
      "29/29 - 3s - loss: 0.6891 - acc: 0.5583 - f1_score: 0.5437 - val_loss: 0.6726 - val_acc: 0.7885 - val_f1_score: 0.7766\n",
      "Epoch 12/300\n",
      "29/29 - 3s - loss: 0.6886 - acc: 0.5595 - f1_score: 0.5425 - val_loss: 0.6704 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 13/300\n",
      "29/29 - 3s - loss: 0.6882 - acc: 0.5605 - f1_score: 0.5432 - val_loss: 0.6682 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 14/300\n",
      "29/29 - 3s - loss: 0.6878 - acc: 0.5614 - f1_score: 0.5430 - val_loss: 0.6661 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 15/300\n",
      "29/29 - 3s - loss: 0.6873 - acc: 0.5614 - f1_score: 0.5425 - val_loss: 0.6639 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 16/300\n",
      "29/29 - 3s - loss: 0.6869 - acc: 0.5614 - f1_score: 0.5421 - val_loss: 0.6620 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 17/300\n",
      "29/29 - 3s - loss: 0.6866 - acc: 0.5613 - f1_score: 0.5416 - val_loss: 0.6599 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 18/300\n",
      "29/29 - 3s - loss: 0.6862 - acc: 0.5616 - f1_score: 0.5415 - val_loss: 0.6578 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 19/300\n",
      "29/29 - 3s - loss: 0.6858 - acc: 0.5620 - f1_score: 0.5416 - val_loss: 0.6558 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 20/300\n",
      "29/29 - 3s - loss: 0.6855 - acc: 0.5616 - f1_score: 0.5409 - val_loss: 0.6539 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 21/300\n",
      "29/29 - 3s - loss: 0.6852 - acc: 0.5623 - f1_score: 0.5416 - val_loss: 0.6520 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 22/300\n",
      "29/29 - 3s - loss: 0.6848 - acc: 0.5619 - f1_score: 0.5408 - val_loss: 0.6502 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 23/300\n",
      "29/29 - 3s - loss: 0.6845 - acc: 0.5624 - f1_score: 0.5412 - val_loss: 0.6485 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 24/300\n",
      "29/29 - 3s - loss: 0.6843 - acc: 0.5623 - f1_score: 0.5406 - val_loss: 0.6469 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 25/300\n",
      "29/29 - 3s - loss: 0.6840 - acc: 0.5623 - f1_score: 0.5401 - val_loss: 0.6453 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 26/300\n",
      "29/29 - 3s - loss: 0.6837 - acc: 0.5627 - f1_score: 0.5401 - val_loss: 0.6437 - val_acc: 0.7981 - val_f1_score: 0.7835\n",
      "Epoch 27/300\n",
      "29/29 - 3s - loss: 0.6835 - acc: 0.5631 - f1_score: 0.5401 - val_loss: 0.6422 - val_acc: 0.7981 - val_f1_score: 0.7835\n",
      "Epoch 28/300\n",
      "29/29 - 3s - loss: 0.6832 - acc: 0.5641 - f1_score: 0.5406 - val_loss: 0.6407 - val_acc: 0.7981 - val_f1_score: 0.7835\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.76        43\n",
      "           1       0.82      0.87      0.84        61\n",
      "\n",
      "    accuracy                           0.81       104\n",
      "   macro avg       0.81      0.79      0.80       104\n",
      "weighted avg       0.81      0.81      0.81       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1624\n",
      "Epoch 1/300\n",
      "28/28 - 16s - loss: 0.6944 - acc: 0.4554 - f1_score: 0.3131 - val_loss: 0.6876 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6938 - acc: 0.4554 - f1_score: 0.3145 - val_loss: 0.6882 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4605 - f1_score: 0.3343 - val_loss: 0.6887 - val_acc: 0.6581 - val_f1_score: 0.3969\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4792 - f1_score: 0.3916 - val_loss: 0.6893 - val_acc: 0.6239 - val_f1_score: 0.4234\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.5256 - f1_score: 0.5023 - val_loss: 0.6897 - val_acc: 0.5442 - val_f1_score: 0.4622\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5504 - f1_score: 0.5503 - val_loss: 0.6902 - val_acc: 0.4786 - val_f1_score: 0.4570\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6910 - acc: 0.5638 - f1_score: 0.5590 - val_loss: 0.6906 - val_acc: 0.4615 - val_f1_score: 0.4609\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5725 - f1_score: 0.5581 - val_loss: 0.6909 - val_acc: 0.4416 - val_f1_score: 0.4408\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6899 - acc: 0.5728 - f1_score: 0.5544 - val_loss: 0.6912 - val_acc: 0.4444 - val_f1_score: 0.4418\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6894 - acc: 0.5745 - f1_score: 0.5523 - val_loss: 0.6914 - val_acc: 0.4473 - val_f1_score: 0.4429\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6889 - acc: 0.5741 - f1_score: 0.5505 - val_loss: 0.6916 - val_acc: 0.4473 - val_f1_score: 0.4424\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6884 - acc: 0.5744 - f1_score: 0.5493 - val_loss: 0.6917 - val_acc: 0.4444 - val_f1_score: 0.4392\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6879 - acc: 0.5744 - f1_score: 0.5486 - val_loss: 0.6918 - val_acc: 0.4501 - val_f1_score: 0.4443\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6874 - acc: 0.5749 - f1_score: 0.5484 - val_loss: 0.6919 - val_acc: 0.4473 - val_f1_score: 0.4411\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5751 - f1_score: 0.5482 - val_loss: 0.6920 - val_acc: 0.4444 - val_f1_score: 0.4379\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5746 - f1_score: 0.5460 - val_loss: 0.6922 - val_acc: 0.4416 - val_f1_score: 0.4346\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5756 - f1_score: 0.5475 - val_loss: 0.6922 - val_acc: 0.4473 - val_f1_score: 0.4404\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6855 - acc: 0.5752 - f1_score: 0.5472 - val_loss: 0.6924 - val_acc: 0.4558 - val_f1_score: 0.4479\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5758 - f1_score: 0.5472 - val_loss: 0.6925 - val_acc: 0.4558 - val_f1_score: 0.4479\n",
      "Epoch 20/300\n",
      "28/28 - 4s - loss: 0.6846 - acc: 0.5746 - f1_score: 0.5466 - val_loss: 0.6926 - val_acc: 0.4558 - val_f1_score: 0.4479\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6842 - acc: 0.5751 - f1_score: 0.5460 - val_loss: 0.6928 - val_acc: 0.4530 - val_f1_score: 0.4447\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5756 - f1_score: 0.5467 - val_loss: 0.6929 - val_acc: 0.4530 - val_f1_score: 0.4447\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6834 - acc: 0.5764 - f1_score: 0.5475 - val_loss: 0.6932 - val_acc: 0.4558 - val_f1_score: 0.4472\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5761 - f1_score: 0.5472 - val_loss: 0.6934 - val_acc: 0.4530 - val_f1_score: 0.4439\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5764 - f1_score: 0.5470 - val_loss: 0.6937 - val_acc: 0.4530 - val_f1_score: 0.4439\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182883D6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.67       254\n",
      "           1       0.23      0.28      0.25        97\n",
      "\n",
      "    accuracy                           0.54       351\n",
      "   macro avg       0.47      0.46      0.46       351\n",
      "weighted avg       0.57      0.54      0.56       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1674\n",
      "Epoch 1/300\n",
      "28/28 - 12s - loss: 0.6932 - acc: 0.4854 - f1_score: 0.3270 - val_loss: 0.7134 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4852 - f1_score: 0.3270 - val_loss: 0.7134 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6922 - acc: 0.4850 - f1_score: 0.3271 - val_loss: 0.7133 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6917 - acc: 0.4850 - f1_score: 0.3273 - val_loss: 0.7135 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6912 - acc: 0.4868 - f1_score: 0.3334 - val_loss: 0.7136 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6907 - acc: 0.4913 - f1_score: 0.3513 - val_loss: 0.7140 - val_acc: 0.0462 - val_f1_score: 0.0451\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6902 - acc: 0.5020 - f1_score: 0.3820 - val_loss: 0.7143 - val_acc: 0.0462 - val_f1_score: 0.0451\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5189 - f1_score: 0.4326 - val_loss: 0.7147 - val_acc: 0.0646 - val_f1_score: 0.0644\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5390 - f1_score: 0.4884 - val_loss: 0.7152 - val_acc: 0.0985 - val_f1_score: 0.0982\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5552 - f1_score: 0.5283 - val_loss: 0.7156 - val_acc: 0.1508 - val_f1_score: 0.1465\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6882 - acc: 0.5685 - f1_score: 0.5581 - val_loss: 0.7161 - val_acc: 0.2215 - val_f1_score: 0.2056\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5721 - f1_score: 0.5685 - val_loss: 0.7166 - val_acc: 0.2708 - val_f1_score: 0.2432\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5687 - f1_score: 0.5681 - val_loss: 0.7173 - val_acc: 0.3354 - val_f1_score: 0.2890\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6868 - acc: 0.5659 - f1_score: 0.5659 - val_loss: 0.7178 - val_acc: 0.4062 - val_f1_score: 0.3355\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5627 - f1_score: 0.5625 - val_loss: 0.7181 - val_acc: 0.4246 - val_f1_score: 0.3471\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5626 - f1_score: 0.5616 - val_loss: 0.7186 - val_acc: 0.4462 - val_f1_score: 0.3604\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5622 - f1_score: 0.5607 - val_loss: 0.7187 - val_acc: 0.4615 - val_f1_score: 0.3698\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6853 - acc: 0.5632 - f1_score: 0.5608 - val_loss: 0.7189 - val_acc: 0.4923 - val_f1_score: 0.3883\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5644 - f1_score: 0.5616 - val_loss: 0.7189 - val_acc: 0.4985 - val_f1_score: 0.3920\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6846 - acc: 0.5643 - f1_score: 0.5608 - val_loss: 0.7189 - val_acc: 0.5077 - val_f1_score: 0.3975\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6843 - acc: 0.5629 - f1_score: 0.5589 - val_loss: 0.7189 - val_acc: 0.5231 - val_f1_score: 0.4066\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5623 - f1_score: 0.5580 - val_loss: 0.7186 - val_acc: 0.5262 - val_f1_score: 0.4084\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6838 - acc: 0.5620 - f1_score: 0.5571 - val_loss: 0.7183 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6835 - acc: 0.5619 - f1_score: 0.5563 - val_loss: 0.7178 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5624 - f1_score: 0.5566 - val_loss: 0.7171 - val_acc: 0.5508 - val_f1_score: 0.4229\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6831 - acc: 0.5632 - f1_score: 0.5568 - val_loss: 0.7167 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5620 - f1_score: 0.5553 - val_loss: 0.7161 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5622 - f1_score: 0.5551 - val_loss: 0.7154 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 29/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5634 - f1_score: 0.5560 - val_loss: 0.7146 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 30/300\n",
      "28/28 - 3s - loss: 0.6823 - acc: 0.5644 - f1_score: 0.5567 - val_loss: 0.7142 - val_acc: 0.5662 - val_f1_score: 0.4319\n",
      "Epoch 31/300\n",
      "28/28 - 3s - loss: 0.6821 - acc: 0.5643 - f1_score: 0.5562 - val_loss: 0.7135 - val_acc: 0.5692 - val_f1_score: 0.4337\n",
      "Epoch 32/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5639 - f1_score: 0.5555 - val_loss: 0.7125 - val_acc: 0.5785 - val_f1_score: 0.4391\n",
      "Epoch 33/300\n",
      "28/28 - 3s - loss: 0.6818 - acc: 0.5630 - f1_score: 0.5545 - val_loss: 0.7116 - val_acc: 0.5785 - val_f1_score: 0.4391\n",
      "Epoch 34/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5624 - f1_score: 0.5535 - val_loss: 0.7101 - val_acc: 0.5785 - val_f1_score: 0.4391\n",
      "Epoch 35/300\n",
      "28/28 - 3s - loss: 0.6814 - acc: 0.5610 - f1_score: 0.5515 - val_loss: 0.7089 - val_acc: 0.5846 - val_f1_score: 0.4427\n",
      "Epoch 36/300\n",
      "28/28 - 3s - loss: 0.6813 - acc: 0.5610 - f1_score: 0.5513 - val_loss: 0.7082 - val_acc: 0.5877 - val_f1_score: 0.4445\n",
      "Epoch 37/300\n",
      "28/28 - 3s - loss: 0.6812 - acc: 0.5596 - f1_score: 0.5495 - val_loss: 0.7070 - val_acc: 0.5908 - val_f1_score: 0.4463\n",
      "Epoch 38/300\n",
      "28/28 - 3s - loss: 0.6810 - acc: 0.5606 - f1_score: 0.5502 - val_loss: 0.7062 - val_acc: 0.5908 - val_f1_score: 0.4463\n",
      "Epoch 39/300\n",
      "28/28 - 3s - loss: 0.6809 - acc: 0.5607 - f1_score: 0.5498 - val_loss: 0.7058 - val_acc: 0.5938 - val_f1_score: 0.4481\n",
      "Epoch 40/300\n",
      "28/28 - 3s - loss: 0.6808 - acc: 0.5610 - f1_score: 0.5499 - val_loss: 0.7056 - val_acc: 0.5969 - val_f1_score: 0.4499\n",
      "Epoch 41/300\n",
      "28/28 - 3s - loss: 0.6807 - acc: 0.5613 - f1_score: 0.5503 - val_loss: 0.7049 - val_acc: 0.6062 - val_f1_score: 0.4554\n",
      "Epoch 42/300\n",
      "28/28 - 3s - loss: 0.6805 - acc: 0.5616 - f1_score: 0.5501 - val_loss: 0.7044 - val_acc: 0.6062 - val_f1_score: 0.4554\n",
      "Epoch 43/300\n",
      "28/28 - 3s - loss: 0.6804 - acc: 0.5619 - f1_score: 0.5502 - val_loss: 0.7041 - val_acc: 0.6123 - val_f1_score: 0.4590\n",
      "Epoch 44/300\n",
      "28/28 - 3s - loss: 0.6803 - acc: 0.5620 - f1_score: 0.5502 - val_loss: 0.7040 - val_acc: 0.6185 - val_f1_score: 0.4627\n",
      "Epoch 45/300\n",
      "28/28 - 3s - loss: 0.6802 - acc: 0.5623 - f1_score: 0.5506 - val_loss: 0.7026 - val_acc: 0.6215 - val_f1_score: 0.4645\n",
      "Epoch 46/300\n",
      "28/28 - 3s - loss: 0.6801 - acc: 0.5630 - f1_score: 0.5509 - val_loss: 0.7020 - val_acc: 0.6215 - val_f1_score: 0.4645\n",
      "Epoch 47/300\n",
      "28/28 - 3s - loss: 0.6800 - acc: 0.5629 - f1_score: 0.5507 - val_loss: 0.7013 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 48/300\n",
      "28/28 - 3s - loss: 0.6799 - acc: 0.5637 - f1_score: 0.5514 - val_loss: 0.7005 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 49/300\n",
      "28/28 - 3s - loss: 0.6798 - acc: 0.5639 - f1_score: 0.5515 - val_loss: 0.6999 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 50/300\n",
      "28/28 - 3s - loss: 0.6797 - acc: 0.5644 - f1_score: 0.5519 - val_loss: 0.6992 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 51/300\n",
      "28/28 - 3s - loss: 0.6796 - acc: 0.5646 - f1_score: 0.5514 - val_loss: 0.6989 - val_acc: 0.6246 - val_f1_score: 0.4663\n",
      "Epoch 52/300\n",
      "28/28 - 3s - loss: 0.6795 - acc: 0.5650 - f1_score: 0.5521 - val_loss: 0.6982 - val_acc: 0.6277 - val_f1_score: 0.4682\n",
      "Epoch 53/300\n",
      "28/28 - 3s - loss: 0.6794 - acc: 0.5659 - f1_score: 0.5529 - val_loss: 0.6974 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 54/300\n",
      "28/28 - 3s - loss: 0.6793 - acc: 0.5657 - f1_score: 0.5527 - val_loss: 0.6959 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 55/300\n",
      "28/28 - 3s - loss: 0.6792 - acc: 0.5666 - f1_score: 0.5532 - val_loss: 0.6949 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 56/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5661 - f1_score: 0.5522 - val_loss: 0.6946 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 57/300\n",
      "28/28 - 3s - loss: 0.6791 - acc: 0.5670 - f1_score: 0.5533 - val_loss: 0.6937 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 58/300\n",
      "28/28 - 3s - loss: 0.6790 - acc: 0.5666 - f1_score: 0.5524 - val_loss: 0.6936 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 59/300\n",
      "28/28 - 3s - loss: 0.6789 - acc: 0.5671 - f1_score: 0.5530 - val_loss: 0.6938 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 60/300\n",
      "28/28 - 3s - loss: 0.6788 - acc: 0.5673 - f1_score: 0.5532 - val_loss: 0.6937 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 61/300\n",
      "28/28 - 3s - loss: 0.6787 - acc: 0.5678 - f1_score: 0.5537 - val_loss: 0.6930 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 62/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5684 - f1_score: 0.5541 - val_loss: 0.6931 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 63/300\n",
      "28/28 - 3s - loss: 0.6786 - acc: 0.5687 - f1_score: 0.5546 - val_loss: 0.6931 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 64/300\n",
      "28/28 - 3s - loss: 0.6785 - acc: 0.5694 - f1_score: 0.5550 - val_loss: 0.6934 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 65/300\n",
      "28/28 - 3s - loss: 0.6784 - acc: 0.5684 - f1_score: 0.5545 - val_loss: 0.6925 - val_acc: 0.6308 - val_f1_score: 0.4700\n",
      "Epoch 66/300\n",
      "28/28 - 3s - loss: 0.6783 - acc: 0.5701 - f1_score: 0.5557 - val_loss: 0.6923 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 67/300\n",
      "28/28 - 3s - loss: 0.6783 - acc: 0.5707 - f1_score: 0.5563 - val_loss: 0.6923 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 68/300\n",
      "28/28 - 3s - loss: 0.6782 - acc: 0.5708 - f1_score: 0.5563 - val_loss: 0.6921 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 69/300\n",
      "28/28 - 3s - loss: 0.6781 - acc: 0.5695 - f1_score: 0.5550 - val_loss: 0.6913 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 70/300\n",
      "28/28 - 3s - loss: 0.6780 - acc: 0.5701 - f1_score: 0.5553 - val_loss: 0.6911 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 71/300\n",
      "28/28 - 3s - loss: 0.6780 - acc: 0.5704 - f1_score: 0.5556 - val_loss: 0.6914 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 72/300\n",
      "28/28 - 3s - loss: 0.6779 - acc: 0.5710 - f1_score: 0.5564 - val_loss: 0.6899 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 73/300\n",
      "28/28 - 3s - loss: 0.6778 - acc: 0.5712 - f1_score: 0.5563 - val_loss: 0.6903 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 74/300\n",
      "28/28 - 3s - loss: 0.6777 - acc: 0.5712 - f1_score: 0.5560 - val_loss: 0.6907 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 75/300\n",
      "28/28 - 3s - loss: 0.6777 - acc: 0.5710 - f1_score: 0.5562 - val_loss: 0.6899 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 76/300\n",
      "28/28 - 3s - loss: 0.6776 - acc: 0.5718 - f1_score: 0.5567 - val_loss: 0.6901 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 77/300\n",
      "28/28 - 3s - loss: 0.6775 - acc: 0.5707 - f1_score: 0.5560 - val_loss: 0.6895 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 78/300\n",
      "28/28 - 3s - loss: 0.6775 - acc: 0.5708 - f1_score: 0.5558 - val_loss: 0.6896 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 79/300\n",
      "28/28 - 3s - loss: 0.6774 - acc: 0.5711 - f1_score: 0.5560 - val_loss: 0.6905 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 80/300\n",
      "28/28 - 3s - loss: 0.6773 - acc: 0.5715 - f1_score: 0.5567 - val_loss: 0.6903 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 81/300\n",
      "28/28 - 3s - loss: 0.6772 - acc: 0.5722 - f1_score: 0.5573 - val_loss: 0.6907 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 82/300\n",
      "28/28 - 3s - loss: 0.6772 - acc: 0.5729 - f1_score: 0.5581 - val_loss: 0.6902 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 83/300\n",
      "28/28 - 3s - loss: 0.6771 - acc: 0.5721 - f1_score: 0.5571 - val_loss: 0.6903 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 84/300\n",
      "28/28 - 3s - loss: 0.6770 - acc: 0.5721 - f1_score: 0.5571 - val_loss: 0.6906 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 85/300\n",
      "28/28 - 3s - loss: 0.6770 - acc: 0.5725 - f1_score: 0.5577 - val_loss: 0.6899 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Epoch 86/300\n",
      "28/28 - 3s - loss: 0.6769 - acc: 0.5721 - f1_score: 0.5574 - val_loss: 0.6894 - val_acc: 0.6338 - val_f1_score: 0.4718\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00086: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182A8D28160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18       132\n",
      "           1       0.62      1.00      0.76       193\n",
      "\n",
      "    accuracy                           0.63       325\n",
      "   macro avg       0.81      0.55      0.47       325\n",
      "weighted avg       0.77      0.63      0.53       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1688\n",
      "Epoch 1/300\n",
      "28/28 - 11s - loss: 0.6942 - acc: 0.4617 - f1_score: 0.3159 - val_loss: 0.6913 - val_acc: 0.5333 - val_f1_score: 0.3528\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6937 - acc: 0.4620 - f1_score: 0.3176 - val_loss: 0.6908 - val_acc: 0.5385 - val_f1_score: 0.3738\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4654 - f1_score: 0.3334 - val_loss: 0.6901 - val_acc: 0.6051 - val_f1_score: 0.5607\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4766 - f1_score: 0.3773 - val_loss: 0.6893 - val_acc: 0.6846 - val_f1_score: 0.6750\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6923 - acc: 0.5098 - f1_score: 0.4690 - val_loss: 0.6886 - val_acc: 0.6923 - val_f1_score: 0.6918\n",
      "Epoch 6/300\n",
      "28/28 - 4s - loss: 0.6918 - acc: 0.5471 - f1_score: 0.5434 - val_loss: 0.6879 - val_acc: 0.6923 - val_f1_score: 0.6920\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6913 - acc: 0.5532 - f1_score: 0.5518 - val_loss: 0.6871 - val_acc: 0.6846 - val_f1_score: 0.6829\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5568 - f1_score: 0.5474 - val_loss: 0.6862 - val_acc: 0.6923 - val_f1_score: 0.6897\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6905 - acc: 0.5614 - f1_score: 0.5454 - val_loss: 0.6854 - val_acc: 0.6923 - val_f1_score: 0.6883\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6900 - acc: 0.5607 - f1_score: 0.5409 - val_loss: 0.6846 - val_acc: 0.6949 - val_f1_score: 0.6887\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6896 - acc: 0.5624 - f1_score: 0.5392 - val_loss: 0.6835 - val_acc: 0.6923 - val_f1_score: 0.6858\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5627 - f1_score: 0.5383 - val_loss: 0.6828 - val_acc: 0.6821 - val_f1_score: 0.6743\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6888 - acc: 0.5621 - f1_score: 0.5356 - val_loss: 0.6819 - val_acc: 0.6846 - val_f1_score: 0.6767\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6885 - acc: 0.5634 - f1_score: 0.5361 - val_loss: 0.6809 - val_acc: 0.6821 - val_f1_score: 0.6738\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5642 - f1_score: 0.5363 - val_loss: 0.6798 - val_acc: 0.6846 - val_f1_score: 0.6772\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6877 - acc: 0.5634 - f1_score: 0.5354 - val_loss: 0.6789 - val_acc: 0.6897 - val_f1_score: 0.6820\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6873 - acc: 0.5641 - f1_score: 0.5348 - val_loss: 0.6777 - val_acc: 0.6923 - val_f1_score: 0.6848\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6870 - acc: 0.5648 - f1_score: 0.5363 - val_loss: 0.6766 - val_acc: 0.6949 - val_f1_score: 0.6877\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5637 - f1_score: 0.5345 - val_loss: 0.6754 - val_acc: 0.6949 - val_f1_score: 0.6877\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6863 - acc: 0.5638 - f1_score: 0.5356 - val_loss: 0.6747 - val_acc: 0.6949 - val_f1_score: 0.6877\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6860 - acc: 0.5651 - f1_score: 0.5361 - val_loss: 0.6739 - val_acc: 0.6923 - val_f1_score: 0.6848\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6857 - acc: 0.5651 - f1_score: 0.5353 - val_loss: 0.6730 - val_acc: 0.6923 - val_f1_score: 0.6853\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5652 - f1_score: 0.5360 - val_loss: 0.6722 - val_acc: 0.6846 - val_f1_score: 0.6772\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6851 - acc: 0.5650 - f1_score: 0.5350 - val_loss: 0.6712 - val_acc: 0.6846 - val_f1_score: 0.6772\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5658 - f1_score: 0.5361 - val_loss: 0.6706 - val_acc: 0.6846 - val_f1_score: 0.6772\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5651 - f1_score: 0.5347 - val_loss: 0.6699 - val_acc: 0.6846 - val_f1_score: 0.6772\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000185697F4820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68       170\n",
      "           1       0.77      0.64      0.70       220\n",
      "\n",
      "    accuracy                           0.69       390\n",
      "   macro avg       0.70      0.70      0.69       390\n",
      "weighted avg       0.71      0.69      0.69       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1717\n",
      "Epoch 1/300\n",
      "28/28 - 21s - loss: 0.6940 - acc: 0.4664 - f1_score: 0.3183 - val_loss: 0.6953 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6933 - acc: 0.4662 - f1_score: 0.3198 - val_loss: 0.6956 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6927 - acc: 0.4711 - f1_score: 0.3361 - val_loss: 0.6959 - val_acc: 0.4444 - val_f1_score: 0.3141\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4869 - f1_score: 0.3952 - val_loss: 0.6963 - val_acc: 0.4957 - val_f1_score: 0.4127\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5169 - f1_score: 0.4811 - val_loss: 0.6967 - val_acc: 0.5769 - val_f1_score: 0.5482\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5487 - f1_score: 0.5442 - val_loss: 0.6972 - val_acc: 0.5684 - val_f1_score: 0.5661\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5544 - f1_score: 0.5540 - val_loss: 0.6979 - val_acc: 0.5556 - val_f1_score: 0.5555\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6897 - acc: 0.5560 - f1_score: 0.5514 - val_loss: 0.6985 - val_acc: 0.5085 - val_f1_score: 0.5053\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5637 - f1_score: 0.5540 - val_loss: 0.6992 - val_acc: 0.4786 - val_f1_score: 0.4721\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6886 - acc: 0.5644 - f1_score: 0.5530 - val_loss: 0.6999 - val_acc: 0.4615 - val_f1_score: 0.4485\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6880 - acc: 0.5628 - f1_score: 0.5501 - val_loss: 0.7007 - val_acc: 0.4487 - val_f1_score: 0.4330\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6874 - acc: 0.5634 - f1_score: 0.5491 - val_loss: 0.7015 - val_acc: 0.4487 - val_f1_score: 0.4313\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6869 - acc: 0.5652 - f1_score: 0.5498 - val_loss: 0.7024 - val_acc: 0.4487 - val_f1_score: 0.4295\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6864 - acc: 0.5641 - f1_score: 0.5486 - val_loss: 0.7032 - val_acc: 0.4530 - val_f1_score: 0.4329\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6859 - acc: 0.5645 - f1_score: 0.5482 - val_loss: 0.7041 - val_acc: 0.4615 - val_f1_score: 0.4418\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6854 - acc: 0.5638 - f1_score: 0.5471 - val_loss: 0.7049 - val_acc: 0.4573 - val_f1_score: 0.4344\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6849 - acc: 0.5641 - f1_score: 0.5472 - val_loss: 0.7057 - val_acc: 0.4530 - val_f1_score: 0.4290\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6845 - acc: 0.5645 - f1_score: 0.5469 - val_loss: 0.7065 - val_acc: 0.4530 - val_f1_score: 0.4290\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5640 - f1_score: 0.5461 - val_loss: 0.7073 - val_acc: 0.4487 - val_f1_score: 0.4234\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6836 - acc: 0.5647 - f1_score: 0.5466 - val_loss: 0.7080 - val_acc: 0.4573 - val_f1_score: 0.4302\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5655 - f1_score: 0.5470 - val_loss: 0.7088 - val_acc: 0.4530 - val_f1_score: 0.4246\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6829 - acc: 0.5652 - f1_score: 0.5466 - val_loss: 0.7094 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6826 - acc: 0.5663 - f1_score: 0.5469 - val_loss: 0.7101 - val_acc: 0.4573 - val_f1_score: 0.4230\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5675 - f1_score: 0.5479 - val_loss: 0.7107 - val_acc: 0.4573 - val_f1_score: 0.4230\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5680 - f1_score: 0.5481 - val_loss: 0.7112 - val_acc: 0.4573 - val_f1_score: 0.4230\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6816 - acc: 0.5680 - f1_score: 0.5479 - val_loss: 0.7117 - val_acc: 0.4573 - val_f1_score: 0.4230\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.51      0.60       147\n",
      "           1       0.45      0.67      0.53        87\n",
      "\n",
      "    accuracy                           0.57       234\n",
      "   macro avg       0.58      0.59      0.57       234\n",
      "weighted avg       0.62      0.57      0.57       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1818\n",
      "Epoch 1/300\n",
      "27/27 - 10s - loss: 0.6938 - acc: 0.4725 - f1_score: 0.3209 - val_loss: 0.6975 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 4s - loss: 0.6934 - acc: 0.4723 - f1_score: 0.3212 - val_loss: 0.6965 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6929 - acc: 0.4727 - f1_score: 0.3231 - val_loss: 0.6955 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.4770 - f1_score: 0.3390 - val_loss: 0.6945 - val_acc: 0.3632 - val_f1_score: 0.2691\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6921 - acc: 0.4886 - f1_score: 0.3827 - val_loss: 0.6935 - val_acc: 0.3718 - val_f1_score: 0.2863\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.5091 - f1_score: 0.4436 - val_loss: 0.6926 - val_acc: 0.3953 - val_f1_score: 0.3456\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5337 - f1_score: 0.5044 - val_loss: 0.6916 - val_acc: 0.4744 - val_f1_score: 0.4630\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6908 - acc: 0.5541 - f1_score: 0.5481 - val_loss: 0.6906 - val_acc: 0.5235 - val_f1_score: 0.5234\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6904 - acc: 0.5535 - f1_score: 0.5534 - val_loss: 0.6897 - val_acc: 0.5449 - val_f1_score: 0.5375\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5551 - f1_score: 0.5542 - val_loss: 0.6888 - val_acc: 0.5791 - val_f1_score: 0.5661\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6896 - acc: 0.5579 - f1_score: 0.5545 - val_loss: 0.6879 - val_acc: 0.5983 - val_f1_score: 0.5799\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5590 - f1_score: 0.5533 - val_loss: 0.6870 - val_acc: 0.6261 - val_f1_score: 0.6020\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6888 - acc: 0.5592 - f1_score: 0.5517 - val_loss: 0.6861 - val_acc: 0.6325 - val_f1_score: 0.6057\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6884 - acc: 0.5583 - f1_score: 0.5491 - val_loss: 0.6852 - val_acc: 0.6325 - val_f1_score: 0.6057\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6880 - acc: 0.5580 - f1_score: 0.5477 - val_loss: 0.6844 - val_acc: 0.6368 - val_f1_score: 0.6084\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6877 - acc: 0.5561 - f1_score: 0.5449 - val_loss: 0.6836 - val_acc: 0.6368 - val_f1_score: 0.6074\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6873 - acc: 0.5561 - f1_score: 0.5445 - val_loss: 0.6828 - val_acc: 0.6325 - val_f1_score: 0.6017\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6870 - acc: 0.5548 - f1_score: 0.5423 - val_loss: 0.6820 - val_acc: 0.6325 - val_f1_score: 0.6017\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5558 - f1_score: 0.5431 - val_loss: 0.6813 - val_acc: 0.6346 - val_f1_score: 0.6036\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6863 - acc: 0.5553 - f1_score: 0.5419 - val_loss: 0.6805 - val_acc: 0.6325 - val_f1_score: 0.5997\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6860 - acc: 0.5564 - f1_score: 0.5426 - val_loss: 0.6798 - val_acc: 0.6346 - val_f1_score: 0.6015\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5557 - f1_score: 0.5415 - val_loss: 0.6790 - val_acc: 0.6346 - val_f1_score: 0.6015\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6855 - acc: 0.5554 - f1_score: 0.5408 - val_loss: 0.6783 - val_acc: 0.6410 - val_f1_score: 0.6068\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6852 - acc: 0.5556 - f1_score: 0.5408 - val_loss: 0.6775 - val_acc: 0.6410 - val_f1_score: 0.6057\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6849 - acc: 0.5557 - f1_score: 0.5407 - val_loss: 0.6768 - val_acc: 0.6432 - val_f1_score: 0.6075\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5558 - f1_score: 0.5405 - val_loss: 0.6761 - val_acc: 0.6453 - val_f1_score: 0.6093\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6845 - acc: 0.5567 - f1_score: 0.5414 - val_loss: 0.6754 - val_acc: 0.6474 - val_f1_score: 0.6111\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6843 - acc: 0.5561 - f1_score: 0.5400 - val_loss: 0.6748 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6840 - acc: 0.5564 - f1_score: 0.5399 - val_loss: 0.6741 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6838 - acc: 0.5574 - f1_score: 0.5403 - val_loss: 0.6735 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5580 - f1_score: 0.5403 - val_loss: 0.6729 - val_acc: 0.6496 - val_f1_score: 0.6129\n",
      "Epoch 32/300\n",
      "27/27 - 3s - loss: 0.6835 - acc: 0.5574 - f1_score: 0.5399 - val_loss: 0.6723 - val_acc: 0.6538 - val_f1_score: 0.6165\n",
      "Epoch 33/300\n",
      "27/27 - 3s - loss: 0.6833 - acc: 0.5586 - f1_score: 0.5407 - val_loss: 0.6717 - val_acc: 0.6581 - val_f1_score: 0.6201\n",
      "Epoch 34/300\n",
      "27/27 - 3s - loss: 0.6831 - acc: 0.5592 - f1_score: 0.5407 - val_loss: 0.6711 - val_acc: 0.6581 - val_f1_score: 0.6201\n",
      "Epoch 35/300\n",
      "27/27 - 3s - loss: 0.6829 - acc: 0.5595 - f1_score: 0.5407 - val_loss: 0.6705 - val_acc: 0.6624 - val_f1_score: 0.6237\n",
      "Epoch 36/300\n",
      "27/27 - 3s - loss: 0.6828 - acc: 0.5593 - f1_score: 0.5406 - val_loss: 0.6700 - val_acc: 0.6645 - val_f1_score: 0.6255\n",
      "Epoch 37/300\n",
      "27/27 - 3s - loss: 0.6826 - acc: 0.5598 - f1_score: 0.5405 - val_loss: 0.6694 - val_acc: 0.6645 - val_f1_score: 0.6255\n",
      "Epoch 38/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5599 - f1_score: 0.5407 - val_loss: 0.6689 - val_acc: 0.6667 - val_f1_score: 0.6274\n",
      "Epoch 39/300\n",
      "27/27 - 3s - loss: 0.6823 - acc: 0.5602 - f1_score: 0.5406 - val_loss: 0.6683 - val_acc: 0.6688 - val_f1_score: 0.6292\n",
      "Epoch 40/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5611 - f1_score: 0.5412 - val_loss: 0.6678 - val_acc: 0.6688 - val_f1_score: 0.6292\n",
      "Epoch 41/300\n",
      "27/27 - 3s - loss: 0.6820 - acc: 0.5608 - f1_score: 0.5403 - val_loss: 0.6673 - val_acc: 0.6688 - val_f1_score: 0.6292\n",
      "Epoch 42/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5609 - f1_score: 0.5404 - val_loss: 0.6669 - val_acc: 0.6688 - val_f1_score: 0.6292\n",
      "Epoch 43/300\n",
      "27/27 - 3s - loss: 0.6818 - acc: 0.5600 - f1_score: 0.5393 - val_loss: 0.6664 - val_acc: 0.6667 - val_f1_score: 0.6262\n",
      "Epoch 44/300\n",
      "27/27 - 3s - loss: 0.6816 - acc: 0.5605 - f1_score: 0.5392 - val_loss: 0.6659 - val_acc: 0.6688 - val_f1_score: 0.6280\n",
      "Epoch 45/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5602 - f1_score: 0.5392 - val_loss: 0.6655 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 46/300\n",
      "27/27 - 3s - loss: 0.6814 - acc: 0.5605 - f1_score: 0.5392 - val_loss: 0.6650 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 47/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5616 - f1_score: 0.5398 - val_loss: 0.6646 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 48/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5614 - f1_score: 0.5395 - val_loss: 0.6642 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 49/300\n",
      "27/27 - 3s - loss: 0.6810 - acc: 0.5625 - f1_score: 0.5402 - val_loss: 0.6637 - val_acc: 0.6688 - val_f1_score: 0.6268\n",
      "Epoch 50/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5621 - f1_score: 0.5391 - val_loss: 0.6634 - val_acc: 0.6667 - val_f1_score: 0.6238\n",
      "Epoch 51/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.5625 - f1_score: 0.5398 - val_loss: 0.6630 - val_acc: 0.6709 - val_f1_score: 0.6274\n",
      "Epoch 52/300\n",
      "27/27 - 3s - loss: 0.6807 - acc: 0.5638 - f1_score: 0.5401 - val_loss: 0.6626 - val_acc: 0.6688 - val_f1_score: 0.6256\n",
      "Epoch 53/300\n",
      "27/27 - 3s - loss: 0.6806 - acc: 0.5632 - f1_score: 0.5402 - val_loss: 0.6623 - val_acc: 0.6688 - val_f1_score: 0.6243\n",
      "Epoch 54/300\n",
      "27/27 - 3s - loss: 0.6805 - acc: 0.5637 - f1_score: 0.5396 - val_loss: 0.6619 - val_acc: 0.6688 - val_f1_score: 0.6243\n",
      "Epoch 55/300\n",
      "27/27 - 3s - loss: 0.6804 - acc: 0.5625 - f1_score: 0.5392 - val_loss: 0.6616 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 56/300\n",
      "27/27 - 3s - loss: 0.6803 - acc: 0.5637 - f1_score: 0.5400 - val_loss: 0.6612 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 57/300\n",
      "27/27 - 3s - loss: 0.6802 - acc: 0.5638 - f1_score: 0.5398 - val_loss: 0.6609 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 58/300\n",
      "27/27 - 3s - loss: 0.6801 - acc: 0.5642 - f1_score: 0.5398 - val_loss: 0.6606 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Epoch 59/300\n",
      "27/27 - 3s - loss: 0.6800 - acc: 0.5640 - f1_score: 0.5396 - val_loss: 0.6603 - val_acc: 0.6731 - val_f1_score: 0.6279\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00059: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000182F44775E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.55      0.51       146\n",
      "           1       0.78      0.72      0.75       322\n",
      "\n",
      "    accuracy                           0.67       468\n",
      "   macro avg       0.63      0.64      0.63       468\n",
      "weighted avg       0.68      0.67      0.67       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1892\n",
      "Epoch 1/300\n",
      "27/27 - 10s - loss: 0.6952 - acc: 0.4311 - f1_score: 0.3017 - val_loss: 0.6785 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6944 - acc: 0.4320 - f1_score: 0.3100 - val_loss: 0.6825 - val_acc: 0.9679 - val_f1_score: 0.4919\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6935 - acc: 0.4539 - f1_score: 0.3780 - val_loss: 0.6864 - val_acc: 0.8761 - val_f1_score: 0.5136\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.5209 - f1_score: 0.5141 - val_loss: 0.6900 - val_acc: 0.5962 - val_f1_score: 0.4097\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6920 - acc: 0.5647 - f1_score: 0.5523 - val_loss: 0.6936 - val_acc: 0.3654 - val_f1_score: 0.2980\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6913 - acc: 0.5851 - f1_score: 0.5448 - val_loss: 0.6969 - val_acc: 0.2799 - val_f1_score: 0.2417\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6905 - acc: 0.5900 - f1_score: 0.5358 - val_loss: 0.7002 - val_acc: 0.2308 - val_f1_score: 0.2064\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6899 - acc: 0.5947 - f1_score: 0.5283 - val_loss: 0.7034 - val_acc: 0.2030 - val_f1_score: 0.1853\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6892 - acc: 0.5961 - f1_score: 0.5227 - val_loss: 0.7064 - val_acc: 0.1902 - val_f1_score: 0.1752\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6885 - acc: 0.5983 - f1_score: 0.5212 - val_loss: 0.7093 - val_acc: 0.1731 - val_f1_score: 0.1615\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6879 - acc: 0.5987 - f1_score: 0.5179 - val_loss: 0.7120 - val_acc: 0.1667 - val_f1_score: 0.1562\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5984 - f1_score: 0.5175 - val_loss: 0.7147 - val_acc: 0.1645 - val_f1_score: 0.1545\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6866 - acc: 0.5992 - f1_score: 0.5172 - val_loss: 0.7172 - val_acc: 0.1624 - val_f1_score: 0.1527\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6859 - acc: 0.6002 - f1_score: 0.5179 - val_loss: 0.7198 - val_acc: 0.1645 - val_f1_score: 0.1545\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5997 - f1_score: 0.5178 - val_loss: 0.7222 - val_acc: 0.1667 - val_f1_score: 0.1562\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6847 - acc: 0.5994 - f1_score: 0.5177 - val_loss: 0.7245 - val_acc: 0.1667 - val_f1_score: 0.1562\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5992 - f1_score: 0.5200 - val_loss: 0.7269 - val_acc: 0.1688 - val_f1_score: 0.1580\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6836 - acc: 0.5996 - f1_score: 0.5190 - val_loss: 0.7291 - val_acc: 0.1688 - val_f1_score: 0.1580\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5994 - f1_score: 0.5216 - val_loss: 0.7313 - val_acc: 0.1731 - val_f1_score: 0.1615\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6824 - acc: 0.6000 - f1_score: 0.5225 - val_loss: 0.7334 - val_acc: 0.1752 - val_f1_score: 0.1632\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.6010 - f1_score: 0.5262 - val_loss: 0.7355 - val_acc: 0.1774 - val_f1_score: 0.1650\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.6022 - f1_score: 0.5293 - val_loss: 0.7376 - val_acc: 0.1774 - val_f1_score: 0.1650\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6808 - acc: 0.6013 - f1_score: 0.5294 - val_loss: 0.7398 - val_acc: 0.1774 - val_f1_score: 0.1650\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       417\n",
      "           1       0.23      0.06      0.09        51\n",
      "\n",
      "    accuracy                           0.88       468\n",
      "   macro avg       0.56      0.52      0.51       468\n",
      "weighted avg       0.82      0.88      0.84       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1929\n",
      "Epoch 1/300\n",
      "27/27 - 20s - loss: 0.6933 - acc: 0.4820 - f1_score: 0.3252 - val_loss: 0.7047 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.4820 - f1_score: 0.3257 - val_loss: 0.7048 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.4818 - f1_score: 0.3259 - val_loss: 0.7050 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.4833 - f1_score: 0.3292 - val_loss: 0.7054 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.4865 - f1_score: 0.3400 - val_loss: 0.7059 - val_acc: 0.2244 - val_f1_score: 0.1849\n",
      "Epoch 6/300\n",
      "27/27 - 3s - loss: 0.6906 - acc: 0.4962 - f1_score: 0.3728 - val_loss: 0.7066 - val_acc: 0.2308 - val_f1_score: 0.1940\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6900 - acc: 0.5037 - f1_score: 0.4030 - val_loss: 0.7072 - val_acc: 0.2607 - val_f1_score: 0.2373\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6894 - acc: 0.5269 - f1_score: 0.4613 - val_loss: 0.7080 - val_acc: 0.3248 - val_f1_score: 0.3193\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6889 - acc: 0.5419 - f1_score: 0.5087 - val_loss: 0.7088 - val_acc: 0.3889 - val_f1_score: 0.3864\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6883 - acc: 0.5573 - f1_score: 0.5439 - val_loss: 0.7097 - val_acc: 0.4487 - val_f1_score: 0.4234\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6878 - acc: 0.5625 - f1_score: 0.5586 - val_loss: 0.7106 - val_acc: 0.4829 - val_f1_score: 0.4382\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6872 - acc: 0.5596 - f1_score: 0.5594 - val_loss: 0.7115 - val_acc: 0.4957 - val_f1_score: 0.4345\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6867 - acc: 0.5574 - f1_score: 0.5573 - val_loss: 0.7125 - val_acc: 0.4979 - val_f1_score: 0.4285\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6862 - acc: 0.5577 - f1_score: 0.5569 - val_loss: 0.7135 - val_acc: 0.5064 - val_f1_score: 0.4344\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6857 - acc: 0.5614 - f1_score: 0.5595 - val_loss: 0.7145 - val_acc: 0.5043 - val_f1_score: 0.4289\n",
      "Epoch 16/300\n",
      "27/27 - 3s - loss: 0.6853 - acc: 0.5637 - f1_score: 0.5610 - val_loss: 0.7155 - val_acc: 0.5085 - val_f1_score: 0.4318\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6848 - acc: 0.5637 - f1_score: 0.5605 - val_loss: 0.7164 - val_acc: 0.5043 - val_f1_score: 0.4248\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6844 - acc: 0.5640 - f1_score: 0.5598 - val_loss: 0.7173 - val_acc: 0.5043 - val_f1_score: 0.4226\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6841 - acc: 0.5648 - f1_score: 0.5600 - val_loss: 0.7181 - val_acc: 0.5064 - val_f1_score: 0.4240\n",
      "Epoch 20/300\n",
      "27/27 - 3s - loss: 0.6837 - acc: 0.5635 - f1_score: 0.5579 - val_loss: 0.7188 - val_acc: 0.5085 - val_f1_score: 0.4254\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6833 - acc: 0.5642 - f1_score: 0.5583 - val_loss: 0.7195 - val_acc: 0.5107 - val_f1_score: 0.4268\n",
      "Epoch 22/300\n",
      "27/27 - 3s - loss: 0.6830 - acc: 0.5654 - f1_score: 0.5588 - val_loss: 0.7201 - val_acc: 0.5150 - val_f1_score: 0.4296\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6827 - acc: 0.5653 - f1_score: 0.5582 - val_loss: 0.7204 - val_acc: 0.5171 - val_f1_score: 0.4310\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6825 - acc: 0.5654 - f1_score: 0.5580 - val_loss: 0.7206 - val_acc: 0.5171 - val_f1_score: 0.4310\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6822 - acc: 0.5638 - f1_score: 0.5557 - val_loss: 0.7208 - val_acc: 0.5171 - val_f1_score: 0.4288\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6819 - acc: 0.5644 - f1_score: 0.5557 - val_loss: 0.7209 - val_acc: 0.5171 - val_f1_score: 0.4265\n",
      "Epoch 27/300\n",
      "27/27 - 3s - loss: 0.6817 - acc: 0.5634 - f1_score: 0.5546 - val_loss: 0.7207 - val_acc: 0.5192 - val_f1_score: 0.4279\n",
      "Epoch 28/300\n",
      "27/27 - 3s - loss: 0.6815 - acc: 0.5640 - f1_score: 0.5546 - val_loss: 0.7207 - val_acc: 0.5235 - val_f1_score: 0.4306\n",
      "Epoch 29/300\n",
      "27/27 - 3s - loss: 0.6813 - acc: 0.5632 - f1_score: 0.5534 - val_loss: 0.7205 - val_acc: 0.5235 - val_f1_score: 0.4306\n",
      "Epoch 30/300\n",
      "27/27 - 3s - loss: 0.6811 - acc: 0.5625 - f1_score: 0.5522 - val_loss: 0.7206 - val_acc: 0.5235 - val_f1_score: 0.4306\n",
      "Epoch 31/300\n",
      "27/27 - 3s - loss: 0.6809 - acc: 0.5624 - f1_score: 0.5515 - val_loss: 0.7206 - val_acc: 0.5235 - val_f1_score: 0.4259\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.20      0.28       232\n",
      "           1       0.49      0.76      0.60       236\n",
      "\n",
      "    accuracy                           0.48       468\n",
      "   macro avg       0.47      0.48      0.44       468\n",
      "weighted avg       0.47      0.48      0.44       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "Testing on 1933\n",
      "Epoch 1/300\n",
      "28/28 - 13s - loss: 0.6938 - acc: 0.4699 - f1_score: 0.3199 - val_loss: 0.6994 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 3s - loss: 0.6932 - acc: 0.4697 - f1_score: 0.3214 - val_loss: 0.6988 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 3/300\n",
      "28/28 - 3s - loss: 0.6926 - acc: 0.4761 - f1_score: 0.3437 - val_loss: 0.6983 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 4/300\n",
      "28/28 - 3s - loss: 0.6921 - acc: 0.4922 - f1_score: 0.4021 - val_loss: 0.6979 - val_acc: 0.3590 - val_f1_score: 0.2998\n",
      "Epoch 5/300\n",
      "28/28 - 3s - loss: 0.6915 - acc: 0.5212 - f1_score: 0.4820 - val_loss: 0.6975 - val_acc: 0.4060 - val_f1_score: 0.3852\n",
      "Epoch 6/300\n",
      "28/28 - 3s - loss: 0.6909 - acc: 0.5556 - f1_score: 0.5510 - val_loss: 0.6973 - val_acc: 0.4145 - val_f1_score: 0.4132\n",
      "Epoch 7/300\n",
      "28/28 - 3s - loss: 0.6903 - acc: 0.5595 - f1_score: 0.5592 - val_loss: 0.6971 - val_acc: 0.4231 - val_f1_score: 0.4192\n",
      "Epoch 8/300\n",
      "28/28 - 3s - loss: 0.6898 - acc: 0.5610 - f1_score: 0.5570 - val_loss: 0.6970 - val_acc: 0.4573 - val_f1_score: 0.4383\n",
      "Epoch 9/300\n",
      "28/28 - 3s - loss: 0.6892 - acc: 0.5661 - f1_score: 0.5574 - val_loss: 0.6970 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 10/300\n",
      "28/28 - 3s - loss: 0.6887 - acc: 0.5652 - f1_score: 0.5543 - val_loss: 0.6971 - val_acc: 0.4487 - val_f1_score: 0.4212\n",
      "Epoch 11/300\n",
      "28/28 - 3s - loss: 0.6881 - acc: 0.5656 - f1_score: 0.5537 - val_loss: 0.6972 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 12/300\n",
      "28/28 - 3s - loss: 0.6876 - acc: 0.5627 - f1_score: 0.5497 - val_loss: 0.6974 - val_acc: 0.4487 - val_f1_score: 0.4189\n",
      "Epoch 13/300\n",
      "28/28 - 3s - loss: 0.6871 - acc: 0.5633 - f1_score: 0.5490 - val_loss: 0.6976 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 14/300\n",
      "28/28 - 3s - loss: 0.6866 - acc: 0.5637 - f1_score: 0.5490 - val_loss: 0.6979 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 15/300\n",
      "28/28 - 3s - loss: 0.6861 - acc: 0.5628 - f1_score: 0.5474 - val_loss: 0.6982 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 16/300\n",
      "28/28 - 3s - loss: 0.6856 - acc: 0.5641 - f1_score: 0.5486 - val_loss: 0.6984 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 17/300\n",
      "28/28 - 3s - loss: 0.6852 - acc: 0.5635 - f1_score: 0.5476 - val_loss: 0.6987 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 18/300\n",
      "28/28 - 3s - loss: 0.6848 - acc: 0.5641 - f1_score: 0.5476 - val_loss: 0.6990 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 19/300\n",
      "28/28 - 3s - loss: 0.6844 - acc: 0.5648 - f1_score: 0.5480 - val_loss: 0.6992 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 20/300\n",
      "28/28 - 3s - loss: 0.6840 - acc: 0.5644 - f1_score: 0.5476 - val_loss: 0.6994 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 21/300\n",
      "28/28 - 3s - loss: 0.6837 - acc: 0.5651 - f1_score: 0.5477 - val_loss: 0.6996 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 22/300\n",
      "28/28 - 3s - loss: 0.6833 - acc: 0.5655 - f1_score: 0.5481 - val_loss: 0.6996 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 23/300\n",
      "28/28 - 3s - loss: 0.6830 - acc: 0.5659 - f1_score: 0.5478 - val_loss: 0.6998 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 24/300\n",
      "28/28 - 3s - loss: 0.6827 - acc: 0.5673 - f1_score: 0.5488 - val_loss: 0.6999 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 25/300\n",
      "28/28 - 3s - loss: 0.6824 - acc: 0.5668 - f1_score: 0.5480 - val_loss: 0.6999 - val_acc: 0.4530 - val_f1_score: 0.4222\n",
      "Epoch 26/300\n",
      "28/28 - 3s - loss: 0.6822 - acc: 0.5676 - f1_score: 0.5486 - val_loss: 0.6999 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 27/300\n",
      "28/28 - 3s - loss: 0.6819 - acc: 0.5686 - f1_score: 0.5491 - val_loss: 0.6998 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Epoch 28/300\n",
      "28/28 - 3s - loss: 0.6817 - acc: 0.5690 - f1_score: 0.5496 - val_loss: 0.6997 - val_acc: 0.4573 - val_f1_score: 0.4255\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.28      0.34       113\n",
      "           1       0.48      0.62      0.54       121\n",
      "\n",
      "    accuracy                           0.46       234\n",
      "   macro avg       0.45      0.45      0.44       234\n",
      "weighted avg       0.45      0.46      0.44       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220524-141100\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Type FF, Stage FF\n",
      "Average Accuracy:  0.6448744380353575\n",
      "F1 score for Baseline:  0.5318069436944147\n",
      "F1 score for Stress:  0.5759303030889197\n",
      "Macro F1:  0.5538686233916673\n",
      "Weighted F1:  0.6264734188646559\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['FF']\n",
    "attx_st = ['FF']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in sub_dict_ecg.keys():\n",
    "\n",
    "            if i in ['1765']:\n",
    "                continue\n",
    "\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=20, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_model(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))    \n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG and EDA VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Training for Modality ecg\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-132454\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_weights\n",
      "(None, 2560, 1)\n",
      "Testing on 1105\n",
      "Epoch 1/300\n",
      "27/27 - 30s - loss: 0.6947 - acc: 0.4689 - f1_score: 0.3192 - val_loss: 0.6952 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 2/300\n",
      "27/27 - 4s - loss: 0.6943 - acc: 0.4686 - f1_score: 0.3193 - val_loss: 0.6947 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 3/300\n",
      "27/27 - 4s - loss: 0.6940 - acc: 0.4663 - f1_score: 0.3281 - val_loss: 0.6942 - val_acc: 0.4167 - val_f1_score: 0.3001\n",
      "Epoch 4/300\n",
      "27/27 - 3s - loss: 0.6937 - acc: 0.4607 - f1_score: 0.3691 - val_loss: 0.6937 - val_acc: 0.4338 - val_f1_score: 0.3660\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6935 - acc: 0.4769 - f1_score: 0.4408 - val_loss: 0.6933 - val_acc: 0.4594 - val_f1_score: 0.4579\n",
      "Epoch 6/300\n",
      "27/27 - 4s - loss: 0.6932 - acc: 0.5012 - f1_score: 0.4996 - val_loss: 0.6928 - val_acc: 0.5385 - val_f1_score: 0.4931\n",
      "Epoch 7/300\n",
      "27/27 - 3s - loss: 0.6930 - acc: 0.5157 - f1_score: 0.5040 - val_loss: 0.6925 - val_acc: 0.5940 - val_f1_score: 0.4587\n",
      "Epoch 8/300\n",
      "27/27 - 3s - loss: 0.6928 - acc: 0.5295 - f1_score: 0.4576 - val_loss: 0.6921 - val_acc: 0.5876 - val_f1_score: 0.3930\n",
      "Epoch 9/300\n",
      "27/27 - 3s - loss: 0.6926 - acc: 0.5286 - f1_score: 0.3995 - val_loss: 0.6918 - val_acc: 0.5919 - val_f1_score: 0.3905\n",
      "Epoch 10/300\n",
      "27/27 - 3s - loss: 0.6925 - acc: 0.5317 - f1_score: 0.3686 - val_loss: 0.6915 - val_acc: 0.5919 - val_f1_score: 0.3905\n",
      "Epoch 11/300\n",
      "27/27 - 3s - loss: 0.6923 - acc: 0.5319 - f1_score: 0.3555 - val_loss: 0.6912 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 12/300\n",
      "27/27 - 3s - loss: 0.6922 - acc: 0.5322 - f1_score: 0.3515 - val_loss: 0.6909 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 13/300\n",
      "27/27 - 3s - loss: 0.6921 - acc: 0.5319 - f1_score: 0.3506 - val_loss: 0.6906 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 14/300\n",
      "27/27 - 3s - loss: 0.6920 - acc: 0.5318 - f1_score: 0.3497 - val_loss: 0.6904 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 15/300\n",
      "27/27 - 3s - loss: 0.6918 - acc: 0.5317 - f1_score: 0.3491 - val_loss: 0.6901 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 16/300\n",
      "27/27 - 4s - loss: 0.6918 - acc: 0.5314 - f1_score: 0.3484 - val_loss: 0.6899 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 17/300\n",
      "27/27 - 3s - loss: 0.6917 - acc: 0.5312 - f1_score: 0.3480 - val_loss: 0.6897 - val_acc: 0.5855 - val_f1_score: 0.3740\n",
      "Epoch 18/300\n",
      "27/27 - 3s - loss: 0.6916 - acc: 0.5311 - f1_score: 0.3474 - val_loss: 0.6895 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 19/300\n",
      "27/27 - 3s - loss: 0.6915 - acc: 0.5311 - f1_score: 0.3474 - val_loss: 0.6893 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 20/300\n",
      "27/27 - 4s - loss: 0.6914 - acc: 0.5311 - f1_score: 0.3474 - val_loss: 0.6892 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 21/300\n",
      "27/27 - 3s - loss: 0.6914 - acc: 0.5309 - f1_score: 0.3471 - val_loss: 0.6890 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 22/300\n",
      "27/27 - 4s - loss: 0.6913 - acc: 0.5309 - f1_score: 0.3471 - val_loss: 0.6888 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 23/300\n",
      "27/27 - 3s - loss: 0.6913 - acc: 0.5309 - f1_score: 0.3471 - val_loss: 0.6887 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 24/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5311 - f1_score: 0.3471 - val_loss: 0.6886 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 25/300\n",
      "27/27 - 3s - loss: 0.6912 - acc: 0.5311 - f1_score: 0.3471 - val_loss: 0.6884 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 26/300\n",
      "27/27 - 3s - loss: 0.6911 - acc: 0.5311 - f1_score: 0.3471 - val_loss: 0.6883 - val_acc: 0.5833 - val_f1_score: 0.3684\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.42      0.34       133\n",
      "           1       0.72      0.59      0.64       335\n",
      "\n",
      "    accuracy                           0.54       468\n",
      "   macro avg       0.50      0.50      0.49       468\n",
      "weighted avg       0.60      0.54      0.56       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1106\n",
      "Epoch 1/300\n",
      "27/27 - 8s - loss: 0.6944 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.6998 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/300\n",
      "27/27 - 3s - loss: 0.6942 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.6983 - val_acc: 0.0855 - val_f1_score: 0.0794\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6940 - acc: 0.4911 - f1_score: 0.3304 - val_loss: 0.6969 - val_acc: 0.0855 - val_f1_score: 0.0794\n",
      "Epoch 4/300\n",
      "27/27 - 4s - loss: 0.6939 - acc: 0.4899 - f1_score: 0.3348 - val_loss: 0.6957 - val_acc: 0.1132 - val_f1_score: 0.1111\n",
      "Epoch 5/300\n",
      "27/27 - 3s - loss: 0.6937 - acc: 0.4847 - f1_score: 0.3432 - val_loss: 0.6945 - val_acc: 0.2286 - val_f1_score: 0.2221\n",
      "Epoch 6/300\n",
      "27/27 - 4s - loss: 0.6936 - acc: 0.4704 - f1_score: 0.3685 - val_loss: 0.6933 - val_acc: 0.4573 - val_f1_score: 0.3770\n",
      "Epoch 7/300\n",
      "27/27 - 4s - loss: 0.6935 - acc: 0.4711 - f1_score: 0.4067 - val_loss: 0.6923 - val_acc: 0.6774 - val_f1_score: 0.4740\n",
      "Epoch 8/300\n",
      "27/27 - 4s - loss: 0.6934 - acc: 0.4728 - f1_score: 0.4553 - val_loss: 0.6914 - val_acc: 0.8312 - val_f1_score: 0.5372\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6933 - acc: 0.4804 - f1_score: 0.4797 - val_loss: 0.6905 - val_acc: 0.8953 - val_f1_score: 0.5423\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.4985 - f1_score: 0.4916 - val_loss: 0.6897 - val_acc: 0.9167 - val_f1_score: 0.5448\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6931 - acc: 0.5054 - f1_score: 0.4744 - val_loss: 0.6889 - val_acc: 0.9188 - val_f1_score: 0.5469\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5098 - f1_score: 0.4402 - val_loss: 0.6882 - val_acc: 0.9188 - val_f1_score: 0.5469\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5124 - f1_score: 0.4161 - val_loss: 0.6875 - val_acc: 0.9188 - val_f1_score: 0.5469\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5085 - f1_score: 0.3813 - val_loss: 0.6869 - val_acc: 0.9188 - val_f1_score: 0.5469\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5121 - f1_score: 0.3686 - val_loss: 0.6863 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5115 - f1_score: 0.3542 - val_loss: 0.6858 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5112 - f1_score: 0.3486 - val_loss: 0.6853 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5112 - f1_score: 0.3458 - val_loss: 0.6848 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5111 - f1_score: 0.3445 - val_loss: 0.6844 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5108 - f1_score: 0.3433 - val_loss: 0.6840 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5108 - f1_score: 0.3433 - val_loss: 0.6837 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5108 - f1_score: 0.3431 - val_loss: 0.6833 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5106 - f1_score: 0.3425 - val_loss: 0.6831 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5105 - f1_score: 0.3422 - val_loss: 0.6828 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5105 - f1_score: 0.3422 - val_loss: 0.6825 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5105 - f1_score: 0.3422 - val_loss: 0.6823 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5105 - f1_score: 0.3422 - val_loss: 0.6821 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5104 - f1_score: 0.3418 - val_loss: 0.6819 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5104 - f1_score: 0.3418 - val_loss: 0.6818 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5104 - f1_score: 0.3418 - val_loss: 0.6816 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6921 - acc: 0.5104 - f1_score: 0.3418 - val_loss: 0.6815 - val_acc: 0.9167 - val_f1_score: 0.5247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.60      0.14         5\n",
      "           1       1.00      0.92      0.96       463\n",
      "\n",
      "    accuracy                           0.92       468\n",
      "   macro avg       0.54      0.76      0.55       468\n",
      "weighted avg       0.99      0.92      0.95       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1175\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6953 - acc: 0.4517 - f1_score: 0.3111 - val_loss: 0.6853 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6947 - acc: 0.4521 - f1_score: 0.3122 - val_loss: 0.6873 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6942 - acc: 0.4505 - f1_score: 0.3255 - val_loss: 0.6893 - val_acc: 0.7188 - val_f1_score: 0.4272\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6937 - acc: 0.4587 - f1_score: 0.4058 - val_loss: 0.6913 - val_acc: 0.6154 - val_f1_score: 0.4769\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4929 - f1_score: 0.4928 - val_loss: 0.6932 - val_acc: 0.5040 - val_f1_score: 0.4852\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5315 - f1_score: 0.4863 - val_loss: 0.6951 - val_acc: 0.3369 - val_f1_score: 0.3235\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5436 - f1_score: 0.4151 - val_loss: 0.6969 - val_acc: 0.2944 - val_f1_score: 0.2508\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5502 - f1_score: 0.3714 - val_loss: 0.6987 - val_acc: 0.2785 - val_f1_score: 0.2203\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6918 - acc: 0.5493 - f1_score: 0.3589 - val_loss: 0.7004 - val_acc: 0.2785 - val_f1_score: 0.2203\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5489 - f1_score: 0.3567 - val_loss: 0.7021 - val_acc: 0.2785 - val_f1_score: 0.2203\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5486 - f1_score: 0.3557 - val_loss: 0.7038 - val_acc: 0.2785 - val_f1_score: 0.2203\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.5485 - f1_score: 0.3551 - val_loss: 0.7055 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.5483 - f1_score: 0.3547 - val_loss: 0.7071 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.5482 - f1_score: 0.3544 - val_loss: 0.7086 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6902 - acc: 0.5482 - f1_score: 0.3541 - val_loss: 0.7101 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5482 - f1_score: 0.3541 - val_loss: 0.7118 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.5482 - f1_score: 0.3541 - val_loss: 0.7131 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7144 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6895 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7158 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6894 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7172 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6892 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7185 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7198 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6890 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7211 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6889 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7224 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6888 - acc: 0.5483 - f1_score: 0.3541 - val_loss: 0.7236 - val_acc: 0.2759 - val_f1_score: 0.2162\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58       176\n",
      "           1       0.57      0.29      0.39       201\n",
      "\n",
      "    accuracy                           0.50       377\n",
      "   macro avg       0.52      0.52      0.49       377\n",
      "weighted avg       0.53      0.50      0.48       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1194\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6941 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7054 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6939 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7041 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6938 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7028 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6937 - acc: 0.4826 - f1_score: 0.3265 - val_loss: 0.7012 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6936 - acc: 0.4819 - f1_score: 0.3310 - val_loss: 0.7001 - val_acc: 0.2022 - val_f1_score: 0.1697\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6935 - acc: 0.4795 - f1_score: 0.3397 - val_loss: 0.6993 - val_acc: 0.2022 - val_f1_score: 0.1697\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6934 - acc: 0.4792 - f1_score: 0.3538 - val_loss: 0.6981 - val_acc: 0.2132 - val_f1_score: 0.1847\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4763 - f1_score: 0.3844 - val_loss: 0.6971 - val_acc: 0.2286 - val_f1_score: 0.2076\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.4842 - f1_score: 0.4295 - val_loss: 0.6965 - val_acc: 0.2505 - val_f1_score: 0.2364\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.4923 - f1_score: 0.4567 - val_loss: 0.6961 - val_acc: 0.2659 - val_f1_score: 0.2558\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.4935 - f1_score: 0.4649 - val_loss: 0.6954 - val_acc: 0.3077 - val_f1_score: 0.3058\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6931 - acc: 0.4993 - f1_score: 0.4860 - val_loss: 0.6945 - val_acc: 0.3692 - val_f1_score: 0.3678\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6931 - acc: 0.5158 - f1_score: 0.5145 - val_loss: 0.6942 - val_acc: 0.3802 - val_f1_score: 0.3746\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5185 - f1_score: 0.5185 - val_loss: 0.6935 - val_acc: 0.4505 - val_f1_score: 0.4277\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5207 - f1_score: 0.5186 - val_loss: 0.6929 - val_acc: 0.4989 - val_f1_score: 0.4489\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5279 - f1_score: 0.5169 - val_loss: 0.6927 - val_acc: 0.5253 - val_f1_score: 0.4573\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5294 - f1_score: 0.5136 - val_loss: 0.6924 - val_acc: 0.5648 - val_f1_score: 0.4717\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5322 - f1_score: 0.5119 - val_loss: 0.6919 - val_acc: 0.6198 - val_f1_score: 0.4953\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5328 - f1_score: 0.4984 - val_loss: 0.6916 - val_acc: 0.6505 - val_f1_score: 0.5029\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5328 - f1_score: 0.4869 - val_loss: 0.6909 - val_acc: 0.7165 - val_f1_score: 0.5179\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5291 - f1_score: 0.4629 - val_loss: 0.6905 - val_acc: 0.7429 - val_f1_score: 0.5146\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5295 - f1_score: 0.4458 - val_loss: 0.6902 - val_acc: 0.7560 - val_f1_score: 0.5113\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5292 - f1_score: 0.4380 - val_loss: 0.6899 - val_acc: 0.7604 - val_f1_score: 0.5077\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5270 - f1_score: 0.4262 - val_loss: 0.6892 - val_acc: 0.7868 - val_f1_score: 0.5024\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5214 - f1_score: 0.3983 - val_loss: 0.6890 - val_acc: 0.7868 - val_f1_score: 0.4945\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5217 - f1_score: 0.3930 - val_loss: 0.6888 - val_acc: 0.7868 - val_f1_score: 0.4863\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5217 - f1_score: 0.3891 - val_loss: 0.6884 - val_acc: 0.7868 - val_f1_score: 0.4690\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5228 - f1_score: 0.3835 - val_loss: 0.6882 - val_acc: 0.7868 - val_f1_score: 0.4599\n",
      "Epoch 29/300\n",
      "28/28 - 1s - loss: 0.6926 - acc: 0.5234 - f1_score: 0.3791 - val_loss: 0.6878 - val_acc: 0.7912 - val_f1_score: 0.4616\n",
      "Epoch 30/300\n",
      "28/28 - 1s - loss: 0.6925 - acc: 0.5239 - f1_score: 0.3755 - val_loss: 0.6875 - val_acc: 0.7912 - val_f1_score: 0.4616\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5230 - f1_score: 0.3697 - val_loss: 0.6868 - val_acc: 0.8000 - val_f1_score: 0.4652\n",
      "Epoch 32/300\n",
      "28/28 - 1s - loss: 0.6925 - acc: 0.5210 - f1_score: 0.3607 - val_loss: 0.6864 - val_acc: 0.8022 - val_f1_score: 0.4661\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5207 - f1_score: 0.3555 - val_loss: 0.6856 - val_acc: 0.7978 - val_f1_score: 0.4438\n",
      "Epoch 34/300\n",
      "28/28 - 1s - loss: 0.6924 - acc: 0.5207 - f1_score: 0.3522 - val_loss: 0.6850 - val_acc: 0.7978 - val_f1_score: 0.4438\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5205 - f1_score: 0.3508 - val_loss: 0.6843 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5194 - f1_score: 0.3482 - val_loss: 0.6837 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5194 - f1_score: 0.3480 - val_loss: 0.6835 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5194 - f1_score: 0.3480 - val_loss: 0.6832 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Epoch 39/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5194 - f1_score: 0.3477 - val_loss: 0.6832 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Epoch 40/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5194 - f1_score: 0.3477 - val_loss: 0.6832 - val_acc: 0.8000 - val_f1_score: 0.4444\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00040: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.24      0.21        72\n",
      "           1       0.85      0.81      0.83       383\n",
      "\n",
      "    accuracy                           0.72       455\n",
      "   macro avg       0.52      0.52      0.52       455\n",
      "weighted avg       0.74      0.72      0.73       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1337\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6950 - acc: 0.4482 - f1_score: 0.3095 - val_loss: 0.6912 - val_acc: 0.7222 - val_f1_score: 0.4194\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6944 - acc: 0.4482 - f1_score: 0.3099 - val_loss: 0.6924 - val_acc: 0.6752 - val_f1_score: 0.4859\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6939 - acc: 0.4557 - f1_score: 0.3389 - val_loss: 0.6935 - val_acc: 0.3504 - val_f1_score: 0.3441\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6935 - acc: 0.4825 - f1_score: 0.4330 - val_loss: 0.6946 - val_acc: 0.2863 - val_f1_score: 0.2306\n",
      "Epoch 5/300\n",
      "27/27 - 1s - loss: 0.6931 - acc: 0.5153 - f1_score: 0.5149 - val_loss: 0.6957 - val_acc: 0.2863 - val_f1_score: 0.2306\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5416 - f1_score: 0.5001 - val_loss: 0.6967 - val_acc: 0.2863 - val_f1_score: 0.2306\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5473 - f1_score: 0.4269 - val_loss: 0.6977 - val_acc: 0.2863 - val_f1_score: 0.2306\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6921 - acc: 0.5516 - f1_score: 0.3760 - val_loss: 0.6986 - val_acc: 0.2842 - val_f1_score: 0.2273\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6918 - acc: 0.5519 - f1_score: 0.3598 - val_loss: 0.6995 - val_acc: 0.2821 - val_f1_score: 0.2240\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6915 - acc: 0.5521 - f1_score: 0.3581 - val_loss: 0.7003 - val_acc: 0.2799 - val_f1_score: 0.2207\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6913 - acc: 0.5521 - f1_score: 0.3578 - val_loss: 0.7012 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6910 - acc: 0.5518 - f1_score: 0.3565 - val_loss: 0.7020 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6908 - acc: 0.5519 - f1_score: 0.3565 - val_loss: 0.7027 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6907 - acc: 0.5518 - f1_score: 0.3562 - val_loss: 0.7035 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.5518 - f1_score: 0.3562 - val_loss: 0.7042 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6903 - acc: 0.5516 - f1_score: 0.3558 - val_loss: 0.7049 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.5518 - f1_score: 0.3559 - val_loss: 0.7056 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6900 - acc: 0.5518 - f1_score: 0.3559 - val_loss: 0.7062 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5516 - f1_score: 0.3555 - val_loss: 0.7069 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6898 - acc: 0.5516 - f1_score: 0.3555 - val_loss: 0.7075 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6897 - acc: 0.5516 - f1_score: 0.3555 - val_loss: 0.7080 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6896 - acc: 0.5516 - f1_score: 0.3555 - val_loss: 0.7086 - val_acc: 0.2778 - val_f1_score: 0.2174\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.72      0.80       414\n",
      "           1       0.12      0.30      0.17        54\n",
      "\n",
      "    accuracy                           0.68       468\n",
      "   macro avg       0.51      0.51      0.49       468\n",
      "weighted avg       0.80      0.68      0.73       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1390\n",
      "Epoch 1/300\n",
      "27/27 - 8s - loss: 0.6947 - acc: 0.4670 - f1_score: 0.3184 - val_loss: 0.6956 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6943 - acc: 0.4669 - f1_score: 0.3192 - val_loss: 0.6950 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6940 - acc: 0.4630 - f1_score: 0.3310 - val_loss: 0.6945 - val_acc: 0.4423 - val_f1_score: 0.3099\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6937 - acc: 0.4662 - f1_score: 0.3895 - val_loss: 0.6941 - val_acc: 0.4295 - val_f1_score: 0.3155\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6934 - acc: 0.4886 - f1_score: 0.4671 - val_loss: 0.6936 - val_acc: 0.4679 - val_f1_score: 0.4132\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.5053 - f1_score: 0.5049 - val_loss: 0.6932 - val_acc: 0.5043 - val_f1_score: 0.4929\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5276 - f1_score: 0.5015 - val_loss: 0.6929 - val_acc: 0.5192 - val_f1_score: 0.5155\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5322 - f1_score: 0.4328 - val_loss: 0.6925 - val_acc: 0.5342 - val_f1_score: 0.4898\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5321 - f1_score: 0.3819 - val_loss: 0.6922 - val_acc: 0.5620 - val_f1_score: 0.4528\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5338 - f1_score: 0.3624 - val_loss: 0.6919 - val_acc: 0.5598 - val_f1_score: 0.3880\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5343 - f1_score: 0.3546 - val_loss: 0.6917 - val_acc: 0.5577 - val_f1_score: 0.3667\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6921 - acc: 0.5347 - f1_score: 0.3534 - val_loss: 0.6914 - val_acc: 0.5534 - val_f1_score: 0.3563\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6920 - acc: 0.5338 - f1_score: 0.3514 - val_loss: 0.6912 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6919 - acc: 0.5337 - f1_score: 0.3508 - val_loss: 0.6910 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6918 - acc: 0.5337 - f1_score: 0.3505 - val_loss: 0.6908 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6917 - acc: 0.5337 - f1_score: 0.3502 - val_loss: 0.6906 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6916 - acc: 0.5335 - f1_score: 0.3499 - val_loss: 0.6904 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6915 - acc: 0.5331 - f1_score: 0.3489 - val_loss: 0.6903 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6914 - acc: 0.5332 - f1_score: 0.3489 - val_loss: 0.6901 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6913 - acc: 0.5332 - f1_score: 0.3489 - val_loss: 0.6900 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6913 - acc: 0.5330 - f1_score: 0.3482 - val_loss: 0.6898 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6912 - acc: 0.5328 - f1_score: 0.3479 - val_loss: 0.6897 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.5328 - f1_score: 0.3479 - val_loss: 0.6896 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6895 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6910 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6894 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6910 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6893 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6910 - acc: 0.5328 - f1_score: 0.3476 - val_loss: 0.6892 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.46      0.47       219\n",
      "           1       0.55      0.57      0.56       249\n",
      "\n",
      "    accuracy                           0.52       468\n",
      "   macro avg       0.52      0.52      0.52       468\n",
      "weighted avg       0.52      0.52      0.52       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1400\n",
      "Epoch 1/300\n",
      "27/27 - 7s - loss: 0.6939 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7079 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6937 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7061 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6936 - acc: 0.4878 - f1_score: 0.3288 - val_loss: 0.7045 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6935 - acc: 0.4862 - f1_score: 0.3311 - val_loss: 0.7029 - val_acc: 0.1410 - val_f1_score: 0.1247\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6934 - acc: 0.4821 - f1_score: 0.3410 - val_loss: 0.7015 - val_acc: 0.1453 - val_f1_score: 0.1300\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6934 - acc: 0.4781 - f1_score: 0.3722 - val_loss: 0.7001 - val_acc: 0.1474 - val_f1_score: 0.1327\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6933 - acc: 0.4849 - f1_score: 0.4023 - val_loss: 0.6989 - val_acc: 0.1709 - val_f1_score: 0.1621\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.4962 - f1_score: 0.4470 - val_loss: 0.6977 - val_acc: 0.2030 - val_f1_score: 0.2011\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.5060 - f1_score: 0.4794 - val_loss: 0.6966 - val_acc: 0.2521 - val_f1_score: 0.2516\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6931 - acc: 0.5117 - f1_score: 0.4982 - val_loss: 0.6956 - val_acc: 0.3034 - val_f1_score: 0.2947\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5130 - f1_score: 0.5102 - val_loss: 0.6946 - val_acc: 0.4017 - val_f1_score: 0.3640\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5167 - f1_score: 0.5166 - val_loss: 0.6936 - val_acc: 0.4872 - val_f1_score: 0.4071\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5173 - f1_score: 0.5107 - val_loss: 0.6928 - val_acc: 0.5833 - val_f1_score: 0.4522\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5263 - f1_score: 0.5074 - val_loss: 0.6919 - val_acc: 0.6538 - val_f1_score: 0.4773\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5215 - f1_score: 0.4771 - val_loss: 0.6911 - val_acc: 0.7201 - val_f1_score: 0.5134\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5217 - f1_score: 0.4578 - val_loss: 0.6904 - val_acc: 0.7543 - val_f1_score: 0.5199\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5231 - f1_score: 0.4406 - val_loss: 0.6897 - val_acc: 0.7927 - val_f1_score: 0.4882\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5193 - f1_score: 0.4164 - val_loss: 0.6890 - val_acc: 0.8077 - val_f1_score: 0.4871\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5179 - f1_score: 0.4033 - val_loss: 0.6884 - val_acc: 0.8291 - val_f1_score: 0.4878\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5173 - f1_score: 0.3866 - val_loss: 0.6878 - val_acc: 0.8376 - val_f1_score: 0.4806\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5164 - f1_score: 0.3753 - val_loss: 0.6873 - val_acc: 0.8419 - val_f1_score: 0.4825\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5167 - f1_score: 0.3710 - val_loss: 0.6867 - val_acc: 0.8526 - val_f1_score: 0.4874\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5170 - f1_score: 0.3660 - val_loss: 0.6862 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5177 - f1_score: 0.3628 - val_loss: 0.6858 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5162 - f1_score: 0.3559 - val_loss: 0.6853 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5163 - f1_score: 0.3532 - val_loss: 0.6849 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5163 - f1_score: 0.3517 - val_loss: 0.6845 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5162 - f1_score: 0.3506 - val_loss: 0.6842 - val_acc: 0.8526 - val_f1_score: 0.4742\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5160 - f1_score: 0.3501 - val_loss: 0.6838 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5162 - f1_score: 0.3489 - val_loss: 0.6835 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5159 - f1_score: 0.3479 - val_loss: 0.6832 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5159 - f1_score: 0.3479 - val_loss: 0.6829 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5156 - f1_score: 0.3473 - val_loss: 0.6826 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5157 - f1_score: 0.3476 - val_loss: 0.6823 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 35/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5153 - f1_score: 0.3467 - val_loss: 0.6820 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Epoch 36/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5156 - f1_score: 0.3473 - val_loss: 0.6818 - val_acc: 0.8504 - val_f1_score: 0.4596\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00036: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.17      0.18        76\n",
      "           1       0.84      0.87      0.86       392\n",
      "\n",
      "    accuracy                           0.75       468\n",
      "   macro avg       0.52      0.52      0.52       468\n",
      "weighted avg       0.74      0.75      0.75       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1419\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6944 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.7007 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6941 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.6995 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6939 - acc: 0.4754 - f1_score: 0.3229 - val_loss: 0.6983 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6937 - acc: 0.4738 - f1_score: 0.3288 - val_loss: 0.6974 - val_acc: 0.3165 - val_f1_score: 0.2427\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6936 - acc: 0.4718 - f1_score: 0.3481 - val_loss: 0.6967 - val_acc: 0.3187 - val_f1_score: 0.2486\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6934 - acc: 0.4712 - f1_score: 0.3808 - val_loss: 0.6960 - val_acc: 0.3187 - val_f1_score: 0.2591\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4818 - f1_score: 0.4352 - val_loss: 0.6952 - val_acc: 0.3209 - val_f1_score: 0.2906\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.4973 - f1_score: 0.4847 - val_loss: 0.6944 - val_acc: 0.3670 - val_f1_score: 0.3654\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5172 - f1_score: 0.5171 - val_loss: 0.6939 - val_acc: 0.4022 - val_f1_score: 0.3986\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5231 - f1_score: 0.5148 - val_loss: 0.6935 - val_acc: 0.4593 - val_f1_score: 0.4368\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5318 - f1_score: 0.5096 - val_loss: 0.6929 - val_acc: 0.5363 - val_f1_score: 0.4651\n",
      "Epoch 12/300\n",
      "28/28 - 1s - loss: 0.6928 - acc: 0.5335 - f1_score: 0.4773 - val_loss: 0.6921 - val_acc: 0.6154 - val_f1_score: 0.4803\n",
      "Epoch 13/300\n",
      "28/28 - 1s - loss: 0.6927 - acc: 0.5317 - f1_score: 0.4315 - val_loss: 0.6916 - val_acc: 0.6615 - val_f1_score: 0.4797\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5253 - f1_score: 0.3938 - val_loss: 0.6912 - val_acc: 0.6725 - val_f1_score: 0.4536\n",
      "Epoch 15/300\n",
      "28/28 - 1s - loss: 0.6926 - acc: 0.5265 - f1_score: 0.3828 - val_loss: 0.6908 - val_acc: 0.6857 - val_f1_score: 0.4440\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5267 - f1_score: 0.3674 - val_loss: 0.6905 - val_acc: 0.6879 - val_f1_score: 0.4450\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5269 - f1_score: 0.3640 - val_loss: 0.6903 - val_acc: 0.6879 - val_f1_score: 0.4392\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5270 - f1_score: 0.3608 - val_loss: 0.6900 - val_acc: 0.6879 - val_f1_score: 0.4332\n",
      "Epoch 19/300\n",
      "28/28 - 1s - loss: 0.6924 - acc: 0.5272 - f1_score: 0.3565 - val_loss: 0.6897 - val_acc: 0.6879 - val_f1_score: 0.4271\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5270 - f1_score: 0.3543 - val_loss: 0.6892 - val_acc: 0.6901 - val_f1_score: 0.4216\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5267 - f1_score: 0.3513 - val_loss: 0.6888 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.5265 - f1_score: 0.3506 - val_loss: 0.6886 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.5263 - f1_score: 0.3503 - val_loss: 0.6884 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.5263 - f1_score: 0.3500 - val_loss: 0.6878 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5266 - f1_score: 0.3501 - val_loss: 0.6876 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5266 - f1_score: 0.3501 - val_loss: 0.6875 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5266 - f1_score: 0.3501 - val_loss: 0.6871 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5263 - f1_score: 0.3495 - val_loss: 0.6870 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5263 - f1_score: 0.3495 - val_loss: 0.6866 - val_acc: 0.6879 - val_f1_score: 0.4143\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5263 - f1_score: 0.3495 - val_loss: 0.6865 - val_acc: 0.6857 - val_f1_score: 0.4068\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5263 - f1_score: 0.3495 - val_loss: 0.6860 - val_acc: 0.6857 - val_f1_score: 0.4068\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5263 - f1_score: 0.3495 - val_loss: 0.6857 - val_acc: 0.6857 - val_f1_score: 0.4068\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.30      0.22        80\n",
      "           1       0.82      0.68      0.75       375\n",
      "\n",
      "    accuracy                           0.62       455\n",
      "   macro avg       0.49      0.49      0.48       455\n",
      "weighted avg       0.71      0.62      0.65       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1517\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6956 - acc: 0.4426 - f1_score: 0.3068 - val_loss: 0.6795 - val_acc: 0.9259 - val_f1_score: 0.4808\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6948 - acc: 0.4409 - f1_score: 0.3103 - val_loss: 0.6843 - val_acc: 0.9202 - val_f1_score: 0.5125\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6940 - acc: 0.4406 - f1_score: 0.3748 - val_loss: 0.6889 - val_acc: 0.8234 - val_f1_score: 0.5317\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6934 - acc: 0.4826 - f1_score: 0.4807 - val_loss: 0.6936 - val_acc: 0.4587 - val_f1_score: 0.3829\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5410 - f1_score: 0.4826 - val_loss: 0.6980 - val_acc: 0.1567 - val_f1_score: 0.1564\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.5560 - f1_score: 0.3950 - val_loss: 0.7022 - val_acc: 0.0798 - val_f1_score: 0.0754\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6917 - acc: 0.5583 - f1_score: 0.3655 - val_loss: 0.7064 - val_acc: 0.0769 - val_f1_score: 0.0722\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5578 - f1_score: 0.3601 - val_loss: 0.7104 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.5574 - f1_score: 0.3588 - val_loss: 0.7143 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.5574 - f1_score: 0.3585 - val_loss: 0.7180 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5573 - f1_score: 0.3581 - val_loss: 0.7215 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5571 - f1_score: 0.3578 - val_loss: 0.7251 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6894 - acc: 0.5573 - f1_score: 0.3578 - val_loss: 0.7285 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5573 - f1_score: 0.3578 - val_loss: 0.7318 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6888 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7352 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6885 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7385 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6883 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7417 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6881 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7448 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6879 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7478 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6877 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7507 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6875 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7535 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6874 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7563 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6872 - acc: 0.5574 - f1_score: 0.3579 - val_loss: 0.7592 - val_acc: 0.0741 - val_f1_score: 0.0690\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       303\n",
      "           1       0.23      0.12      0.16        48\n",
      "\n",
      "    accuracy                           0.82       351\n",
      "   macro avg       0.55      0.53      0.53       351\n",
      "weighted avg       0.78      0.82      0.80       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1544\n",
      "Epoch 1/300\n",
      "29/29 - 8s - loss: 0.6947 - acc: 0.4669 - f1_score: 0.3183 - val_loss: 0.6954 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/300\n",
      "29/29 - 2s - loss: 0.6943 - acc: 0.4664 - f1_score: 0.3187 - val_loss: 0.6945 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 3/300\n",
      "29/29 - 2s - loss: 0.6940 - acc: 0.4632 - f1_score: 0.3329 - val_loss: 0.6936 - val_acc: 0.3750 - val_f1_score: 0.2844\n",
      "Epoch 4/300\n",
      "29/29 - 2s - loss: 0.6936 - acc: 0.4631 - f1_score: 0.3943 - val_loss: 0.6928 - val_acc: 0.4327 - val_f1_score: 0.4206\n",
      "Epoch 5/300\n",
      "29/29 - 2s - loss: 0.6934 - acc: 0.4864 - f1_score: 0.4756 - val_loss: 0.6921 - val_acc: 0.5192 - val_f1_score: 0.5008\n",
      "Epoch 6/300\n",
      "29/29 - 2s - loss: 0.6931 - acc: 0.5140 - f1_score: 0.5091 - val_loss: 0.6914 - val_acc: 0.6635 - val_f1_score: 0.5859\n",
      "Epoch 7/300\n",
      "29/29 - 2s - loss: 0.6929 - acc: 0.5253 - f1_score: 0.4604 - val_loss: 0.6908 - val_acc: 0.6827 - val_f1_score: 0.5595\n",
      "Epoch 8/300\n",
      "29/29 - 2s - loss: 0.6927 - acc: 0.5330 - f1_score: 0.4064 - val_loss: 0.6902 - val_acc: 0.6731 - val_f1_score: 0.5100\n",
      "Epoch 9/300\n",
      "29/29 - 2s - loss: 0.6925 - acc: 0.5335 - f1_score: 0.3687 - val_loss: 0.6896 - val_acc: 0.6442 - val_f1_score: 0.4380\n",
      "Epoch 10/300\n",
      "29/29 - 2s - loss: 0.6923 - acc: 0.5345 - f1_score: 0.3562 - val_loss: 0.6891 - val_acc: 0.6442 - val_f1_score: 0.4380\n",
      "Epoch 11/300\n",
      "29/29 - 2s - loss: 0.6921 - acc: 0.5341 - f1_score: 0.3519 - val_loss: 0.6886 - val_acc: 0.6442 - val_f1_score: 0.4380\n",
      "Epoch 12/300\n",
      "29/29 - 2s - loss: 0.6920 - acc: 0.5335 - f1_score: 0.3503 - val_loss: 0.6882 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 13/300\n",
      "29/29 - 2s - loss: 0.6919 - acc: 0.5336 - f1_score: 0.3501 - val_loss: 0.6877 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 14/300\n",
      "29/29 - 2s - loss: 0.6917 - acc: 0.5336 - f1_score: 0.3498 - val_loss: 0.6873 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 15/300\n",
      "29/29 - 2s - loss: 0.6916 - acc: 0.5332 - f1_score: 0.3489 - val_loss: 0.6870 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 16/300\n",
      "29/29 - 2s - loss: 0.6915 - acc: 0.5331 - f1_score: 0.3485 - val_loss: 0.6866 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 17/300\n",
      "29/29 - 2s - loss: 0.6915 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6863 - val_acc: 0.6346 - val_f1_score: 0.4119\n",
      "Epoch 18/300\n",
      "29/29 - 2s - loss: 0.6914 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6860 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 19/300\n",
      "29/29 - 2s - loss: 0.6913 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6857 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 20/300\n",
      "29/29 - 2s - loss: 0.6912 - acc: 0.5330 - f1_score: 0.3479 - val_loss: 0.6854 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 21/300\n",
      "29/29 - 2s - loss: 0.6912 - acc: 0.5331 - f1_score: 0.3480 - val_loss: 0.6852 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 22/300\n",
      "29/29 - 2s - loss: 0.6911 - acc: 0.5331 - f1_score: 0.3480 - val_loss: 0.6849 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 23/300\n",
      "29/29 - 2s - loss: 0.6911 - acc: 0.5331 - f1_score: 0.3480 - val_loss: 0.6847 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 24/300\n",
      "29/29 - 2s - loss: 0.6910 - acc: 0.5330 - f1_score: 0.3477 - val_loss: 0.6844 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 25/300\n",
      "29/29 - 2s - loss: 0.6910 - acc: 0.5330 - f1_score: 0.3477 - val_loss: 0.6842 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Epoch 26/300\n",
      "29/29 - 2s - loss: 0.6909 - acc: 0.5330 - f1_score: 0.3477 - val_loss: 0.6840 - val_acc: 0.6250 - val_f1_score: 0.3846\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.60      0.41        20\n",
      "           1       0.88      0.68      0.77        84\n",
      "\n",
      "    accuracy                           0.66       104\n",
      "   macro avg       0.59      0.64      0.59       104\n",
      "weighted avg       0.77      0.66      0.70       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1624\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6951 - acc: 0.4556 - f1_score: 0.3130 - val_loss: 0.6881 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6946 - acc: 0.4553 - f1_score: 0.3135 - val_loss: 0.6894 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6941 - acc: 0.4534 - f1_score: 0.3324 - val_loss: 0.6907 - val_acc: 0.6496 - val_f1_score: 0.4014\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6937 - acc: 0.4598 - f1_score: 0.4028 - val_loss: 0.6921 - val_acc: 0.5954 - val_f1_score: 0.4444\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4946 - f1_score: 0.4937 - val_loss: 0.6933 - val_acc: 0.4900 - val_f1_score: 0.4725\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5302 - f1_score: 0.4963 - val_loss: 0.6946 - val_acc: 0.3846 - val_f1_score: 0.3655\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5425 - f1_score: 0.4304 - val_loss: 0.6958 - val_acc: 0.3333 - val_f1_score: 0.2705\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5457 - f1_score: 0.3732 - val_loss: 0.6969 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5462 - f1_score: 0.3594 - val_loss: 0.6981 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6917 - acc: 0.5450 - f1_score: 0.3553 - val_loss: 0.6991 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5453 - f1_score: 0.3552 - val_loss: 0.7001 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5447 - f1_score: 0.3538 - val_loss: 0.7011 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5444 - f1_score: 0.3528 - val_loss: 0.7021 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.5444 - f1_score: 0.3528 - val_loss: 0.7030 - val_acc: 0.3333 - val_f1_score: 0.2532\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7039 - val_acc: 0.3305 - val_f1_score: 0.2484\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7048 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7057 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7065 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6902 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7073 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5443 - f1_score: 0.3525 - val_loss: 0.7081 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6899 - acc: 0.5444 - f1_score: 0.3525 - val_loss: 0.7088 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.5444 - f1_score: 0.3525 - val_loss: 0.7096 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.5444 - f1_score: 0.3525 - val_loss: 0.7103 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5444 - f1_score: 0.3525 - val_loss: 0.7110 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6896 - acc: 0.5444 - f1_score: 0.3525 - val_loss: 0.7116 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D9A010F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.65      0.57       181\n",
      "           1       0.46      0.32      0.38       170\n",
      "\n",
      "    accuracy                           0.49       351\n",
      "   macro avg       0.48      0.48      0.47       351\n",
      "weighted avg       0.48      0.49      0.48       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1674\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6939 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7129 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6938 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7106 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6937 - acc: 0.4854 - f1_score: 0.3270 - val_loss: 0.7083 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6936 - acc: 0.4850 - f1_score: 0.3285 - val_loss: 0.7063 - val_acc: 0.0431 - val_f1_score: 0.0418\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6935 - acc: 0.4818 - f1_score: 0.3354 - val_loss: 0.7042 - val_acc: 0.0523 - val_f1_score: 0.0516\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6934 - acc: 0.4746 - f1_score: 0.3571 - val_loss: 0.7024 - val_acc: 0.0646 - val_f1_score: 0.0644\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4821 - f1_score: 0.3958 - val_loss: 0.7007 - val_acc: 0.0831 - val_f1_score: 0.0831\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.4874 - f1_score: 0.4350 - val_loss: 0.6990 - val_acc: 0.1200 - val_f1_score: 0.1186\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6932 - acc: 0.5038 - f1_score: 0.4794 - val_loss: 0.6975 - val_acc: 0.1754 - val_f1_score: 0.1668\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6931 - acc: 0.5105 - f1_score: 0.5004 - val_loss: 0.6961 - val_acc: 0.2585 - val_f1_score: 0.2305\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5165 - f1_score: 0.5155 - val_loss: 0.6947 - val_acc: 0.3446 - val_f1_score: 0.2875\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5200 - f1_score: 0.5191 - val_loss: 0.6934 - val_acc: 0.4554 - val_f1_score: 0.3438\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5274 - f1_score: 0.5162 - val_loss: 0.6923 - val_acc: 0.5538 - val_f1_score: 0.3964\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5278 - f1_score: 0.4965 - val_loss: 0.6912 - val_acc: 0.6708 - val_f1_score: 0.4497\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5288 - f1_score: 0.4844 - val_loss: 0.6900 - val_acc: 0.7600 - val_f1_score: 0.4668\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5268 - f1_score: 0.4537 - val_loss: 0.6890 - val_acc: 0.8185 - val_f1_score: 0.4663\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5271 - f1_score: 0.4378 - val_loss: 0.6880 - val_acc: 0.8708 - val_f1_score: 0.4881\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5213 - f1_score: 0.4048 - val_loss: 0.6870 - val_acc: 0.8985 - val_f1_score: 0.5017\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5199 - f1_score: 0.3914 - val_loss: 0.6862 - val_acc: 0.9200 - val_f1_score: 0.4792\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6927 - acc: 0.5200 - f1_score: 0.3796 - val_loss: 0.6853 - val_acc: 0.9323 - val_f1_score: 0.4825\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5199 - f1_score: 0.3695 - val_loss: 0.6845 - val_acc: 0.9354 - val_f1_score: 0.4833\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5193 - f1_score: 0.3646 - val_loss: 0.6838 - val_acc: 0.9446 - val_f1_score: 0.4858\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5183 - f1_score: 0.3571 - val_loss: 0.6831 - val_acc: 0.9508 - val_f1_score: 0.4874\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5187 - f1_score: 0.3541 - val_loss: 0.6824 - val_acc: 0.9508 - val_f1_score: 0.4874\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5180 - f1_score: 0.3521 - val_loss: 0.6818 - val_acc: 0.9538 - val_f1_score: 0.4882\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5180 - f1_score: 0.3503 - val_loss: 0.6813 - val_acc: 0.9538 - val_f1_score: 0.4882\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5180 - f1_score: 0.3495 - val_loss: 0.6807 - val_acc: 0.9538 - val_f1_score: 0.4882\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5177 - f1_score: 0.3487 - val_loss: 0.6802 - val_acc: 0.9569 - val_f1_score: 0.4890\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5170 - f1_score: 0.3471 - val_loss: 0.6797 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5169 - f1_score: 0.3467 - val_loss: 0.6794 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5169 - f1_score: 0.3467 - val_loss: 0.6789 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5170 - f1_score: 0.3468 - val_loss: 0.6786 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5170 - f1_score: 0.3468 - val_loss: 0.6781 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 34/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5170 - f1_score: 0.3468 - val_loss: 0.6776 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5169 - f1_score: 0.3462 - val_loss: 0.6771 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5167 - f1_score: 0.3459 - val_loss: 0.6769 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5169 - f1_score: 0.3462 - val_loss: 0.6766 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5167 - f1_score: 0.3459 - val_loss: 0.6762 - val_acc: 0.9600 - val_f1_score: 0.4898\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014E1A7CA280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.05      0.06        22\n",
      "           1       0.93      0.96      0.95       303\n",
      "\n",
      "    accuracy                           0.90       325\n",
      "   macro avg       0.50      0.50      0.50       325\n",
      "weighted avg       0.87      0.90      0.89       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1688\n",
      "Epoch 1/300\n",
      "28/28 - 10s - loss: 0.6948 - acc: 0.4618 - f1_score: 0.3159 - val_loss: 0.6925 - val_acc: 0.5333 - val_f1_score: 0.3478\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6944 - acc: 0.4615 - f1_score: 0.3165 - val_loss: 0.6927 - val_acc: 0.5333 - val_f1_score: 0.3576\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6940 - acc: 0.4537 - f1_score: 0.3320 - val_loss: 0.6929 - val_acc: 0.5359 - val_f1_score: 0.4706\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6936 - acc: 0.4645 - f1_score: 0.3953 - val_loss: 0.6931 - val_acc: 0.4590 - val_f1_score: 0.4435\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4932 - f1_score: 0.4805 - val_loss: 0.6932 - val_acc: 0.4821 - val_f1_score: 0.3675\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6930 - acc: 0.5207 - f1_score: 0.5134 - val_loss: 0.6934 - val_acc: 0.4821 - val_f1_score: 0.3496\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6928 - acc: 0.5333 - f1_score: 0.4645 - val_loss: 0.6936 - val_acc: 0.4769 - val_f1_score: 0.3393\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5383 - f1_score: 0.3998 - val_loss: 0.6937 - val_acc: 0.4769 - val_f1_score: 0.3393\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5405 - f1_score: 0.3661 - val_loss: 0.6939 - val_acc: 0.4744 - val_f1_score: 0.3341\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5396 - f1_score: 0.3563 - val_loss: 0.6941 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5389 - f1_score: 0.3533 - val_loss: 0.6942 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6918 - acc: 0.5386 - f1_score: 0.3520 - val_loss: 0.6944 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.5383 - f1_score: 0.3511 - val_loss: 0.6945 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5383 - f1_score: 0.3511 - val_loss: 0.6946 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5383 - f1_score: 0.3508 - val_loss: 0.6948 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5382 - f1_score: 0.3504 - val_loss: 0.6949 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5380 - f1_score: 0.3501 - val_loss: 0.6950 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6910 - acc: 0.5380 - f1_score: 0.3501 - val_loss: 0.6951 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.5380 - f1_score: 0.3501 - val_loss: 0.6952 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.5382 - f1_score: 0.3502 - val_loss: 0.6954 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.5382 - f1_score: 0.3502 - val_loss: 0.6955 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.5380 - f1_score: 0.3498 - val_loss: 0.6956 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.5380 - f1_score: 0.3498 - val_loss: 0.6957 - val_acc: 0.4667 - val_f1_score: 0.3182\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D995E4CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.66       319\n",
      "           1       0.20      0.51      0.28        71\n",
      "\n",
      "    accuracy                           0.54       390\n",
      "   macro avg       0.51      0.52      0.47       390\n",
      "weighted avg       0.72      0.54      0.59       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1717\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6947 - acc: 0.4663 - f1_score: 0.3180 - val_loss: 0.6951 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6943 - acc: 0.4656 - f1_score: 0.3190 - val_loss: 0.6945 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6939 - acc: 0.4638 - f1_score: 0.3361 - val_loss: 0.6940 - val_acc: 0.4487 - val_f1_score: 0.3225\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6936 - acc: 0.4657 - f1_score: 0.4060 - val_loss: 0.6935 - val_acc: 0.4744 - val_f1_score: 0.3987\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6933 - acc: 0.4903 - f1_score: 0.4822 - val_loss: 0.6931 - val_acc: 0.5256 - val_f1_score: 0.5231\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6931 - acc: 0.5111 - f1_score: 0.5046 - val_loss: 0.6927 - val_acc: 0.5385 - val_f1_score: 0.5060\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5302 - f1_score: 0.4701 - val_loss: 0.6924 - val_acc: 0.5641 - val_f1_score: 0.4568\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6926 - acc: 0.5341 - f1_score: 0.4092 - val_loss: 0.6921 - val_acc: 0.5470 - val_f1_score: 0.3701\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5357 - f1_score: 0.3691 - val_loss: 0.6918 - val_acc: 0.5513 - val_f1_score: 0.3639\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5361 - f1_score: 0.3584 - val_loss: 0.6916 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5354 - f1_score: 0.3541 - val_loss: 0.6913 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5344 - f1_score: 0.3515 - val_loss: 0.6911 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5343 - f1_score: 0.3507 - val_loss: 0.6909 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6917 - acc: 0.5344 - f1_score: 0.3505 - val_loss: 0.6907 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.5340 - f1_score: 0.3495 - val_loss: 0.6905 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5340 - f1_score: 0.3495 - val_loss: 0.6904 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.5338 - f1_score: 0.3489 - val_loss: 0.6902 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5337 - f1_score: 0.3485 - val_loss: 0.6901 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5336 - f1_score: 0.3482 - val_loss: 0.6900 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5336 - f1_score: 0.3482 - val_loss: 0.6898 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5336 - f1_score: 0.3482 - val_loss: 0.6897 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5337 - f1_score: 0.3483 - val_loss: 0.6896 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6910 - acc: 0.5337 - f1_score: 0.3483 - val_loss: 0.6895 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6910 - acc: 0.5336 - f1_score: 0.3479 - val_loss: 0.6894 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.5336 - f1_score: 0.3479 - val_loss: 0.6894 - val_acc: 0.5556 - val_f1_score: 0.3571\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.48      0.56       147\n",
      "           1       0.41      0.61      0.49        87\n",
      "\n",
      "    accuracy                           0.53       234\n",
      "   macro avg       0.54      0.54      0.52       234\n",
      "weighted avg       0.57      0.53      0.53       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1818\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6944 - acc: 0.4727 - f1_score: 0.3210 - val_loss: 0.6990 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 1s - loss: 0.6942 - acc: 0.4728 - f1_score: 0.3215 - val_loss: 0.6980 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6939 - acc: 0.4714 - f1_score: 0.3223 - val_loss: 0.6970 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6937 - acc: 0.4644 - f1_score: 0.3349 - val_loss: 0.6960 - val_acc: 0.3632 - val_f1_score: 0.2716\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6935 - acc: 0.4689 - f1_score: 0.3942 - val_loss: 0.6951 - val_acc: 0.3697 - val_f1_score: 0.2920\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6933 - acc: 0.4889 - f1_score: 0.4640 - val_loss: 0.6943 - val_acc: 0.4038 - val_f1_score: 0.3655\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.5054 - f1_score: 0.5031 - val_loss: 0.6935 - val_acc: 0.4637 - val_f1_score: 0.4621\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5159 - f1_score: 0.5076 - val_loss: 0.6928 - val_acc: 0.5513 - val_f1_score: 0.5324\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5266 - f1_score: 0.4804 - val_loss: 0.6921 - val_acc: 0.5940 - val_f1_score: 0.4988\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5282 - f1_score: 0.4349 - val_loss: 0.6914 - val_acc: 0.6175 - val_f1_score: 0.4524\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5292 - f1_score: 0.3948 - val_loss: 0.6908 - val_acc: 0.6218 - val_f1_score: 0.4133\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5299 - f1_score: 0.3712 - val_loss: 0.6902 - val_acc: 0.6346 - val_f1_score: 0.3882\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5305 - f1_score: 0.3596 - val_loss: 0.6897 - val_acc: 0.6368 - val_f1_score: 0.3890\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5299 - f1_score: 0.3535 - val_loss: 0.6891 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5293 - f1_score: 0.3519 - val_loss: 0.6887 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5290 - f1_score: 0.3509 - val_loss: 0.6882 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6921 - acc: 0.5288 - f1_score: 0.3503 - val_loss: 0.6878 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6921 - acc: 0.5285 - f1_score: 0.3496 - val_loss: 0.6874 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6920 - acc: 0.5286 - f1_score: 0.3494 - val_loss: 0.6870 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6919 - acc: 0.5285 - f1_score: 0.3491 - val_loss: 0.6866 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6919 - acc: 0.5286 - f1_score: 0.3491 - val_loss: 0.6862 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6918 - acc: 0.5283 - f1_score: 0.3484 - val_loss: 0.6859 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6918 - acc: 0.5283 - f1_score: 0.3484 - val_loss: 0.6856 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6917 - acc: 0.5282 - f1_score: 0.3481 - val_loss: 0.6853 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6917 - acc: 0.5282 - f1_score: 0.3481 - val_loss: 0.6850 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6917 - acc: 0.5282 - f1_score: 0.3481 - val_loss: 0.6847 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6916 - acc: 0.5282 - f1_score: 0.3481 - val_loss: 0.6844 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6916 - acc: 0.5280 - f1_score: 0.3478 - val_loss: 0.6842 - val_acc: 0.6389 - val_f1_score: 0.3898\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014DBFEB8820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.40      0.44       205\n",
      "           1       0.59      0.67      0.63       263\n",
      "\n",
      "    accuracy                           0.55       468\n",
      "   macro avg       0.54      0.53      0.53       468\n",
      "weighted avg       0.54      0.55      0.54       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1892\n",
      "Epoch 1/300\n",
      "27/27 - 5s - loss: 0.6960 - acc: 0.4313 - f1_score: 0.3013 - val_loss: 0.6785 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6950 - acc: 0.4279 - f1_score: 0.3039 - val_loss: 0.6841 - val_acc: 0.9573 - val_f1_score: 0.4891\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6941 - acc: 0.4344 - f1_score: 0.3763 - val_loss: 0.6896 - val_acc: 0.8098 - val_f1_score: 0.4687\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6933 - acc: 0.4998 - f1_score: 0.4986 - val_loss: 0.6949 - val_acc: 0.3504 - val_f1_score: 0.2804\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5573 - f1_score: 0.4645 - val_loss: 0.7002 - val_acc: 0.0449 - val_f1_score: 0.0448\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6918 - acc: 0.5696 - f1_score: 0.3836 - val_loss: 0.7052 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.5695 - f1_score: 0.3662 - val_loss: 0.7102 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.5690 - f1_score: 0.3642 - val_loss: 0.7151 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5687 - f1_score: 0.3632 - val_loss: 0.7200 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6894 - acc: 0.5686 - f1_score: 0.3628 - val_loss: 0.7248 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6889 - acc: 0.5686 - f1_score: 0.3625 - val_loss: 0.7295 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6884 - acc: 0.5686 - f1_score: 0.3625 - val_loss: 0.7342 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6879 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7388 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6875 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7434 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7479 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6867 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7522 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6864 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7566 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6861 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7608 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6858 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7651 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6855 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7693 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6852 - acc: 0.5687 - f1_score: 0.3625 - val_loss: 0.7733 - val_acc: 0.0278 - val_f1_score: 0.0270\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       468\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       468\n",
      "   macro avg       0.50      0.49      0.49       468\n",
      "weighted avg       1.00      0.97      0.99       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1929\n",
      "Epoch 1/300\n",
      "27/27 - 8s - loss: 0.6940 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7056 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6939 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7041 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6937 - acc: 0.4821 - f1_score: 0.3260 - val_loss: 0.7026 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6936 - acc: 0.4812 - f1_score: 0.3285 - val_loss: 0.7013 - val_acc: 0.2222 - val_f1_score: 0.1818\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6935 - acc: 0.4798 - f1_score: 0.3423 - val_loss: 0.7000 - val_acc: 0.2265 - val_f1_score: 0.1879\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6934 - acc: 0.4741 - f1_score: 0.3726 - val_loss: 0.6988 - val_acc: 0.2286 - val_f1_score: 0.1910\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6933 - acc: 0.4815 - f1_score: 0.4138 - val_loss: 0.6976 - val_acc: 0.2329 - val_f1_score: 0.1999\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6932 - acc: 0.4931 - f1_score: 0.4564 - val_loss: 0.6965 - val_acc: 0.2714 - val_f1_score: 0.2561\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6931 - acc: 0.5069 - f1_score: 0.4942 - val_loss: 0.6955 - val_acc: 0.3269 - val_f1_score: 0.3247\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5140 - f1_score: 0.5124 - val_loss: 0.6945 - val_acc: 0.4060 - val_f1_score: 0.4032\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6930 - acc: 0.5160 - f1_score: 0.5143 - val_loss: 0.6936 - val_acc: 0.4637 - val_f1_score: 0.4352\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5264 - f1_score: 0.5097 - val_loss: 0.6927 - val_acc: 0.5470 - val_f1_score: 0.4782\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6929 - acc: 0.5285 - f1_score: 0.4840 - val_loss: 0.6918 - val_acc: 0.6197 - val_f1_score: 0.4957\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5259 - f1_score: 0.4544 - val_loss: 0.6910 - val_acc: 0.6774 - val_f1_score: 0.5182\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6928 - acc: 0.5263 - f1_score: 0.4244 - val_loss: 0.6903 - val_acc: 0.7137 - val_f1_score: 0.4999\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5231 - f1_score: 0.4008 - val_loss: 0.6896 - val_acc: 0.7308 - val_f1_score: 0.4887\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6927 - acc: 0.5234 - f1_score: 0.3893 - val_loss: 0.6889 - val_acc: 0.7350 - val_f1_score: 0.4462\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5221 - f1_score: 0.3730 - val_loss: 0.6883 - val_acc: 0.7585 - val_f1_score: 0.4561\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5227 - f1_score: 0.3657 - val_loss: 0.6877 - val_acc: 0.7628 - val_f1_score: 0.4414\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6926 - acc: 0.5211 - f1_score: 0.3578 - val_loss: 0.6871 - val_acc: 0.7714 - val_f1_score: 0.4445\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5214 - f1_score: 0.3538 - val_loss: 0.6865 - val_acc: 0.7735 - val_f1_score: 0.4361\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5211 - f1_score: 0.3516 - val_loss: 0.6860 - val_acc: 0.7735 - val_f1_score: 0.4361\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6925 - acc: 0.5205 - f1_score: 0.3501 - val_loss: 0.6855 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5201 - f1_score: 0.3483 - val_loss: 0.6850 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5201 - f1_score: 0.3480 - val_loss: 0.6845 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5201 - f1_score: 0.3480 - val_loss: 0.6841 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.5201 - f1_score: 0.3478 - val_loss: 0.6837 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 28/300\n",
      "27/27 - 1s - loss: 0.6923 - acc: 0.5199 - f1_score: 0.3474 - val_loss: 0.6833 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 29/300\n",
      "27/27 - 1s - loss: 0.6923 - acc: 0.5199 - f1_score: 0.3474 - val_loss: 0.6830 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5199 - f1_score: 0.3474 - val_loss: 0.6826 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6923 - acc: 0.5199 - f1_score: 0.3474 - val_loss: 0.6823 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5199 - f1_score: 0.3474 - val_loss: 0.6820 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5198 - f1_score: 0.3471 - val_loss: 0.6817 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6922 - acc: 0.5198 - f1_score: 0.3471 - val_loss: 0.6814 - val_acc: 0.7778 - val_f1_score: 0.4375\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00034: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.25      0.24        95\n",
      "           1       0.80      0.79      0.80       373\n",
      "\n",
      "    accuracy                           0.68       468\n",
      "   macro avg       0.52      0.52      0.52       468\n",
      "weighted avg       0.69      0.68      0.68       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "Testing on 1933\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6945 - acc: 0.4699 - f1_score: 0.3197 - val_loss: 0.7003 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 1s - loss: 0.6941 - acc: 0.4691 - f1_score: 0.3207 - val_loss: 0.6988 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 3/300\n",
      "28/28 - 1s - loss: 0.6938 - acc: 0.4655 - f1_score: 0.3392 - val_loss: 0.6973 - val_acc: 0.3248 - val_f1_score: 0.2452\n",
      "Epoch 4/300\n",
      "28/28 - 1s - loss: 0.6936 - acc: 0.4702 - f1_score: 0.4085 - val_loss: 0.6960 - val_acc: 0.3632 - val_f1_score: 0.3168\n",
      "Epoch 5/300\n",
      "28/28 - 1s - loss: 0.6933 - acc: 0.4949 - f1_score: 0.4809 - val_loss: 0.6948 - val_acc: 0.4188 - val_f1_score: 0.4091\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6931 - acc: 0.5099 - f1_score: 0.5075 - val_loss: 0.6936 - val_acc: 0.4872 - val_f1_score: 0.4848\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6929 - acc: 0.5284 - f1_score: 0.4833 - val_loss: 0.6925 - val_acc: 0.5427 - val_f1_score: 0.5070\n",
      "Epoch 8/300\n",
      "28/28 - 1s - loss: 0.6927 - acc: 0.5326 - f1_score: 0.4222 - val_loss: 0.6916 - val_acc: 0.5726 - val_f1_score: 0.4675\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6925 - acc: 0.5320 - f1_score: 0.3748 - val_loss: 0.6907 - val_acc: 0.6111 - val_f1_score: 0.4171\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6924 - acc: 0.5333 - f1_score: 0.3615 - val_loss: 0.6898 - val_acc: 0.6624 - val_f1_score: 0.4103\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6923 - acc: 0.5326 - f1_score: 0.3550 - val_loss: 0.6890 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.5315 - f1_score: 0.3516 - val_loss: 0.6883 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6921 - acc: 0.5309 - f1_score: 0.3500 - val_loss: 0.6876 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.5308 - f1_score: 0.3497 - val_loss: 0.6869 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.5308 - f1_score: 0.3491 - val_loss: 0.6863 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6918 - acc: 0.5309 - f1_score: 0.3492 - val_loss: 0.6857 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6917 - acc: 0.5309 - f1_score: 0.3492 - val_loss: 0.6852 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.5305 - f1_score: 0.3482 - val_loss: 0.6847 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.5303 - f1_score: 0.3479 - val_loss: 0.6842 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5305 - f1_score: 0.3480 - val_loss: 0.6837 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6915 - acc: 0.5303 - f1_score: 0.3476 - val_loss: 0.6833 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.5303 - f1_score: 0.3476 - val_loss: 0.6829 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.5302 - f1_score: 0.3473 - val_loss: 0.6825 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5301 - f1_score: 0.3470 - val_loss: 0.6822 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.5301 - f1_score: 0.3470 - val_loss: 0.6818 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5302 - f1_score: 0.3470 - val_loss: 0.6815 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6912 - acc: 0.5302 - f1_score: 0.3470 - val_loss: 0.6812 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.34      0.37        93\n",
      "           1       0.61      0.67      0.64       141\n",
      "\n",
      "    accuracy                           0.54       234\n",
      "   macro avg       0.51      0.51      0.51       234\n",
      "weighted avg       0.53      0.54      0.53       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-132454\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Ablation ecg\n",
      "Average Accuracy:  0.662348411840749\n",
      "F1 score for Baseline:  0.4515888174413253\n",
      "F1 score for Stress:  0.5684983944356026\n",
      "Macro F1:  0.5100436059384639\n",
      "Weighted F1:  0.6713110636517347\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Training for Modality eda\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-134518\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_weights\n",
      "(None, 2560, 3)\n",
      "Testing on 1105\n",
      "Epoch 1/300\n",
      "27/27 - 15s - loss: 0.6922 - acc: 0.4692 - f1_score: 0.3198 - val_loss: 0.6895 - val_acc: 0.4167 - val_f1_score: 0.2941\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6919 - acc: 0.4731 - f1_score: 0.3282 - val_loss: 0.6885 - val_acc: 0.4252 - val_f1_score: 0.3132\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6916 - acc: 0.4807 - f1_score: 0.3532 - val_loss: 0.6876 - val_acc: 0.4338 - val_f1_score: 0.3465\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6913 - acc: 0.4896 - f1_score: 0.3878 - val_loss: 0.6867 - val_acc: 0.4594 - val_f1_score: 0.4179\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.4976 - f1_score: 0.4204 - val_loss: 0.6858 - val_acc: 0.4679 - val_f1_score: 0.4489\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6908 - acc: 0.5138 - f1_score: 0.4639 - val_loss: 0.6849 - val_acc: 0.4936 - val_f1_score: 0.4860\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6906 - acc: 0.5308 - f1_score: 0.4995 - val_loss: 0.6841 - val_acc: 0.5385 - val_f1_score: 0.5372\n",
      "Epoch 8/300\n",
      "27/27 - 1s - loss: 0.6903 - acc: 0.5454 - f1_score: 0.5303 - val_loss: 0.6832 - val_acc: 0.5726 - val_f1_score: 0.5726\n",
      "Epoch 9/300\n",
      "27/27 - 1s - loss: 0.6901 - acc: 0.5515 - f1_score: 0.5436 - val_loss: 0.6824 - val_acc: 0.6004 - val_f1_score: 0.5995\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5521 - f1_score: 0.5492 - val_loss: 0.6816 - val_acc: 0.6325 - val_f1_score: 0.6295\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6897 - acc: 0.5551 - f1_score: 0.5540 - val_loss: 0.6808 - val_acc: 0.6453 - val_f1_score: 0.6401\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6894 - acc: 0.5558 - f1_score: 0.5557 - val_loss: 0.6800 - val_acc: 0.6389 - val_f1_score: 0.6294\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6892 - acc: 0.5563 - f1_score: 0.5563 - val_loss: 0.6792 - val_acc: 0.6346 - val_f1_score: 0.6239\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6890 - acc: 0.5564 - f1_score: 0.5561 - val_loss: 0.6785 - val_acc: 0.6410 - val_f1_score: 0.6291\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6888 - acc: 0.5553 - f1_score: 0.5545 - val_loss: 0.6777 - val_acc: 0.6496 - val_f1_score: 0.6348\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6886 - acc: 0.5534 - f1_score: 0.5517 - val_loss: 0.6770 - val_acc: 0.6538 - val_f1_score: 0.6386\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6884 - acc: 0.5544 - f1_score: 0.5521 - val_loss: 0.6762 - val_acc: 0.6496 - val_f1_score: 0.6328\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6883 - acc: 0.5560 - f1_score: 0.5530 - val_loss: 0.6755 - val_acc: 0.6517 - val_f1_score: 0.6332\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6881 - acc: 0.5561 - f1_score: 0.5527 - val_loss: 0.6749 - val_acc: 0.6538 - val_f1_score: 0.6344\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6879 - acc: 0.5551 - f1_score: 0.5511 - val_loss: 0.6743 - val_acc: 0.6624 - val_f1_score: 0.6419\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5557 - f1_score: 0.5511 - val_loss: 0.6736 - val_acc: 0.6624 - val_f1_score: 0.6411\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6876 - acc: 0.5563 - f1_score: 0.5513 - val_loss: 0.6730 - val_acc: 0.6603 - val_f1_score: 0.6384\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6874 - acc: 0.5566 - f1_score: 0.5510 - val_loss: 0.6724 - val_acc: 0.6645 - val_f1_score: 0.6422\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6873 - acc: 0.5569 - f1_score: 0.5504 - val_loss: 0.6718 - val_acc: 0.6667 - val_f1_score: 0.6432\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5570 - f1_score: 0.5504 - val_loss: 0.6713 - val_acc: 0.6667 - val_f1_score: 0.6432\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6870 - acc: 0.5573 - f1_score: 0.5503 - val_loss: 0.6708 - val_acc: 0.6667 - val_f1_score: 0.6432\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6868 - acc: 0.5573 - f1_score: 0.5494 - val_loss: 0.6703 - val_acc: 0.6667 - val_f1_score: 0.6424\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6867 - acc: 0.5589 - f1_score: 0.5505 - val_loss: 0.6698 - val_acc: 0.6709 - val_f1_score: 0.6461\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5582 - f1_score: 0.5494 - val_loss: 0.6693 - val_acc: 0.6709 - val_f1_score: 0.6461\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6864 - acc: 0.5577 - f1_score: 0.5485 - val_loss: 0.6688 - val_acc: 0.6709 - val_f1_score: 0.6461\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6863 - acc: 0.5569 - f1_score: 0.5472 - val_loss: 0.6684 - val_acc: 0.6731 - val_f1_score: 0.6480\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6862 - acc: 0.5566 - f1_score: 0.5467 - val_loss: 0.6680 - val_acc: 0.6774 - val_f1_score: 0.6517\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6861 - acc: 0.5564 - f1_score: 0.5459 - val_loss: 0.6676 - val_acc: 0.6795 - val_f1_score: 0.6536\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6859 - acc: 0.5560 - f1_score: 0.5452 - val_loss: 0.6672 - val_acc: 0.6795 - val_f1_score: 0.6536\n",
      "Epoch 35/300\n",
      "27/27 - 2s - loss: 0.6858 - acc: 0.5571 - f1_score: 0.5458 - val_loss: 0.6668 - val_acc: 0.6774 - val_f1_score: 0.6508\n",
      "Epoch 36/300\n",
      "27/27 - 2s - loss: 0.6857 - acc: 0.5570 - f1_score: 0.5454 - val_loss: 0.6664 - val_acc: 0.6795 - val_f1_score: 0.6527\n",
      "Epoch 37/300\n",
      "27/27 - 2s - loss: 0.6856 - acc: 0.5569 - f1_score: 0.5448 - val_loss: 0.6660 - val_acc: 0.6774 - val_f1_score: 0.6499\n",
      "Epoch 38/300\n",
      "27/27 - 2s - loss: 0.6855 - acc: 0.5561 - f1_score: 0.5435 - val_loss: 0.6657 - val_acc: 0.6774 - val_f1_score: 0.6499\n",
      "Epoch 39/300\n",
      "27/27 - 2s - loss: 0.6854 - acc: 0.5560 - f1_score: 0.5428 - val_loss: 0.6652 - val_acc: 0.6774 - val_f1_score: 0.6499\n",
      "Epoch 40/300\n",
      "27/27 - 2s - loss: 0.6853 - acc: 0.5560 - f1_score: 0.5427 - val_loss: 0.6649 - val_acc: 0.6795 - val_f1_score: 0.6518\n",
      "Epoch 41/300\n",
      "27/27 - 2s - loss: 0.6852 - acc: 0.5560 - f1_score: 0.5425 - val_loss: 0.6645 - val_acc: 0.6816 - val_f1_score: 0.6537\n",
      "Epoch 42/300\n",
      "27/27 - 2s - loss: 0.6850 - acc: 0.5571 - f1_score: 0.5428 - val_loss: 0.6642 - val_acc: 0.6816 - val_f1_score: 0.6537\n",
      "Epoch 43/300\n",
      "27/27 - 2s - loss: 0.6849 - acc: 0.5573 - f1_score: 0.5431 - val_loss: 0.6638 - val_acc: 0.6838 - val_f1_score: 0.6555\n",
      "Epoch 44/300\n",
      "27/27 - 2s - loss: 0.6848 - acc: 0.5576 - f1_score: 0.5432 - val_loss: 0.6635 - val_acc: 0.6838 - val_f1_score: 0.6555\n",
      "Epoch 45/300\n",
      "27/27 - 2s - loss: 0.6847 - acc: 0.5579 - f1_score: 0.5428 - val_loss: 0.6632 - val_acc: 0.6880 - val_f1_score: 0.6593\n",
      "Epoch 46/300\n",
      "27/27 - 2s - loss: 0.6846 - acc: 0.5587 - f1_score: 0.5434 - val_loss: 0.6629 - val_acc: 0.6880 - val_f1_score: 0.6593\n",
      "Epoch 47/300\n",
      "27/27 - 2s - loss: 0.6845 - acc: 0.5590 - f1_score: 0.5435 - val_loss: 0.6626 - val_acc: 0.6902 - val_f1_score: 0.6611\n",
      "Epoch 48/300\n",
      "27/27 - 2s - loss: 0.6844 - acc: 0.5587 - f1_score: 0.5429 - val_loss: 0.6623 - val_acc: 0.6902 - val_f1_score: 0.6611\n",
      "Epoch 49/300\n",
      "27/27 - 2s - loss: 0.6843 - acc: 0.5603 - f1_score: 0.5439 - val_loss: 0.6620 - val_acc: 0.6902 - val_f1_score: 0.6602\n",
      "Epoch 50/300\n",
      "27/27 - 2s - loss: 0.6842 - acc: 0.5596 - f1_score: 0.5434 - val_loss: 0.6617 - val_acc: 0.6902 - val_f1_score: 0.6602\n",
      "Epoch 51/300\n",
      "27/27 - 2s - loss: 0.6841 - acc: 0.5609 - f1_score: 0.5439 - val_loss: 0.6614 - val_acc: 0.6902 - val_f1_score: 0.6602\n",
      "Epoch 52/300\n",
      "27/27 - 2s - loss: 0.6840 - acc: 0.5609 - f1_score: 0.5436 - val_loss: 0.6611 - val_acc: 0.6902 - val_f1_score: 0.6602\n",
      "Epoch 53/300\n",
      "27/27 - 2s - loss: 0.6839 - acc: 0.5608 - f1_score: 0.5434 - val_loss: 0.6608 - val_acc: 0.6923 - val_f1_score: 0.6621\n",
      "Epoch 54/300\n",
      "27/27 - 2s - loss: 0.6838 - acc: 0.5622 - f1_score: 0.5446 - val_loss: 0.6605 - val_acc: 0.6923 - val_f1_score: 0.6621\n",
      "Epoch 55/300\n",
      "27/27 - 2s - loss: 0.6837 - acc: 0.5614 - f1_score: 0.5436 - val_loss: 0.6603 - val_acc: 0.6923 - val_f1_score: 0.6621\n",
      "Epoch 56/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5615 - f1_score: 0.5432 - val_loss: 0.6599 - val_acc: 0.6923 - val_f1_score: 0.6621\n",
      "Epoch 57/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5621 - f1_score: 0.5438 - val_loss: 0.6597 - val_acc: 0.6923 - val_f1_score: 0.6621\n",
      "Epoch 58/300\n",
      "27/27 - 2s - loss: 0.6835 - acc: 0.5629 - f1_score: 0.5442 - val_loss: 0.6593 - val_acc: 0.6944 - val_f1_score: 0.6639\n",
      "Epoch 59/300\n",
      "27/27 - 2s - loss: 0.6834 - acc: 0.5615 - f1_score: 0.5430 - val_loss: 0.6591 - val_acc: 0.6944 - val_f1_score: 0.6639\n",
      "Epoch 60/300\n",
      "27/27 - 2s - loss: 0.6833 - acc: 0.5615 - f1_score: 0.5426 - val_loss: 0.6588 - val_acc: 0.6944 - val_f1_score: 0.6639\n",
      "Epoch 61/300\n",
      "27/27 - 2s - loss: 0.6832 - acc: 0.5616 - f1_score: 0.5431 - val_loss: 0.6587 - val_acc: 0.6944 - val_f1_score: 0.6639\n",
      "Epoch 62/300\n",
      "27/27 - 2s - loss: 0.6831 - acc: 0.5616 - f1_score: 0.5426 - val_loss: 0.6585 - val_acc: 0.6966 - val_f1_score: 0.6658\n",
      "Epoch 63/300\n",
      "27/27 - 2s - loss: 0.6830 - acc: 0.5627 - f1_score: 0.5427 - val_loss: 0.6582 - val_acc: 0.6966 - val_f1_score: 0.6658\n",
      "Epoch 64/300\n",
      "27/27 - 2s - loss: 0.6829 - acc: 0.5634 - f1_score: 0.5432 - val_loss: 0.6578 - val_acc: 0.6966 - val_f1_score: 0.6658\n",
      "Epoch 65/300\n",
      "27/27 - 2s - loss: 0.6828 - acc: 0.5628 - f1_score: 0.5428 - val_loss: 0.6577 - val_acc: 0.6987 - val_f1_score: 0.6677\n",
      "Epoch 66/300\n",
      "27/27 - 2s - loss: 0.6827 - acc: 0.5627 - f1_score: 0.5422 - val_loss: 0.6574 - val_acc: 0.6987 - val_f1_score: 0.6677\n",
      "Epoch 67/300\n",
      "27/27 - 2s - loss: 0.6827 - acc: 0.5631 - f1_score: 0.5428 - val_loss: 0.6572 - val_acc: 0.6987 - val_f1_score: 0.6677\n",
      "Epoch 68/300\n",
      "27/27 - 2s - loss: 0.6826 - acc: 0.5634 - f1_score: 0.5430 - val_loss: 0.6569 - val_acc: 0.6987 - val_f1_score: 0.6677\n",
      "Epoch 69/300\n",
      "27/27 - 2s - loss: 0.6825 - acc: 0.5635 - f1_score: 0.5432 - val_loss: 0.6567 - val_acc: 0.6966 - val_f1_score: 0.6649\n",
      "Epoch 70/300\n",
      "27/27 - 2s - loss: 0.6824 - acc: 0.5638 - f1_score: 0.5436 - val_loss: 0.6566 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 71/300\n",
      "27/27 - 2s - loss: 0.6823 - acc: 0.5640 - f1_score: 0.5431 - val_loss: 0.6564 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 72/300\n",
      "27/27 - 2s - loss: 0.6822 - acc: 0.5635 - f1_score: 0.5430 - val_loss: 0.6562 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 73/300\n",
      "27/27 - 2s - loss: 0.6821 - acc: 0.5637 - f1_score: 0.5426 - val_loss: 0.6560 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 74/300\n",
      "27/27 - 2s - loss: 0.6821 - acc: 0.5627 - f1_score: 0.5411 - val_loss: 0.6557 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 75/300\n",
      "27/27 - 2s - loss: 0.6820 - acc: 0.5635 - f1_score: 0.5419 - val_loss: 0.6555 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 76/300\n",
      "27/27 - 2s - loss: 0.6819 - acc: 0.5632 - f1_score: 0.5414 - val_loss: 0.6552 - val_acc: 0.6987 - val_f1_score: 0.6667\n",
      "Epoch 77/300\n",
      "27/27 - 2s - loss: 0.6818 - acc: 0.5640 - f1_score: 0.5427 - val_loss: 0.6551 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 78/300\n",
      "27/27 - 2s - loss: 0.6817 - acc: 0.5634 - f1_score: 0.5413 - val_loss: 0.6549 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 79/300\n",
      "27/27 - 2s - loss: 0.6817 - acc: 0.5641 - f1_score: 0.5420 - val_loss: 0.6547 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 80/300\n",
      "27/27 - 2s - loss: 0.6816 - acc: 0.5645 - f1_score: 0.5431 - val_loss: 0.6546 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 81/300\n",
      "27/27 - 2s - loss: 0.6815 - acc: 0.5641 - f1_score: 0.5412 - val_loss: 0.6544 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 82/300\n",
      "27/27 - 2s - loss: 0.6814 - acc: 0.5641 - f1_score: 0.5413 - val_loss: 0.6541 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 83/300\n",
      "27/27 - 2s - loss: 0.6813 - acc: 0.5638 - f1_score: 0.5412 - val_loss: 0.6539 - val_acc: 0.7009 - val_f1_score: 0.6686\n",
      "Epoch 84/300\n",
      "27/27 - 2s - loss: 0.6813 - acc: 0.5642 - f1_score: 0.5419 - val_loss: 0.6539 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 85/300\n",
      "27/27 - 2s - loss: 0.6812 - acc: 0.5654 - f1_score: 0.5420 - val_loss: 0.6536 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 86/300\n",
      "27/27 - 2s - loss: 0.6811 - acc: 0.5650 - f1_score: 0.5426 - val_loss: 0.6536 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 87/300\n",
      "27/27 - 2s - loss: 0.6810 - acc: 0.5658 - f1_score: 0.5425 - val_loss: 0.6533 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 88/300\n",
      "27/27 - 2s - loss: 0.6810 - acc: 0.5656 - f1_score: 0.5426 - val_loss: 0.6531 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 89/300\n",
      "27/27 - 2s - loss: 0.6809 - acc: 0.5664 - f1_score: 0.5431 - val_loss: 0.6529 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 90/300\n",
      "27/27 - 2s - loss: 0.6808 - acc: 0.5661 - f1_score: 0.5428 - val_loss: 0.6527 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 91/300\n",
      "27/27 - 2s - loss: 0.6807 - acc: 0.5657 - f1_score: 0.5423 - val_loss: 0.6525 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 92/300\n",
      "27/27 - 2s - loss: 0.6807 - acc: 0.5663 - f1_score: 0.5430 - val_loss: 0.6524 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 93/300\n",
      "27/27 - 2s - loss: 0.6806 - acc: 0.5667 - f1_score: 0.5434 - val_loss: 0.6523 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 94/300\n",
      "27/27 - 2s - loss: 0.6805 - acc: 0.5670 - f1_score: 0.5434 - val_loss: 0.6522 - val_acc: 0.7030 - val_f1_score: 0.6705\n",
      "Epoch 95/300\n",
      "27/27 - 2s - loss: 0.6804 - acc: 0.5660 - f1_score: 0.5425 - val_loss: 0.6521 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 96/300\n",
      "27/27 - 2s - loss: 0.6804 - acc: 0.5670 - f1_score: 0.5433 - val_loss: 0.6520 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 97/300\n",
      "27/27 - 2s - loss: 0.6803 - acc: 0.5673 - f1_score: 0.5437 - val_loss: 0.6519 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 98/300\n",
      "27/27 - 2s - loss: 0.6802 - acc: 0.5669 - f1_score: 0.5431 - val_loss: 0.6518 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 99/300\n",
      "27/27 - 2s - loss: 0.6801 - acc: 0.5664 - f1_score: 0.5417 - val_loss: 0.6515 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 100/300\n",
      "27/27 - 2s - loss: 0.6801 - acc: 0.5660 - f1_score: 0.5413 - val_loss: 0.6513 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 101/300\n",
      "27/27 - 2s - loss: 0.6800 - acc: 0.5671 - f1_score: 0.5434 - val_loss: 0.6513 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 102/300\n",
      "27/27 - 2s - loss: 0.6799 - acc: 0.5666 - f1_score: 0.5428 - val_loss: 0.6513 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 103/300\n",
      "27/27 - 2s - loss: 0.6799 - acc: 0.5669 - f1_score: 0.5421 - val_loss: 0.6510 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Epoch 104/300\n",
      "27/27 - 2s - loss: 0.6798 - acc: 0.5667 - f1_score: 0.5428 - val_loss: 0.6510 - val_acc: 0.7009 - val_f1_score: 0.6676\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00104: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.72      0.57       126\n",
      "           1       0.87      0.70      0.77       342\n",
      "\n",
      "    accuracy                           0.70       468\n",
      "   macro avg       0.67      0.71      0.67       468\n",
      "weighted avg       0.76      0.70      0.72       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1106\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6910 - acc: 0.4915 - f1_score: 0.3295 - val_loss: 0.7071 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.4924 - f1_score: 0.3314 - val_loss: 0.7083 - val_acc: 0.0855 - val_f1_score: 0.0794\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6900 - acc: 0.4949 - f1_score: 0.3375 - val_loss: 0.7096 - val_acc: 0.0855 - val_f1_score: 0.0794\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6895 - acc: 0.4998 - f1_score: 0.3502 - val_loss: 0.7109 - val_acc: 0.0919 - val_f1_score: 0.0868\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6891 - acc: 0.5025 - f1_score: 0.3623 - val_loss: 0.7122 - val_acc: 0.1004 - val_f1_score: 0.0965\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6886 - acc: 0.5069 - f1_score: 0.3820 - val_loss: 0.7135 - val_acc: 0.1090 - val_f1_score: 0.1064\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6882 - acc: 0.5102 - f1_score: 0.3991 - val_loss: 0.7148 - val_acc: 0.1175 - val_f1_score: 0.1161\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5160 - f1_score: 0.4249 - val_loss: 0.7161 - val_acc: 0.1560 - val_f1_score: 0.1559\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6875 - acc: 0.5211 - f1_score: 0.4438 - val_loss: 0.7174 - val_acc: 0.1688 - val_f1_score: 0.1679\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5260 - f1_score: 0.4613 - val_loss: 0.7186 - val_acc: 0.1902 - val_f1_score: 0.1872\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6868 - acc: 0.5359 - f1_score: 0.4830 - val_loss: 0.7197 - val_acc: 0.2329 - val_f1_score: 0.2222\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6865 - acc: 0.5438 - f1_score: 0.5037 - val_loss: 0.7207 - val_acc: 0.2650 - val_f1_score: 0.2471\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6862 - acc: 0.5502 - f1_score: 0.5173 - val_loss: 0.7215 - val_acc: 0.3056 - val_f1_score: 0.2783\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6860 - acc: 0.5574 - f1_score: 0.5334 - val_loss: 0.7221 - val_acc: 0.3462 - val_f1_score: 0.3079\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6858 - acc: 0.5645 - f1_score: 0.5471 - val_loss: 0.7227 - val_acc: 0.3718 - val_f1_score: 0.3260\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6856 - acc: 0.5632 - f1_score: 0.5519 - val_loss: 0.7232 - val_acc: 0.4038 - val_f1_score: 0.3480\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6854 - acc: 0.5609 - f1_score: 0.5533 - val_loss: 0.7235 - val_acc: 0.4252 - val_f1_score: 0.3623\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6852 - acc: 0.5616 - f1_score: 0.5564 - val_loss: 0.7236 - val_acc: 0.4679 - val_f1_score: 0.3903\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6850 - acc: 0.5609 - f1_score: 0.5570 - val_loss: 0.7235 - val_acc: 0.4893 - val_f1_score: 0.4041\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6849 - acc: 0.5634 - f1_score: 0.5612 - val_loss: 0.7234 - val_acc: 0.4979 - val_f1_score: 0.4095\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6847 - acc: 0.5602 - f1_score: 0.5588 - val_loss: 0.7232 - val_acc: 0.5171 - val_f1_score: 0.4218\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6846 - acc: 0.5595 - f1_score: 0.5587 - val_loss: 0.7229 - val_acc: 0.5299 - val_f1_score: 0.4299\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6845 - acc: 0.5621 - f1_score: 0.5618 - val_loss: 0.7226 - val_acc: 0.5406 - val_f1_score: 0.4367\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6843 - acc: 0.5599 - f1_score: 0.5598 - val_loss: 0.7225 - val_acc: 0.5449 - val_f1_score: 0.4394\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6842 - acc: 0.5579 - f1_score: 0.5579 - val_loss: 0.7221 - val_acc: 0.5556 - val_f1_score: 0.4462\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6841 - acc: 0.5600 - f1_score: 0.5600 - val_loss: 0.7216 - val_acc: 0.5620 - val_f1_score: 0.4502\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6840 - acc: 0.5585 - f1_score: 0.5584 - val_loss: 0.7212 - val_acc: 0.5641 - val_f1_score: 0.4516\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6839 - acc: 0.5593 - f1_score: 0.5591 - val_loss: 0.7209 - val_acc: 0.5705 - val_f1_score: 0.4556\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6837 - acc: 0.5612 - f1_score: 0.5609 - val_loss: 0.7207 - val_acc: 0.5726 - val_f1_score: 0.4570\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5611 - f1_score: 0.5606 - val_loss: 0.7204 - val_acc: 0.5812 - val_f1_score: 0.4624\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6835 - acc: 0.5621 - f1_score: 0.5616 - val_loss: 0.7200 - val_acc: 0.5876 - val_f1_score: 0.4665\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6834 - acc: 0.5625 - f1_score: 0.5619 - val_loss: 0.7194 - val_acc: 0.5897 - val_f1_score: 0.4678\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6833 - acc: 0.5614 - f1_score: 0.5605 - val_loss: 0.7191 - val_acc: 0.5919 - val_f1_score: 0.4692\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6832 - acc: 0.5609 - f1_score: 0.5600 - val_loss: 0.7185 - val_acc: 0.5919 - val_f1_score: 0.4692\n",
      "Epoch 35/300\n",
      "27/27 - 2s - loss: 0.6831 - acc: 0.5600 - f1_score: 0.5589 - val_loss: 0.7181 - val_acc: 0.5940 - val_f1_score: 0.4706\n",
      "Epoch 36/300\n",
      "27/27 - 2s - loss: 0.6830 - acc: 0.5596 - f1_score: 0.5584 - val_loss: 0.7175 - val_acc: 0.5962 - val_f1_score: 0.4719\n",
      "Epoch 37/300\n",
      "27/27 - 2s - loss: 0.6829 - acc: 0.5596 - f1_score: 0.5582 - val_loss: 0.7171 - val_acc: 0.6004 - val_f1_score: 0.4746\n",
      "Epoch 38/300\n",
      "27/27 - 2s - loss: 0.6829 - acc: 0.5600 - f1_score: 0.5584 - val_loss: 0.7167 - val_acc: 0.6047 - val_f1_score: 0.4774\n",
      "Epoch 39/300\n",
      "27/27 - 2s - loss: 0.6828 - acc: 0.5600 - f1_score: 0.5582 - val_loss: 0.7165 - val_acc: 0.6068 - val_f1_score: 0.4787\n",
      "Epoch 40/300\n",
      "27/27 - 2s - loss: 0.6827 - acc: 0.5606 - f1_score: 0.5586 - val_loss: 0.7162 - val_acc: 0.6132 - val_f1_score: 0.4829\n",
      "Epoch 41/300\n",
      "27/27 - 2s - loss: 0.6826 - acc: 0.5603 - f1_score: 0.5582 - val_loss: 0.7157 - val_acc: 0.6132 - val_f1_score: 0.4829\n",
      "Epoch 42/300\n",
      "27/27 - 2s - loss: 0.6825 - acc: 0.5596 - f1_score: 0.5573 - val_loss: 0.7153 - val_acc: 0.6132 - val_f1_score: 0.4829\n",
      "Epoch 43/300\n",
      "27/27 - 2s - loss: 0.6824 - acc: 0.5602 - f1_score: 0.5578 - val_loss: 0.7149 - val_acc: 0.6132 - val_f1_score: 0.4829\n",
      "Epoch 44/300\n",
      "27/27 - 2s - loss: 0.6823 - acc: 0.5603 - f1_score: 0.5578 - val_loss: 0.7145 - val_acc: 0.6154 - val_f1_score: 0.4842\n",
      "Epoch 45/300\n",
      "27/27 - 2s - loss: 0.6822 - acc: 0.5602 - f1_score: 0.5575 - val_loss: 0.7141 - val_acc: 0.6154 - val_f1_score: 0.4842\n",
      "Epoch 46/300\n",
      "27/27 - 2s - loss: 0.6821 - acc: 0.5602 - f1_score: 0.5574 - val_loss: 0.7137 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 47/300\n",
      "27/27 - 2s - loss: 0.6821 - acc: 0.5603 - f1_score: 0.5575 - val_loss: 0.7132 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 48/300\n",
      "27/27 - 2s - loss: 0.6820 - acc: 0.5608 - f1_score: 0.5577 - val_loss: 0.7128 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 49/300\n",
      "27/27 - 2s - loss: 0.6819 - acc: 0.5596 - f1_score: 0.5563 - val_loss: 0.7124 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 50/300\n",
      "27/27 - 2s - loss: 0.6818 - acc: 0.5596 - f1_score: 0.5563 - val_loss: 0.7119 - val_acc: 0.6197 - val_f1_score: 0.4870\n",
      "Epoch 51/300\n",
      "27/27 - 2s - loss: 0.6817 - acc: 0.5593 - f1_score: 0.5558 - val_loss: 0.7118 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 52/300\n",
      "27/27 - 2s - loss: 0.6816 - acc: 0.5611 - f1_score: 0.5573 - val_loss: 0.7118 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 53/300\n",
      "27/27 - 2s - loss: 0.6816 - acc: 0.5606 - f1_score: 0.5570 - val_loss: 0.7114 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 54/300\n",
      "27/27 - 2s - loss: 0.6815 - acc: 0.5611 - f1_score: 0.5573 - val_loss: 0.7110 - val_acc: 0.6218 - val_f1_score: 0.4884\n",
      "Epoch 55/300\n",
      "27/27 - 2s - loss: 0.6814 - acc: 0.5614 - f1_score: 0.5574 - val_loss: 0.7107 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 56/300\n",
      "27/27 - 2s - loss: 0.6813 - acc: 0.5619 - f1_score: 0.5578 - val_loss: 0.7107 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 57/300\n",
      "27/27 - 2s - loss: 0.6813 - acc: 0.5619 - f1_score: 0.5578 - val_loss: 0.7104 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 58/300\n",
      "27/27 - 2s - loss: 0.6812 - acc: 0.5621 - f1_score: 0.5577 - val_loss: 0.7106 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 59/300\n",
      "27/27 - 2s - loss: 0.6811 - acc: 0.5624 - f1_score: 0.5580 - val_loss: 0.7103 - val_acc: 0.6239 - val_f1_score: 0.4897\n",
      "Epoch 60/300\n",
      "27/27 - 2s - loss: 0.6810 - acc: 0.5624 - f1_score: 0.5579 - val_loss: 0.7101 - val_acc: 0.6261 - val_f1_score: 0.4911\n",
      "Epoch 61/300\n",
      "27/27 - 2s - loss: 0.6809 - acc: 0.5622 - f1_score: 0.5578 - val_loss: 0.7092 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 62/300\n",
      "27/27 - 2s - loss: 0.6809 - acc: 0.5622 - f1_score: 0.5577 - val_loss: 0.7088 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 63/300\n",
      "27/27 - 2s - loss: 0.6808 - acc: 0.5627 - f1_score: 0.5579 - val_loss: 0.7089 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 64/300\n",
      "27/27 - 2s - loss: 0.6807 - acc: 0.5629 - f1_score: 0.5580 - val_loss: 0.7091 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 65/300\n",
      "27/27 - 2s - loss: 0.6806 - acc: 0.5629 - f1_score: 0.5582 - val_loss: 0.7086 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 66/300\n",
      "27/27 - 2s - loss: 0.6806 - acc: 0.5627 - f1_score: 0.5577 - val_loss: 0.7085 - val_acc: 0.6346 - val_f1_score: 0.4967\n",
      "Epoch 67/300\n",
      "27/27 - 2s - loss: 0.6805 - acc: 0.5625 - f1_score: 0.5575 - val_loss: 0.7085 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 68/300\n",
      "27/27 - 2s - loss: 0.6804 - acc: 0.5629 - f1_score: 0.5580 - val_loss: 0.7083 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 69/300\n",
      "27/27 - 2s - loss: 0.6803 - acc: 0.5628 - f1_score: 0.5578 - val_loss: 0.7079 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 70/300\n",
      "27/27 - 2s - loss: 0.6803 - acc: 0.5622 - f1_score: 0.5572 - val_loss: 0.7073 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 71/300\n",
      "27/27 - 2s - loss: 0.6802 - acc: 0.5627 - f1_score: 0.5575 - val_loss: 0.7070 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 72/300\n",
      "27/27 - 2s - loss: 0.6801 - acc: 0.5628 - f1_score: 0.5576 - val_loss: 0.7067 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 73/300\n",
      "27/27 - 2s - loss: 0.6800 - acc: 0.5631 - f1_score: 0.5577 - val_loss: 0.7065 - val_acc: 0.6368 - val_f1_score: 0.4981\n",
      "Epoch 74/300\n",
      "27/27 - 2s - loss: 0.6800 - acc: 0.5624 - f1_score: 0.5568 - val_loss: 0.7064 - val_acc: 0.6389 - val_f1_score: 0.4995\n",
      "Epoch 75/300\n",
      "27/27 - 2s - loss: 0.6799 - acc: 0.5621 - f1_score: 0.5564 - val_loss: 0.7062 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 76/300\n",
      "27/27 - 2s - loss: 0.6798 - acc: 0.5621 - f1_score: 0.5563 - val_loss: 0.7061 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 77/300\n",
      "27/27 - 2s - loss: 0.6797 - acc: 0.5618 - f1_score: 0.5561 - val_loss: 0.7056 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 78/300\n",
      "27/27 - 2s - loss: 0.6797 - acc: 0.5618 - f1_score: 0.5558 - val_loss: 0.7055 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 79/300\n",
      "27/27 - 2s - loss: 0.6796 - acc: 0.5621 - f1_score: 0.5562 - val_loss: 0.7055 - val_acc: 0.6410 - val_f1_score: 0.5009\n",
      "Epoch 80/300\n",
      "27/27 - 2s - loss: 0.6795 - acc: 0.5616 - f1_score: 0.5558 - val_loss: 0.7049 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 81/300\n",
      "27/27 - 2s - loss: 0.6795 - acc: 0.5621 - f1_score: 0.5558 - val_loss: 0.7051 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 82/300\n",
      "27/27 - 2s - loss: 0.6794 - acc: 0.5621 - f1_score: 0.5560 - val_loss: 0.7049 - val_acc: 0.6432 - val_f1_score: 0.5023\n",
      "Epoch 83/300\n",
      "27/27 - 2s - loss: 0.6793 - acc: 0.5628 - f1_score: 0.5566 - val_loss: 0.7048 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 84/300\n",
      "27/27 - 2s - loss: 0.6793 - acc: 0.5628 - f1_score: 0.5568 - val_loss: 0.7039 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 85/300\n",
      "27/27 - 2s - loss: 0.6792 - acc: 0.5635 - f1_score: 0.5570 - val_loss: 0.7042 - val_acc: 0.6453 - val_f1_score: 0.5037\n",
      "Epoch 86/300\n",
      "27/27 - 2s - loss: 0.6791 - acc: 0.5638 - f1_score: 0.5577 - val_loss: 0.7034 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 87/300\n",
      "27/27 - 2s - loss: 0.6791 - acc: 0.5635 - f1_score: 0.5570 - val_loss: 0.7036 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 88/300\n",
      "27/27 - 2s - loss: 0.6790 - acc: 0.5638 - f1_score: 0.5574 - val_loss: 0.7037 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 89/300\n",
      "27/27 - 2s - loss: 0.6789 - acc: 0.5638 - f1_score: 0.5572 - val_loss: 0.7036 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 90/300\n",
      "27/27 - 2s - loss: 0.6788 - acc: 0.5645 - f1_score: 0.5578 - val_loss: 0.7036 - val_acc: 0.6474 - val_f1_score: 0.5051\n",
      "Epoch 91/300\n",
      "27/27 - 2s - loss: 0.6788 - acc: 0.5640 - f1_score: 0.5574 - val_loss: 0.7033 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 92/300\n",
      "27/27 - 2s - loss: 0.6787 - acc: 0.5641 - f1_score: 0.5575 - val_loss: 0.7030 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 93/300\n",
      "27/27 - 2s - loss: 0.6786 - acc: 0.5641 - f1_score: 0.5575 - val_loss: 0.7027 - val_acc: 0.6496 - val_f1_score: 0.5065\n",
      "Epoch 94/300\n",
      "27/27 - 2s - loss: 0.6786 - acc: 0.5645 - f1_score: 0.5578 - val_loss: 0.7027 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 95/300\n",
      "27/27 - 2s - loss: 0.6785 - acc: 0.5650 - f1_score: 0.5583 - val_loss: 0.7025 - val_acc: 0.6517 - val_f1_score: 0.5079\n",
      "Epoch 96/300\n",
      "27/27 - 2s - loss: 0.6785 - acc: 0.5653 - f1_score: 0.5585 - val_loss: 0.7022 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 97/300\n",
      "27/27 - 2s - loss: 0.6784 - acc: 0.5653 - f1_score: 0.5586 - val_loss: 0.7018 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 98/300\n",
      "27/27 - 2s - loss: 0.6783 - acc: 0.5657 - f1_score: 0.5589 - val_loss: 0.7018 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 99/300\n",
      "27/27 - 2s - loss: 0.6782 - acc: 0.5656 - f1_score: 0.5585 - val_loss: 0.7020 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 100/300\n",
      "27/27 - 2s - loss: 0.6782 - acc: 0.5658 - f1_score: 0.5587 - val_loss: 0.7022 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 101/300\n",
      "27/27 - 2s - loss: 0.6781 - acc: 0.5661 - f1_score: 0.5593 - val_loss: 0.7017 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 102/300\n",
      "27/27 - 2s - loss: 0.6780 - acc: 0.5660 - f1_score: 0.5591 - val_loss: 0.7012 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 103/300\n",
      "27/27 - 2s - loss: 0.6780 - acc: 0.5667 - f1_score: 0.5596 - val_loss: 0.7018 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 104/300\n",
      "27/27 - 2s - loss: 0.6779 - acc: 0.5660 - f1_score: 0.5591 - val_loss: 0.7012 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 105/300\n",
      "27/27 - 2s - loss: 0.6779 - acc: 0.5663 - f1_score: 0.5593 - val_loss: 0.7008 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 106/300\n",
      "27/27 - 2s - loss: 0.6778 - acc: 0.5671 - f1_score: 0.5600 - val_loss: 0.7011 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 107/300\n",
      "27/27 - 2s - loss: 0.6777 - acc: 0.5670 - f1_score: 0.5600 - val_loss: 0.7004 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 108/300\n",
      "27/27 - 2s - loss: 0.6776 - acc: 0.5666 - f1_score: 0.5596 - val_loss: 0.6999 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 109/300\n",
      "27/27 - 2s - loss: 0.6776 - acc: 0.5671 - f1_score: 0.5599 - val_loss: 0.7000 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 110/300\n",
      "27/27 - 2s - loss: 0.6775 - acc: 0.5674 - f1_score: 0.5602 - val_loss: 0.7001 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 111/300\n",
      "27/27 - 2s - loss: 0.6775 - acc: 0.5679 - f1_score: 0.5603 - val_loss: 0.7008 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 112/300\n",
      "27/27 - 2s - loss: 0.6774 - acc: 0.5676 - f1_score: 0.5605 - val_loss: 0.7002 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 113/300\n",
      "27/27 - 2s - loss: 0.6773 - acc: 0.5687 - f1_score: 0.5614 - val_loss: 0.7007 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 114/300\n",
      "27/27 - 2s - loss: 0.6773 - acc: 0.5684 - f1_score: 0.5611 - val_loss: 0.7008 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 115/300\n",
      "27/27 - 2s - loss: 0.6772 - acc: 0.5698 - f1_score: 0.5621 - val_loss: 0.7016 - val_acc: 0.6560 - val_f1_score: 0.5107\n",
      "Epoch 116/300\n",
      "27/27 - 2s - loss: 0.6771 - acc: 0.5684 - f1_score: 0.5612 - val_loss: 0.7010 - val_acc: 0.6581 - val_f1_score: 0.5121\n",
      "Epoch 117/300\n",
      "27/27 - 2s - loss: 0.6771 - acc: 0.5692 - f1_score: 0.5619 - val_loss: 0.7002 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 118/300\n",
      "27/27 - 2s - loss: 0.6770 - acc: 0.5696 - f1_score: 0.5621 - val_loss: 0.7003 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 119/300\n",
      "27/27 - 2s - loss: 0.6769 - acc: 0.5698 - f1_score: 0.5622 - val_loss: 0.7002 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 120/300\n",
      "27/27 - 2s - loss: 0.6769 - acc: 0.5700 - f1_score: 0.5625 - val_loss: 0.7000 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 121/300\n",
      "27/27 - 2s - loss: 0.6768 - acc: 0.5702 - f1_score: 0.5626 - val_loss: 0.7002 - val_acc: 0.6624 - val_f1_score: 0.5150\n",
      "Epoch 122/300\n",
      "27/27 - 2s - loss: 0.6768 - acc: 0.5705 - f1_score: 0.5629 - val_loss: 0.6997 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 123/300\n",
      "27/27 - 2s - loss: 0.6767 - acc: 0.5709 - f1_score: 0.5631 - val_loss: 0.7004 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 124/300\n",
      "27/27 - 2s - loss: 0.6766 - acc: 0.5702 - f1_score: 0.5627 - val_loss: 0.6997 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 125/300\n",
      "27/27 - 2s - loss: 0.6766 - acc: 0.5703 - f1_score: 0.5629 - val_loss: 0.6984 - val_acc: 0.6667 - val_f1_score: 0.5179\n",
      "Epoch 126/300\n",
      "27/27 - 2s - loss: 0.6765 - acc: 0.5716 - f1_score: 0.5635 - val_loss: 0.6996 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 127/300\n",
      "27/27 - 2s - loss: 0.6764 - acc: 0.5699 - f1_score: 0.5623 - val_loss: 0.6993 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 128/300\n",
      "27/27 - 2s - loss: 0.6764 - acc: 0.5705 - f1_score: 0.5628 - val_loss: 0.6988 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 129/300\n",
      "27/27 - 2s - loss: 0.6763 - acc: 0.5715 - f1_score: 0.5636 - val_loss: 0.6989 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 130/300\n",
      "27/27 - 2s - loss: 0.6762 - acc: 0.5715 - f1_score: 0.5634 - val_loss: 0.6993 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 131/300\n",
      "27/27 - 2s - loss: 0.6762 - acc: 0.5709 - f1_score: 0.5630 - val_loss: 0.6994 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 132/300\n",
      "27/27 - 2s - loss: 0.6761 - acc: 0.5709 - f1_score: 0.5631 - val_loss: 0.6993 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 133/300\n",
      "27/27 - 2s - loss: 0.6761 - acc: 0.5712 - f1_score: 0.5632 - val_loss: 0.6994 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 134/300\n",
      "27/27 - 2s - loss: 0.6760 - acc: 0.5721 - f1_score: 0.5639 - val_loss: 0.7001 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 135/300\n",
      "27/27 - 2s - loss: 0.6759 - acc: 0.5713 - f1_score: 0.5635 - val_loss: 0.7003 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 136/300\n",
      "27/27 - 2s - loss: 0.6759 - acc: 0.5709 - f1_score: 0.5633 - val_loss: 0.6992 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 137/300\n",
      "27/27 - 2s - loss: 0.6758 - acc: 0.5711 - f1_score: 0.5632 - val_loss: 0.6990 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 138/300\n",
      "27/27 - 2s - loss: 0.6757 - acc: 0.5715 - f1_score: 0.5636 - val_loss: 0.6990 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 139/300\n",
      "27/27 - 2s - loss: 0.6757 - acc: 0.5718 - f1_score: 0.5638 - val_loss: 0.6994 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 140/300\n",
      "27/27 - 2s - loss: 0.6756 - acc: 0.5708 - f1_score: 0.5633 - val_loss: 0.6976 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 141/300\n",
      "27/27 - 2s - loss: 0.6756 - acc: 0.5725 - f1_score: 0.5643 - val_loss: 0.6988 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 142/300\n",
      "27/27 - 2s - loss: 0.6755 - acc: 0.5712 - f1_score: 0.5637 - val_loss: 0.6979 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 143/300\n",
      "27/27 - 2s - loss: 0.6754 - acc: 0.5722 - f1_score: 0.5640 - val_loss: 0.6989 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 144/300\n",
      "27/27 - 2s - loss: 0.6754 - acc: 0.5719 - f1_score: 0.5639 - val_loss: 0.6992 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Epoch 145/300\n",
      "27/27 - 2s - loss: 0.6753 - acc: 0.5721 - f1_score: 0.5638 - val_loss: 0.7002 - val_acc: 0.6645 - val_f1_score: 0.5164\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00145: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.15      0.25       169\n",
      "           1       0.67      0.96      0.79       299\n",
      "\n",
      "    accuracy                           0.67       468\n",
      "   macro avg       0.67      0.56      0.52       468\n",
      "weighted avg       0.67      0.67      0.59       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1175\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6925 - acc: 0.4517 - f1_score: 0.3111 - val_loss: 0.6832 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.4544 - f1_score: 0.3171 - val_loss: 0.6826 - val_acc: 0.7241 - val_f1_score: 0.4200\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6919 - acc: 0.4625 - f1_score: 0.3393 - val_loss: 0.6824 - val_acc: 0.7215 - val_f1_score: 0.4282\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.4735 - f1_score: 0.3786 - val_loss: 0.6820 - val_acc: 0.6844 - val_f1_score: 0.4143\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.4904 - f1_score: 0.4304 - val_loss: 0.6816 - val_acc: 0.6631 - val_f1_score: 0.4270\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5086 - f1_score: 0.4750 - val_loss: 0.6814 - val_acc: 0.6605 - val_f1_score: 0.4506\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.5310 - f1_score: 0.5151 - val_loss: 0.6811 - val_acc: 0.6525 - val_f1_score: 0.5164\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6905 - acc: 0.5488 - f1_score: 0.5435 - val_loss: 0.6808 - val_acc: 0.6340 - val_f1_score: 0.5419\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.5558 - f1_score: 0.5551 - val_loss: 0.6804 - val_acc: 0.6154 - val_f1_score: 0.5384\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5573 - f1_score: 0.5573 - val_loss: 0.6802 - val_acc: 0.6127 - val_f1_score: 0.5558\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5583 - f1_score: 0.5578 - val_loss: 0.6798 - val_acc: 0.5889 - val_f1_score: 0.5506\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6895 - acc: 0.5598 - f1_score: 0.5581 - val_loss: 0.6797 - val_acc: 0.5756 - val_f1_score: 0.5468\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6892 - acc: 0.5583 - f1_score: 0.5547 - val_loss: 0.6795 - val_acc: 0.5756 - val_f1_score: 0.5540\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6890 - acc: 0.5593 - f1_score: 0.5546 - val_loss: 0.6793 - val_acc: 0.5809 - val_f1_score: 0.5617\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6887 - acc: 0.5621 - f1_score: 0.5557 - val_loss: 0.6791 - val_acc: 0.5809 - val_f1_score: 0.5636\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6885 - acc: 0.5618 - f1_score: 0.5545 - val_loss: 0.6791 - val_acc: 0.5809 - val_f1_score: 0.5672\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6882 - acc: 0.5601 - f1_score: 0.5516 - val_loss: 0.6789 - val_acc: 0.5915 - val_f1_score: 0.5805\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6880 - acc: 0.5602 - f1_score: 0.5507 - val_loss: 0.6788 - val_acc: 0.5995 - val_f1_score: 0.5904\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6878 - acc: 0.5623 - f1_score: 0.5521 - val_loss: 0.6785 - val_acc: 0.6048 - val_f1_score: 0.5971\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6875 - acc: 0.5638 - f1_score: 0.5528 - val_loss: 0.6786 - val_acc: 0.5915 - val_f1_score: 0.5856\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5631 - f1_score: 0.5509 - val_loss: 0.6787 - val_acc: 0.5915 - val_f1_score: 0.5861\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6871 - acc: 0.5629 - f1_score: 0.5495 - val_loss: 0.6785 - val_acc: 0.5836 - val_f1_score: 0.5788\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5626 - f1_score: 0.5486 - val_loss: 0.6786 - val_acc: 0.5729 - val_f1_score: 0.5690\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6866 - acc: 0.5631 - f1_score: 0.5480 - val_loss: 0.6788 - val_acc: 0.5650 - val_f1_score: 0.5616\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5623 - f1_score: 0.5466 - val_loss: 0.6788 - val_acc: 0.5650 - val_f1_score: 0.5616\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6862 - acc: 0.5632 - f1_score: 0.5470 - val_loss: 0.6789 - val_acc: 0.5729 - val_f1_score: 0.5702\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5626 - f1_score: 0.5450 - val_loss: 0.6788 - val_acc: 0.5703 - val_f1_score: 0.5677\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6858 - acc: 0.5621 - f1_score: 0.5438 - val_loss: 0.6789 - val_acc: 0.5703 - val_f1_score: 0.5681\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5631 - f1_score: 0.5444 - val_loss: 0.6788 - val_acc: 0.5676 - val_f1_score: 0.5656\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5645 - f1_score: 0.5450 - val_loss: 0.6786 - val_acc: 0.5650 - val_f1_score: 0.5634\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5631 - f1_score: 0.5438 - val_loss: 0.6791 - val_acc: 0.5676 - val_f1_score: 0.5664\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5649 - f1_score: 0.5450 - val_loss: 0.6795 - val_acc: 0.5650 - val_f1_score: 0.5639\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6848 - acc: 0.5659 - f1_score: 0.5449 - val_loss: 0.6800 - val_acc: 0.5570 - val_f1_score: 0.5564\n",
      "Epoch 34/300\n",
      "28/28 - 2s - loss: 0.6846 - acc: 0.5666 - f1_score: 0.5446 - val_loss: 0.6800 - val_acc: 0.5570 - val_f1_score: 0.5564\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5665 - f1_score: 0.5443 - val_loss: 0.6804 - val_acc: 0.5597 - val_f1_score: 0.5594\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6843 - acc: 0.5675 - f1_score: 0.5448 - val_loss: 0.6806 - val_acc: 0.5570 - val_f1_score: 0.5568\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5682 - f1_score: 0.5448 - val_loss: 0.6808 - val_acc: 0.5570 - val_f1_score: 0.5568\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5679 - f1_score: 0.5444 - val_loss: 0.6810 - val_acc: 0.5597 - val_f1_score: 0.5595\n",
      "Epoch 39/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5691 - f1_score: 0.5447 - val_loss: 0.6813 - val_acc: 0.5597 - val_f1_score: 0.5595\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00039: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.90      0.65       156\n",
      "           1       0.85      0.40      0.54       221\n",
      "\n",
      "    accuracy                           0.60       377\n",
      "   macro avg       0.68      0.65      0.60       377\n",
      "weighted avg       0.71      0.60      0.59       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1194\n",
      "Epoch 1/300\n",
      "28/28 - 10s - loss: 0.6912 - acc: 0.4831 - f1_score: 0.3257 - val_loss: 0.7050 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.4835 - f1_score: 0.3267 - val_loss: 0.7063 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.4845 - f1_score: 0.3288 - val_loss: 0.7080 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.4873 - f1_score: 0.3354 - val_loss: 0.7094 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6893 - acc: 0.4906 - f1_score: 0.3457 - val_loss: 0.7109 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6890 - acc: 0.4931 - f1_score: 0.3532 - val_loss: 0.7129 - val_acc: 0.2000 - val_f1_score: 0.1667\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6886 - acc: 0.4951 - f1_score: 0.3599 - val_loss: 0.7141 - val_acc: 0.2044 - val_f1_score: 0.1727\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6882 - acc: 0.4986 - f1_score: 0.3743 - val_loss: 0.7156 - val_acc: 0.2066 - val_f1_score: 0.1772\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6879 - acc: 0.5020 - f1_score: 0.3887 - val_loss: 0.7179 - val_acc: 0.2088 - val_f1_score: 0.1802\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6876 - acc: 0.5027 - f1_score: 0.3944 - val_loss: 0.7197 - val_acc: 0.2110 - val_f1_score: 0.1831\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5052 - f1_score: 0.4023 - val_loss: 0.7206 - val_acc: 0.2154 - val_f1_score: 0.1904\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6872 - acc: 0.5097 - f1_score: 0.4151 - val_loss: 0.7219 - val_acc: 0.2110 - val_f1_score: 0.1896\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5132 - f1_score: 0.4284 - val_loss: 0.7236 - val_acc: 0.2110 - val_f1_score: 0.1919\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6868 - acc: 0.5139 - f1_score: 0.4330 - val_loss: 0.7242 - val_acc: 0.2220 - val_f1_score: 0.2068\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6866 - acc: 0.5163 - f1_score: 0.4420 - val_loss: 0.7242 - val_acc: 0.2396 - val_f1_score: 0.2291\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6865 - acc: 0.5198 - f1_score: 0.4551 - val_loss: 0.7256 - val_acc: 0.2462 - val_f1_score: 0.2369\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5215 - f1_score: 0.4621 - val_loss: 0.7261 - val_acc: 0.2725 - val_f1_score: 0.2679\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5259 - f1_score: 0.4715 - val_loss: 0.7260 - val_acc: 0.2857 - val_f1_score: 0.2834\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6862 - acc: 0.5315 - f1_score: 0.4851 - val_loss: 0.7271 - val_acc: 0.3055 - val_f1_score: 0.3047\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6861 - acc: 0.5354 - f1_score: 0.4944 - val_loss: 0.7272 - val_acc: 0.3275 - val_f1_score: 0.3275\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5442 - f1_score: 0.5105 - val_loss: 0.7273 - val_acc: 0.3451 - val_f1_score: 0.3447\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6858 - acc: 0.5499 - f1_score: 0.5242 - val_loss: 0.7280 - val_acc: 0.3692 - val_f1_score: 0.3675\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6857 - acc: 0.5552 - f1_score: 0.5336 - val_loss: 0.7286 - val_acc: 0.3934 - val_f1_score: 0.3884\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5558 - f1_score: 0.5373 - val_loss: 0.7278 - val_acc: 0.4374 - val_f1_score: 0.4225\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6855 - acc: 0.5565 - f1_score: 0.5439 - val_loss: 0.7280 - val_acc: 0.4549 - val_f1_score: 0.4322\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5557 - f1_score: 0.5456 - val_loss: 0.7289 - val_acc: 0.4725 - val_f1_score: 0.4440\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6853 - acc: 0.5564 - f1_score: 0.5477 - val_loss: 0.7286 - val_acc: 0.4769 - val_f1_score: 0.4412\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5564 - f1_score: 0.5503 - val_loss: 0.7294 - val_acc: 0.4857 - val_f1_score: 0.4466\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6851 - acc: 0.5564 - f1_score: 0.5516 - val_loss: 0.7297 - val_acc: 0.4989 - val_f1_score: 0.4551\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5581 - f1_score: 0.5546 - val_loss: 0.7302 - val_acc: 0.5033 - val_f1_score: 0.4569\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6849 - acc: 0.5583 - f1_score: 0.5557 - val_loss: 0.7299 - val_acc: 0.5143 - val_f1_score: 0.4619\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6848 - acc: 0.5580 - f1_score: 0.5565 - val_loss: 0.7298 - val_acc: 0.5165 - val_f1_score: 0.4601\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6847 - acc: 0.5574 - f1_score: 0.5567 - val_loss: 0.7283 - val_acc: 0.5275 - val_f1_score: 0.4681\n",
      "Epoch 34/300\n",
      "28/28 - 2s - loss: 0.6846 - acc: 0.5587 - f1_score: 0.5584 - val_loss: 0.7284 - val_acc: 0.5341 - val_f1_score: 0.4729\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5596 - f1_score: 0.5595 - val_loss: 0.7270 - val_acc: 0.5363 - val_f1_score: 0.4727\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5578 - f1_score: 0.5578 - val_loss: 0.7262 - val_acc: 0.5385 - val_f1_score: 0.4724\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5593 - f1_score: 0.5592 - val_loss: 0.7254 - val_acc: 0.5385 - val_f1_score: 0.4724\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5596 - f1_score: 0.5594 - val_loss: 0.7261 - val_acc: 0.5407 - val_f1_score: 0.4740\n",
      "Epoch 39/300\n",
      "28/28 - 2s - loss: 0.6843 - acc: 0.5607 - f1_score: 0.5606 - val_loss: 0.7265 - val_acc: 0.5385 - val_f1_score: 0.4705\n",
      "Epoch 40/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5609 - f1_score: 0.5607 - val_loss: 0.7271 - val_acc: 0.5363 - val_f1_score: 0.4670\n",
      "Epoch 41/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5610 - f1_score: 0.5608 - val_loss: 0.7270 - val_acc: 0.5341 - val_f1_score: 0.4635\n",
      "Epoch 42/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5610 - f1_score: 0.5607 - val_loss: 0.7257 - val_acc: 0.5363 - val_f1_score: 0.4651\n",
      "Epoch 43/300\n",
      "28/28 - 2s - loss: 0.6840 - acc: 0.5607 - f1_score: 0.5603 - val_loss: 0.7258 - val_acc: 0.5341 - val_f1_score: 0.4615\n",
      "Epoch 44/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5602 - f1_score: 0.5596 - val_loss: 0.7254 - val_acc: 0.5341 - val_f1_score: 0.4615\n",
      "Epoch 45/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5615 - f1_score: 0.5607 - val_loss: 0.7243 - val_acc: 0.5385 - val_f1_score: 0.4605\n",
      "Epoch 46/300\n",
      "28/28 - 2s - loss: 0.6838 - acc: 0.5622 - f1_score: 0.5612 - val_loss: 0.7233 - val_acc: 0.5407 - val_f1_score: 0.4598\n",
      "Epoch 47/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5625 - f1_score: 0.5613 - val_loss: 0.7233 - val_acc: 0.5407 - val_f1_score: 0.4598\n",
      "Epoch 48/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5629 - f1_score: 0.5615 - val_loss: 0.7238 - val_acc: 0.5407 - val_f1_score: 0.4598\n",
      "Epoch 49/300\n",
      "28/28 - 2s - loss: 0.6836 - acc: 0.5635 - f1_score: 0.5621 - val_loss: 0.7246 - val_acc: 0.5407 - val_f1_score: 0.4598\n",
      "Epoch 50/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5635 - f1_score: 0.5620 - val_loss: 0.7256 - val_acc: 0.5407 - val_f1_score: 0.4598\n",
      "Epoch 51/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5633 - f1_score: 0.5619 - val_loss: 0.7252 - val_acc: 0.5451 - val_f1_score: 0.4628\n",
      "Epoch 52/300\n",
      "28/28 - 2s - loss: 0.6834 - acc: 0.5633 - f1_score: 0.5617 - val_loss: 0.7254 - val_acc: 0.5451 - val_f1_score: 0.4606\n",
      "Epoch 53/300\n",
      "28/28 - 2s - loss: 0.6833 - acc: 0.5641 - f1_score: 0.5623 - val_loss: 0.7250 - val_acc: 0.5451 - val_f1_score: 0.4606\n",
      "Epoch 54/300\n",
      "28/28 - 2s - loss: 0.6832 - acc: 0.5642 - f1_score: 0.5621 - val_loss: 0.7238 - val_acc: 0.5429 - val_f1_score: 0.4569\n",
      "Epoch 55/300\n",
      "28/28 - 2s - loss: 0.6831 - acc: 0.5642 - f1_score: 0.5619 - val_loss: 0.7246 - val_acc: 0.5429 - val_f1_score: 0.4569\n",
      "Epoch 56/300\n",
      "28/28 - 2s - loss: 0.6831 - acc: 0.5639 - f1_score: 0.5615 - val_loss: 0.7241 - val_acc: 0.5451 - val_f1_score: 0.4584\n",
      "Epoch 57/300\n",
      "28/28 - 2s - loss: 0.6830 - acc: 0.5641 - f1_score: 0.5616 - val_loss: 0.7237 - val_acc: 0.5473 - val_f1_score: 0.4599\n",
      "Epoch 58/300\n",
      "28/28 - 2s - loss: 0.6830 - acc: 0.5643 - f1_score: 0.5617 - val_loss: 0.7236 - val_acc: 0.5473 - val_f1_score: 0.4576\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.21      0.29       202\n",
      "           1       0.56      0.81      0.66       253\n",
      "\n",
      "    accuracy                           0.54       455\n",
      "   macro avg       0.51      0.51      0.47       455\n",
      "weighted avg       0.52      0.54      0.49       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1337\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6927 - acc: 0.4485 - f1_score: 0.3101 - val_loss: 0.6818 - val_acc: 0.7222 - val_f1_score: 0.4194\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6924 - acc: 0.4555 - f1_score: 0.3282 - val_loss: 0.6819 - val_acc: 0.7222 - val_f1_score: 0.4194\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6920 - acc: 0.4736 - f1_score: 0.3898 - val_loss: 0.6819 - val_acc: 0.7222 - val_f1_score: 0.4408\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6917 - acc: 0.4917 - f1_score: 0.4456 - val_loss: 0.6818 - val_acc: 0.7179 - val_f1_score: 0.4870\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6914 - acc: 0.5180 - f1_score: 0.5006 - val_loss: 0.6819 - val_acc: 0.7179 - val_f1_score: 0.5607\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.5441 - f1_score: 0.5413 - val_loss: 0.6818 - val_acc: 0.7201 - val_f1_score: 0.6257\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6908 - acc: 0.5518 - f1_score: 0.5518 - val_loss: 0.6818 - val_acc: 0.7222 - val_f1_score: 0.6674\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.5543 - f1_score: 0.5528 - val_loss: 0.6817 - val_acc: 0.6923 - val_f1_score: 0.6538\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.5524 - f1_score: 0.5485 - val_loss: 0.6816 - val_acc: 0.6581 - val_f1_score: 0.6332\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5534 - f1_score: 0.5472 - val_loss: 0.6816 - val_acc: 0.6517 - val_f1_score: 0.6317\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6896 - acc: 0.5573 - f1_score: 0.5487 - val_loss: 0.6815 - val_acc: 0.6346 - val_f1_score: 0.6196\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6894 - acc: 0.5580 - f1_score: 0.5475 - val_loss: 0.6814 - val_acc: 0.6132 - val_f1_score: 0.6013\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6891 - acc: 0.5589 - f1_score: 0.5467 - val_loss: 0.6814 - val_acc: 0.6026 - val_f1_score: 0.5929\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6888 - acc: 0.5608 - f1_score: 0.5467 - val_loss: 0.6813 - val_acc: 0.6004 - val_f1_score: 0.5915\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6886 - acc: 0.5611 - f1_score: 0.5457 - val_loss: 0.6812 - val_acc: 0.5919 - val_f1_score: 0.5843\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6883 - acc: 0.5621 - f1_score: 0.5451 - val_loss: 0.6812 - val_acc: 0.5940 - val_f1_score: 0.5877\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6881 - acc: 0.5625 - f1_score: 0.5449 - val_loss: 0.6812 - val_acc: 0.5876 - val_f1_score: 0.5822\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5642 - f1_score: 0.5448 - val_loss: 0.6811 - val_acc: 0.5855 - val_f1_score: 0.5803\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6875 - acc: 0.5629 - f1_score: 0.5429 - val_loss: 0.6811 - val_acc: 0.5726 - val_f1_score: 0.5685\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6873 - acc: 0.5644 - f1_score: 0.5432 - val_loss: 0.6811 - val_acc: 0.5705 - val_f1_score: 0.5665\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5656 - f1_score: 0.5436 - val_loss: 0.6811 - val_acc: 0.5684 - val_f1_score: 0.5645\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6868 - acc: 0.5674 - f1_score: 0.5447 - val_loss: 0.6811 - val_acc: 0.5641 - val_f1_score: 0.5606\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5682 - f1_score: 0.5445 - val_loss: 0.6811 - val_acc: 0.5598 - val_f1_score: 0.5566\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6863 - acc: 0.5684 - f1_score: 0.5441 - val_loss: 0.6810 - val_acc: 0.5513 - val_f1_score: 0.5486\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6861 - acc: 0.5680 - f1_score: 0.5437 - val_loss: 0.6811 - val_acc: 0.5513 - val_f1_score: 0.5486\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6859 - acc: 0.5684 - f1_score: 0.5434 - val_loss: 0.6813 - val_acc: 0.5513 - val_f1_score: 0.5489\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6857 - acc: 0.5677 - f1_score: 0.5415 - val_loss: 0.6813 - val_acc: 0.5470 - val_f1_score: 0.5449\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       320\n",
      "           1       0.57      0.50      0.53       148\n",
      "\n",
      "    accuracy                           0.72       468\n",
      "   macro avg       0.68      0.66      0.67       468\n",
      "weighted avg       0.71      0.72      0.72       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1390\n",
      "Epoch 1/300\n",
      "27/27 - 7s - loss: 0.6921 - acc: 0.4672 - f1_score: 0.3187 - val_loss: 0.6897 - val_acc: 0.4487 - val_f1_score: 0.3162\n",
      "Epoch 2/300\n",
      "27/27 - 1s - loss: 0.6918 - acc: 0.4715 - f1_score: 0.3295 - val_loss: 0.6891 - val_acc: 0.4765 - val_f1_score: 0.3719\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6915 - acc: 0.4808 - f1_score: 0.3643 - val_loss: 0.6884 - val_acc: 0.5171 - val_f1_score: 0.4476\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6911 - acc: 0.4885 - f1_score: 0.4024 - val_loss: 0.6877 - val_acc: 0.5513 - val_f1_score: 0.5208\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6908 - acc: 0.5020 - f1_score: 0.4449 - val_loss: 0.6871 - val_acc: 0.6111 - val_f1_score: 0.6046\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.5201 - f1_score: 0.4897 - val_loss: 0.6865 - val_acc: 0.5812 - val_f1_score: 0.5797\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.5395 - f1_score: 0.5230 - val_loss: 0.6859 - val_acc: 0.5534 - val_f1_score: 0.5534\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5567 - f1_score: 0.5516 - val_loss: 0.6853 - val_acc: 0.5363 - val_f1_score: 0.5357\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6897 - acc: 0.5596 - f1_score: 0.5581 - val_loss: 0.6847 - val_acc: 0.5513 - val_f1_score: 0.5480\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6894 - acc: 0.5593 - f1_score: 0.5589 - val_loss: 0.6841 - val_acc: 0.5534 - val_f1_score: 0.5476\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6891 - acc: 0.5609 - f1_score: 0.5609 - val_loss: 0.6835 - val_acc: 0.5620 - val_f1_score: 0.5544\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6889 - acc: 0.5600 - f1_score: 0.5597 - val_loss: 0.6830 - val_acc: 0.5641 - val_f1_score: 0.5547\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6886 - acc: 0.5583 - f1_score: 0.5574 - val_loss: 0.6824 - val_acc: 0.5684 - val_f1_score: 0.5573\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6884 - acc: 0.5574 - f1_score: 0.5557 - val_loss: 0.6819 - val_acc: 0.5769 - val_f1_score: 0.5635\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6882 - acc: 0.5573 - f1_score: 0.5549 - val_loss: 0.6814 - val_acc: 0.5833 - val_f1_score: 0.5684\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6880 - acc: 0.5593 - f1_score: 0.5563 - val_loss: 0.6809 - val_acc: 0.5897 - val_f1_score: 0.5740\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5600 - f1_score: 0.5563 - val_loss: 0.6804 - val_acc: 0.5897 - val_f1_score: 0.5740\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6875 - acc: 0.5596 - f1_score: 0.5550 - val_loss: 0.6799 - val_acc: 0.5897 - val_f1_score: 0.5740\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6873 - acc: 0.5600 - f1_score: 0.5551 - val_loss: 0.6795 - val_acc: 0.5940 - val_f1_score: 0.5777\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5599 - f1_score: 0.5543 - val_loss: 0.6790 - val_acc: 0.5919 - val_f1_score: 0.5751\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6870 - acc: 0.5622 - f1_score: 0.5561 - val_loss: 0.6786 - val_acc: 0.5940 - val_f1_score: 0.5770\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6868 - acc: 0.5624 - f1_score: 0.5558 - val_loss: 0.6782 - val_acc: 0.5983 - val_f1_score: 0.5806\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5616 - f1_score: 0.5543 - val_loss: 0.6778 - val_acc: 0.5983 - val_f1_score: 0.5806\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6864 - acc: 0.5611 - f1_score: 0.5534 - val_loss: 0.6774 - val_acc: 0.6004 - val_f1_score: 0.5817\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6863 - acc: 0.5611 - f1_score: 0.5532 - val_loss: 0.6771 - val_acc: 0.6047 - val_f1_score: 0.5854\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.66       320\n",
      "           1       0.43      0.76      0.55       148\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.63      0.65      0.60       468\n",
      "weighted avg       0.71      0.61      0.62       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1400\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6909 - acc: 0.4878 - f1_score: 0.3278 - val_loss: 0.7083 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6903 - acc: 0.4891 - f1_score: 0.3309 - val_loss: 0.7104 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 3/300\n",
      "27/27 - 3s - loss: 0.6897 - acc: 0.4937 - f1_score: 0.3432 - val_loss: 0.7125 - val_acc: 0.1389 - val_f1_score: 0.1220\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6892 - acc: 0.4979 - f1_score: 0.3590 - val_loss: 0.7146 - val_acc: 0.1474 - val_f1_score: 0.1327\n",
      "Epoch 5/300\n",
      "27/27 - 1s - loss: 0.6886 - acc: 0.5017 - f1_score: 0.3775 - val_loss: 0.7169 - val_acc: 0.1731 - val_f1_score: 0.1647\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6881 - acc: 0.5089 - f1_score: 0.4067 - val_loss: 0.7192 - val_acc: 0.2009 - val_f1_score: 0.1971\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6876 - acc: 0.5143 - f1_score: 0.4258 - val_loss: 0.7215 - val_acc: 0.2286 - val_f1_score: 0.2276\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5186 - f1_score: 0.4487 - val_loss: 0.7239 - val_acc: 0.2521 - val_f1_score: 0.2521\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5289 - f1_score: 0.4754 - val_loss: 0.7263 - val_acc: 0.2735 - val_f1_score: 0.2716\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6862 - acc: 0.5361 - f1_score: 0.4928 - val_loss: 0.7284 - val_acc: 0.2885 - val_f1_score: 0.2845\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6858 - acc: 0.5482 - f1_score: 0.5180 - val_loss: 0.7304 - val_acc: 0.3205 - val_f1_score: 0.3120\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6855 - acc: 0.5564 - f1_score: 0.5349 - val_loss: 0.7322 - val_acc: 0.3462 - val_f1_score: 0.3329\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6852 - acc: 0.5632 - f1_score: 0.5492 - val_loss: 0.7338 - val_acc: 0.3889 - val_f1_score: 0.3643\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6849 - acc: 0.5625 - f1_score: 0.5536 - val_loss: 0.7354 - val_acc: 0.4209 - val_f1_score: 0.3890\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6846 - acc: 0.5586 - f1_score: 0.5532 - val_loss: 0.7365 - val_acc: 0.4444 - val_f1_score: 0.4053\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6844 - acc: 0.5609 - f1_score: 0.5577 - val_loss: 0.7375 - val_acc: 0.4722 - val_f1_score: 0.4243\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6841 - acc: 0.5632 - f1_score: 0.5615 - val_loss: 0.7382 - val_acc: 0.4915 - val_f1_score: 0.4366\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6839 - acc: 0.5609 - f1_score: 0.5603 - val_loss: 0.7389 - val_acc: 0.5043 - val_f1_score: 0.4405\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6837 - acc: 0.5621 - f1_score: 0.5618 - val_loss: 0.7392 - val_acc: 0.5128 - val_f1_score: 0.4388\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5625 - f1_score: 0.5625 - val_loss: 0.7395 - val_acc: 0.5171 - val_f1_score: 0.4310\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6834 - acc: 0.5624 - f1_score: 0.5624 - val_loss: 0.7397 - val_acc: 0.5256 - val_f1_score: 0.4344\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6832 - acc: 0.5609 - f1_score: 0.5608 - val_loss: 0.7396 - val_acc: 0.5342 - val_f1_score: 0.4399\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6831 - acc: 0.5602 - f1_score: 0.5599 - val_loss: 0.7397 - val_acc: 0.5406 - val_f1_score: 0.4441\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6829 - acc: 0.5621 - f1_score: 0.5615 - val_loss: 0.7395 - val_acc: 0.5406 - val_f1_score: 0.4441\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6828 - acc: 0.5619 - f1_score: 0.5611 - val_loss: 0.7391 - val_acc: 0.5406 - val_f1_score: 0.4441\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6827 - acc: 0.5619 - f1_score: 0.5608 - val_loss: 0.7388 - val_acc: 0.5449 - val_f1_score: 0.4444\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6825 - acc: 0.5619 - f1_score: 0.5606 - val_loss: 0.7384 - val_acc: 0.5491 - val_f1_score: 0.4421\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6824 - acc: 0.5611 - f1_score: 0.5594 - val_loss: 0.7382 - val_acc: 0.5534 - val_f1_score: 0.4448\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6823 - acc: 0.5618 - f1_score: 0.5598 - val_loss: 0.7381 - val_acc: 0.5534 - val_f1_score: 0.4448\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6821 - acc: 0.5609 - f1_score: 0.5585 - val_loss: 0.7378 - val_acc: 0.5534 - val_f1_score: 0.4422\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6820 - acc: 0.5621 - f1_score: 0.5593 - val_loss: 0.7376 - val_acc: 0.5577 - val_f1_score: 0.4448\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6819 - acc: 0.5612 - f1_score: 0.5584 - val_loss: 0.7369 - val_acc: 0.5641 - val_f1_score: 0.4489\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6818 - acc: 0.5624 - f1_score: 0.5590 - val_loss: 0.7366 - val_acc: 0.5662 - val_f1_score: 0.4502\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6817 - acc: 0.5628 - f1_score: 0.5593 - val_loss: 0.7362 - val_acc: 0.5726 - val_f1_score: 0.4542\n",
      "Epoch 35/300\n",
      "27/27 - 2s - loss: 0.6816 - acc: 0.5624 - f1_score: 0.5583 - val_loss: 0.7361 - val_acc: 0.5748 - val_f1_score: 0.4556\n",
      "Epoch 36/300\n",
      "27/27 - 2s - loss: 0.6815 - acc: 0.5622 - f1_score: 0.5581 - val_loss: 0.7355 - val_acc: 0.5791 - val_f1_score: 0.4583\n",
      "Epoch 37/300\n",
      "27/27 - 2s - loss: 0.6814 - acc: 0.5618 - f1_score: 0.5573 - val_loss: 0.7352 - val_acc: 0.5791 - val_f1_score: 0.4583\n",
      "Epoch 38/300\n",
      "27/27 - 2s - loss: 0.6813 - acc: 0.5615 - f1_score: 0.5569 - val_loss: 0.7346 - val_acc: 0.5855 - val_f1_score: 0.4623\n",
      "Epoch 39/300\n",
      "27/27 - 2s - loss: 0.6812 - acc: 0.5622 - f1_score: 0.5574 - val_loss: 0.7344 - val_acc: 0.5833 - val_f1_score: 0.4581\n",
      "Epoch 40/300\n",
      "27/27 - 2s - loss: 0.6811 - acc: 0.5615 - f1_score: 0.5565 - val_loss: 0.7339 - val_acc: 0.5833 - val_f1_score: 0.4581\n",
      "Epoch 41/300\n",
      "27/27 - 2s - loss: 0.6810 - acc: 0.5614 - f1_score: 0.5560 - val_loss: 0.7339 - val_acc: 0.5833 - val_f1_score: 0.4551\n",
      "Epoch 42/300\n",
      "27/27 - 2s - loss: 0.6809 - acc: 0.5612 - f1_score: 0.5558 - val_loss: 0.7336 - val_acc: 0.5855 - val_f1_score: 0.4565\n",
      "Epoch 43/300\n",
      "27/27 - 2s - loss: 0.6808 - acc: 0.5616 - f1_score: 0.5561 - val_loss: 0.7333 - val_acc: 0.5876 - val_f1_score: 0.4578\n",
      "Epoch 44/300\n",
      "27/27 - 2s - loss: 0.6807 - acc: 0.5611 - f1_score: 0.5553 - val_loss: 0.7331 - val_acc: 0.5876 - val_f1_score: 0.4578\n",
      "Epoch 45/300\n",
      "27/27 - 2s - loss: 0.6806 - acc: 0.5612 - f1_score: 0.5553 - val_loss: 0.7325 - val_acc: 0.5897 - val_f1_score: 0.4591\n",
      "Epoch 46/300\n",
      "27/27 - 2s - loss: 0.6805 - acc: 0.5619 - f1_score: 0.5558 - val_loss: 0.7317 - val_acc: 0.5919 - val_f1_score: 0.4604\n",
      "Epoch 47/300\n",
      "27/27 - 2s - loss: 0.6804 - acc: 0.5621 - f1_score: 0.5557 - val_loss: 0.7312 - val_acc: 0.5919 - val_f1_score: 0.4604\n",
      "Epoch 48/300\n",
      "27/27 - 2s - loss: 0.6804 - acc: 0.5629 - f1_score: 0.5562 - val_loss: 0.7310 - val_acc: 0.5919 - val_f1_score: 0.4604\n",
      "Epoch 49/300\n",
      "27/27 - 2s - loss: 0.6803 - acc: 0.5625 - f1_score: 0.5554 - val_loss: 0.7307 - val_acc: 0.5897 - val_f1_score: 0.4561\n",
      "Epoch 50/300\n",
      "27/27 - 2s - loss: 0.6802 - acc: 0.5638 - f1_score: 0.5567 - val_loss: 0.7304 - val_acc: 0.5940 - val_f1_score: 0.4587\n",
      "Epoch 51/300\n",
      "27/27 - 2s - loss: 0.6801 - acc: 0.5631 - f1_score: 0.5557 - val_loss: 0.7301 - val_acc: 0.5962 - val_f1_score: 0.4600\n",
      "Epoch 52/300\n",
      "27/27 - 2s - loss: 0.6800 - acc: 0.5622 - f1_score: 0.5543 - val_loss: 0.7303 - val_acc: 0.5983 - val_f1_score: 0.4613\n",
      "Epoch 53/300\n",
      "27/27 - 2s - loss: 0.6799 - acc: 0.5634 - f1_score: 0.5557 - val_loss: 0.7296 - val_acc: 0.5983 - val_f1_score: 0.4613\n",
      "Epoch 54/300\n",
      "27/27 - 2s - loss: 0.6798 - acc: 0.5624 - f1_score: 0.5541 - val_loss: 0.7296 - val_acc: 0.5962 - val_f1_score: 0.4569\n",
      "Epoch 55/300\n",
      "27/27 - 2s - loss: 0.6798 - acc: 0.5619 - f1_score: 0.5537 - val_loss: 0.7290 - val_acc: 0.5962 - val_f1_score: 0.4569\n",
      "Epoch 56/300\n",
      "27/27 - 2s - loss: 0.6797 - acc: 0.5614 - f1_score: 0.5527 - val_loss: 0.7292 - val_acc: 0.5962 - val_f1_score: 0.4569\n",
      "Epoch 57/300\n",
      "27/27 - 2s - loss: 0.6796 - acc: 0.5614 - f1_score: 0.5528 - val_loss: 0.7287 - val_acc: 0.5983 - val_f1_score: 0.4582\n",
      "Epoch 58/300\n",
      "27/27 - 2s - loss: 0.6795 - acc: 0.5616 - f1_score: 0.5525 - val_loss: 0.7290 - val_acc: 0.5983 - val_f1_score: 0.4582\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.14      0.20       179\n",
      "           1       0.62      0.86      0.72       289\n",
      "\n",
      "    accuracy                           0.59       468\n",
      "   macro avg       0.50      0.50      0.46       468\n",
      "weighted avg       0.53      0.59      0.52       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1419\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6919 - acc: 0.4756 - f1_score: 0.3223 - val_loss: 0.6941 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.4764 - f1_score: 0.3241 - val_loss: 0.6936 - val_acc: 0.3143 - val_f1_score: 0.2391\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.4786 - f1_score: 0.3294 - val_loss: 0.6930 - val_acc: 0.3165 - val_f1_score: 0.2450\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.4844 - f1_score: 0.3448 - val_loss: 0.6925 - val_acc: 0.3297 - val_f1_score: 0.2721\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.4848 - f1_score: 0.3522 - val_loss: 0.6921 - val_acc: 0.3341 - val_f1_score: 0.2806\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.4886 - f1_score: 0.3648 - val_loss: 0.6918 - val_acc: 0.3560 - val_f1_score: 0.3176\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.4920 - f1_score: 0.3814 - val_loss: 0.6914 - val_acc: 0.3648 - val_f1_score: 0.3326\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.4974 - f1_score: 0.3990 - val_loss: 0.6909 - val_acc: 0.3780 - val_f1_score: 0.3592\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6902 - acc: 0.5019 - f1_score: 0.4197 - val_loss: 0.6906 - val_acc: 0.3846 - val_f1_score: 0.3684\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5064 - f1_score: 0.4323 - val_loss: 0.6903 - val_acc: 0.3978 - val_f1_score: 0.3853\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.5108 - f1_score: 0.4436 - val_loss: 0.6900 - val_acc: 0.4088 - val_f1_score: 0.4003\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5147 - f1_score: 0.4584 - val_loss: 0.6896 - val_acc: 0.4286 - val_f1_score: 0.4243\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6895 - acc: 0.5247 - f1_score: 0.4796 - val_loss: 0.6892 - val_acc: 0.4396 - val_f1_score: 0.4377\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6894 - acc: 0.5317 - f1_score: 0.4966 - val_loss: 0.6889 - val_acc: 0.4725 - val_f1_score: 0.4724\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6892 - acc: 0.5363 - f1_score: 0.5061 - val_loss: 0.6886 - val_acc: 0.4813 - val_f1_score: 0.4812\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5419 - f1_score: 0.5184 - val_loss: 0.6883 - val_acc: 0.5011 - val_f1_score: 0.5005\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6890 - acc: 0.5442 - f1_score: 0.5231 - val_loss: 0.6881 - val_acc: 0.5077 - val_f1_score: 0.5066\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6889 - acc: 0.5454 - f1_score: 0.5263 - val_loss: 0.6879 - val_acc: 0.5253 - val_f1_score: 0.5233\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6887 - acc: 0.5503 - f1_score: 0.5353 - val_loss: 0.6876 - val_acc: 0.5319 - val_f1_score: 0.5289\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6886 - acc: 0.5519 - f1_score: 0.5394 - val_loss: 0.6873 - val_acc: 0.5495 - val_f1_score: 0.5439\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6886 - acc: 0.5544 - f1_score: 0.5466 - val_loss: 0.6870 - val_acc: 0.5648 - val_f1_score: 0.5574\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6885 - acc: 0.5532 - f1_score: 0.5479 - val_loss: 0.6867 - val_acc: 0.5604 - val_f1_score: 0.5518\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6884 - acc: 0.5526 - f1_score: 0.5484 - val_loss: 0.6865 - val_acc: 0.5648 - val_f1_score: 0.5552\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6883 - acc: 0.5539 - f1_score: 0.5506 - val_loss: 0.6861 - val_acc: 0.5758 - val_f1_score: 0.5630\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6882 - acc: 0.5536 - f1_score: 0.5521 - val_loss: 0.6859 - val_acc: 0.5824 - val_f1_score: 0.5688\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6881 - acc: 0.5535 - f1_score: 0.5522 - val_loss: 0.6856 - val_acc: 0.5824 - val_f1_score: 0.5688\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6880 - acc: 0.5555 - f1_score: 0.5546 - val_loss: 0.6854 - val_acc: 0.5868 - val_f1_score: 0.5711\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6879 - acc: 0.5534 - f1_score: 0.5528 - val_loss: 0.6851 - val_acc: 0.5912 - val_f1_score: 0.5749\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6878 - acc: 0.5554 - f1_score: 0.5551 - val_loss: 0.6848 - val_acc: 0.5934 - val_f1_score: 0.5769\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6877 - acc: 0.5557 - f1_score: 0.5555 - val_loss: 0.6845 - val_acc: 0.5934 - val_f1_score: 0.5761\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6876 - acc: 0.5557 - f1_score: 0.5556 - val_loss: 0.6841 - val_acc: 0.6000 - val_f1_score: 0.5818\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6875 - acc: 0.5542 - f1_score: 0.5542 - val_loss: 0.6838 - val_acc: 0.6044 - val_f1_score: 0.5847\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6874 - acc: 0.5535 - f1_score: 0.5535 - val_loss: 0.6835 - val_acc: 0.6154 - val_f1_score: 0.5942\n",
      "Epoch 34/300\n",
      "28/28 - 2s - loss: 0.6874 - acc: 0.5544 - f1_score: 0.5542 - val_loss: 0.6831 - val_acc: 0.6198 - val_f1_score: 0.5971\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5557 - f1_score: 0.5553 - val_loss: 0.6827 - val_acc: 0.6242 - val_f1_score: 0.6008\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6872 - acc: 0.5549 - f1_score: 0.5542 - val_loss: 0.6825 - val_acc: 0.6242 - val_f1_score: 0.6008\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6871 - acc: 0.5547 - f1_score: 0.5540 - val_loss: 0.6823 - val_acc: 0.6286 - val_f1_score: 0.6046\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6871 - acc: 0.5557 - f1_score: 0.5547 - val_loss: 0.6821 - val_acc: 0.6308 - val_f1_score: 0.6065\n",
      "Epoch 39/300\n",
      "28/28 - 2s - loss: 0.6870 - acc: 0.5552 - f1_score: 0.5543 - val_loss: 0.6818 - val_acc: 0.6330 - val_f1_score: 0.6084\n",
      "Epoch 40/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5573 - f1_score: 0.5559 - val_loss: 0.6816 - val_acc: 0.6330 - val_f1_score: 0.6084\n",
      "Epoch 41/300\n",
      "28/28 - 2s - loss: 0.6868 - acc: 0.5578 - f1_score: 0.5565 - val_loss: 0.6814 - val_acc: 0.6330 - val_f1_score: 0.6084\n",
      "Epoch 42/300\n",
      "28/28 - 2s - loss: 0.6868 - acc: 0.5578 - f1_score: 0.5564 - val_loss: 0.6810 - val_acc: 0.6374 - val_f1_score: 0.6122\n",
      "Epoch 43/300\n",
      "28/28 - 2s - loss: 0.6867 - acc: 0.5570 - f1_score: 0.5551 - val_loss: 0.6808 - val_acc: 0.6374 - val_f1_score: 0.6122\n",
      "Epoch 44/300\n",
      "28/28 - 2s - loss: 0.6866 - acc: 0.5574 - f1_score: 0.5554 - val_loss: 0.6805 - val_acc: 0.6374 - val_f1_score: 0.6122\n",
      "Epoch 45/300\n",
      "28/28 - 2s - loss: 0.6865 - acc: 0.5567 - f1_score: 0.5545 - val_loss: 0.6802 - val_acc: 0.6374 - val_f1_score: 0.6122\n",
      "Epoch 46/300\n",
      "28/28 - 2s - loss: 0.6865 - acc: 0.5577 - f1_score: 0.5551 - val_loss: 0.6798 - val_acc: 0.6374 - val_f1_score: 0.6103\n",
      "Epoch 47/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5583 - f1_score: 0.5553 - val_loss: 0.6796 - val_acc: 0.6374 - val_f1_score: 0.6103\n",
      "Epoch 48/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5591 - f1_score: 0.5560 - val_loss: 0.6794 - val_acc: 0.6374 - val_f1_score: 0.6103\n",
      "Epoch 49/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5594 - f1_score: 0.5562 - val_loss: 0.6792 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 50/300\n",
      "28/28 - 2s - loss: 0.6862 - acc: 0.5593 - f1_score: 0.5560 - val_loss: 0.6790 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 51/300\n",
      "28/28 - 2s - loss: 0.6861 - acc: 0.5593 - f1_score: 0.5558 - val_loss: 0.6788 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 52/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5587 - f1_score: 0.5549 - val_loss: 0.6786 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 53/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5587 - f1_score: 0.5551 - val_loss: 0.6782 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 54/300\n",
      "28/28 - 2s - loss: 0.6859 - acc: 0.5584 - f1_score: 0.5543 - val_loss: 0.6778 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 55/300\n",
      "28/28 - 2s - loss: 0.6858 - acc: 0.5596 - f1_score: 0.5551 - val_loss: 0.6777 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 56/300\n",
      "28/28 - 2s - loss: 0.6857 - acc: 0.5588 - f1_score: 0.5545 - val_loss: 0.6774 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 57/300\n",
      "28/28 - 2s - loss: 0.6857 - acc: 0.5596 - f1_score: 0.5550 - val_loss: 0.6773 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 58/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5594 - f1_score: 0.5549 - val_loss: 0.6771 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 59/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5594 - f1_score: 0.5546 - val_loss: 0.6768 - val_acc: 0.6396 - val_f1_score: 0.6121\n",
      "Epoch 60/300\n",
      "28/28 - 2s - loss: 0.6855 - acc: 0.5607 - f1_score: 0.5555 - val_loss: 0.6766 - val_acc: 0.6418 - val_f1_score: 0.6140\n",
      "Epoch 61/300\n",
      "28/28 - 2s - loss: 0.6855 - acc: 0.5607 - f1_score: 0.5553 - val_loss: 0.6764 - val_acc: 0.6440 - val_f1_score: 0.6159\n",
      "Epoch 62/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5610 - f1_score: 0.5553 - val_loss: 0.6760 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 63/300\n",
      "28/28 - 2s - loss: 0.6853 - acc: 0.5603 - f1_score: 0.5543 - val_loss: 0.6759 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 64/300\n",
      "28/28 - 2s - loss: 0.6853 - acc: 0.5607 - f1_score: 0.5549 - val_loss: 0.6756 - val_acc: 0.6440 - val_f1_score: 0.6149\n",
      "Epoch 65/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5606 - f1_score: 0.5542 - val_loss: 0.6755 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 66/300\n",
      "28/28 - 2s - loss: 0.6851 - acc: 0.5603 - f1_score: 0.5542 - val_loss: 0.6753 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 67/300\n",
      "28/28 - 2s - loss: 0.6851 - acc: 0.5603 - f1_score: 0.5542 - val_loss: 0.6751 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 68/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5599 - f1_score: 0.5537 - val_loss: 0.6749 - val_acc: 0.6418 - val_f1_score: 0.6130\n",
      "Epoch 69/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5600 - f1_score: 0.5538 - val_loss: 0.6745 - val_acc: 0.6440 - val_f1_score: 0.6149\n",
      "Epoch 70/300\n",
      "28/28 - 2s - loss: 0.6849 - acc: 0.5610 - f1_score: 0.5543 - val_loss: 0.6742 - val_acc: 0.6462 - val_f1_score: 0.6168\n",
      "Epoch 71/300\n",
      "28/28 - 2s - loss: 0.6848 - acc: 0.5610 - f1_score: 0.5539 - val_loss: 0.6740 - val_acc: 0.6462 - val_f1_score: 0.6168\n",
      "Epoch 72/300\n",
      "28/28 - 2s - loss: 0.6847 - acc: 0.5604 - f1_score: 0.5531 - val_loss: 0.6737 - val_acc: 0.6440 - val_f1_score: 0.6139\n",
      "Epoch 73/300\n",
      "28/28 - 2s - loss: 0.6847 - acc: 0.5609 - f1_score: 0.5534 - val_loss: 0.6734 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 74/300\n",
      "28/28 - 2s - loss: 0.6846 - acc: 0.5616 - f1_score: 0.5535 - val_loss: 0.6734 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 75/300\n",
      "28/28 - 2s - loss: 0.6846 - acc: 0.5612 - f1_score: 0.5533 - val_loss: 0.6733 - val_acc: 0.6418 - val_f1_score: 0.6110\n",
      "Epoch 76/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5612 - f1_score: 0.5534 - val_loss: 0.6732 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 77/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5616 - f1_score: 0.5536 - val_loss: 0.6729 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 78/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5615 - f1_score: 0.5530 - val_loss: 0.6728 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 79/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5623 - f1_score: 0.5541 - val_loss: 0.6728 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 80/300\n",
      "28/28 - 2s - loss: 0.6843 - acc: 0.5625 - f1_score: 0.5544 - val_loss: 0.6725 - val_acc: 0.6396 - val_f1_score: 0.6081\n",
      "Epoch 81/300\n",
      "28/28 - 2s - loss: 0.6843 - acc: 0.5623 - f1_score: 0.5542 - val_loss: 0.6723 - val_acc: 0.6418 - val_f1_score: 0.6099\n",
      "Epoch 82/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5619 - f1_score: 0.5535 - val_loss: 0.6721 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 83/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5615 - f1_score: 0.5528 - val_loss: 0.6718 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 84/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5609 - f1_score: 0.5520 - val_loss: 0.6716 - val_acc: 0.6462 - val_f1_score: 0.6114\n",
      "Epoch 85/300\n",
      "28/28 - 2s - loss: 0.6840 - acc: 0.5612 - f1_score: 0.5521 - val_loss: 0.6716 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 86/300\n",
      "28/28 - 2s - loss: 0.6840 - acc: 0.5609 - f1_score: 0.5517 - val_loss: 0.6714 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 87/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5610 - f1_score: 0.5522 - val_loss: 0.6714 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 88/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5610 - f1_score: 0.5525 - val_loss: 0.6713 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 89/300\n",
      "28/28 - 2s - loss: 0.6838 - acc: 0.5615 - f1_score: 0.5526 - val_loss: 0.6711 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 90/300\n",
      "28/28 - 2s - loss: 0.6838 - acc: 0.5612 - f1_score: 0.5520 - val_loss: 0.6709 - val_acc: 0.6462 - val_f1_score: 0.6114\n",
      "Epoch 91/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5613 - f1_score: 0.5517 - val_loss: 0.6708 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 92/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5610 - f1_score: 0.5519 - val_loss: 0.6707 - val_acc: 0.6505 - val_f1_score: 0.6174\n",
      "Epoch 93/300\n",
      "28/28 - 2s - loss: 0.6836 - acc: 0.5612 - f1_score: 0.5519 - val_loss: 0.6707 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 94/300\n",
      "28/28 - 2s - loss: 0.6836 - acc: 0.5613 - f1_score: 0.5524 - val_loss: 0.6707 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 95/300\n",
      "28/28 - 2s - loss: 0.6836 - acc: 0.5610 - f1_score: 0.5521 - val_loss: 0.6705 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 96/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5616 - f1_score: 0.5527 - val_loss: 0.6705 - val_acc: 0.6418 - val_f1_score: 0.6099\n",
      "Epoch 97/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5617 - f1_score: 0.5531 - val_loss: 0.6703 - val_acc: 0.6484 - val_f1_score: 0.6155\n",
      "Epoch 98/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5615 - f1_score: 0.5524 - val_loss: 0.6700 - val_acc: 0.6462 - val_f1_score: 0.6114\n",
      "Epoch 99/300\n",
      "28/28 - 2s - loss: 0.6834 - acc: 0.5616 - f1_score: 0.5520 - val_loss: 0.6699 - val_acc: 0.6462 - val_f1_score: 0.6125\n",
      "Epoch 100/300\n",
      "28/28 - 2s - loss: 0.6834 - acc: 0.5612 - f1_score: 0.5517 - val_loss: 0.6697 - val_acc: 0.6484 - val_f1_score: 0.6133\n",
      "Epoch 101/300\n",
      "28/28 - 2s - loss: 0.6833 - acc: 0.5610 - f1_score: 0.5514 - val_loss: 0.6696 - val_acc: 0.6462 - val_f1_score: 0.6114\n",
      "Epoch 102/300\n",
      "28/28 - 2s - loss: 0.6833 - acc: 0.5615 - f1_score: 0.5519 - val_loss: 0.6694 - val_acc: 0.6462 - val_f1_score: 0.6103\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00102: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.46      0.50       178\n",
      "           1       0.69      0.78      0.73       277\n",
      "\n",
      "    accuracy                           0.65       455\n",
      "   macro avg       0.63      0.62      0.62       455\n",
      "weighted avg       0.64      0.65      0.64       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1517\n",
      "Epoch 1/300\n",
      "28/28 - 6s - loss: 0.6927 - acc: 0.4427 - f1_score: 0.3071 - val_loss: 0.6795 - val_acc: 0.9202 - val_f1_score: 0.4792\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6922 - acc: 0.4534 - f1_score: 0.3314 - val_loss: 0.6803 - val_acc: 0.8604 - val_f1_score: 0.4625\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6918 - acc: 0.4725 - f1_score: 0.3893 - val_loss: 0.6811 - val_acc: 0.7721 - val_f1_score: 0.4357\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.4969 - f1_score: 0.4531 - val_loss: 0.6819 - val_acc: 0.7037 - val_f1_score: 0.4130\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5313 - f1_score: 0.5184 - val_loss: 0.6827 - val_acc: 0.6182 - val_f1_score: 0.3820\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6907 - acc: 0.5563 - f1_score: 0.5551 - val_loss: 0.6834 - val_acc: 0.5413 - val_f1_score: 0.3512\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.5635 - f1_score: 0.5634 - val_loss: 0.6842 - val_acc: 0.4387 - val_f1_score: 0.3050\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5684 - f1_score: 0.5665 - val_loss: 0.6849 - val_acc: 0.3903 - val_f1_score: 0.2807\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6896 - acc: 0.5681 - f1_score: 0.5639 - val_loss: 0.6856 - val_acc: 0.3618 - val_f1_score: 0.2725\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6892 - acc: 0.5709 - f1_score: 0.5646 - val_loss: 0.6861 - val_acc: 0.3618 - val_f1_score: 0.2758\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6889 - acc: 0.5732 - f1_score: 0.5652 - val_loss: 0.6867 - val_acc: 0.3533 - val_f1_score: 0.2709\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6886 - acc: 0.5734 - f1_score: 0.5636 - val_loss: 0.6875 - val_acc: 0.3419 - val_f1_score: 0.2643\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6882 - acc: 0.5735 - f1_score: 0.5623 - val_loss: 0.6881 - val_acc: 0.3276 - val_f1_score: 0.2559\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6879 - acc: 0.5769 - f1_score: 0.5642 - val_loss: 0.6885 - val_acc: 0.3248 - val_f1_score: 0.2542\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6876 - acc: 0.5752 - f1_score: 0.5616 - val_loss: 0.6893 - val_acc: 0.3134 - val_f1_score: 0.2473\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5752 - f1_score: 0.5597 - val_loss: 0.6901 - val_acc: 0.3105 - val_f1_score: 0.2483\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5758 - f1_score: 0.5598 - val_loss: 0.6907 - val_acc: 0.3077 - val_f1_score: 0.2465\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6866 - acc: 0.5758 - f1_score: 0.5585 - val_loss: 0.6915 - val_acc: 0.3077 - val_f1_score: 0.2465\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5751 - f1_score: 0.5567 - val_loss: 0.6922 - val_acc: 0.3077 - val_f1_score: 0.2465\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5749 - f1_score: 0.5564 - val_loss: 0.6930 - val_acc: 0.3077 - val_f1_score: 0.2492\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6857 - acc: 0.5766 - f1_score: 0.5564 - val_loss: 0.6938 - val_acc: 0.3048 - val_f1_score: 0.2474\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       349\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.92       351\n",
      "   macro avg       0.50      0.46      0.48       351\n",
      "weighted avg       0.99      0.92      0.95       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1544\n",
      "Epoch 1/300\n",
      "29/29 - 7s - loss: 0.6920 - acc: 0.4672 - f1_score: 0.3189 - val_loss: 0.6876 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/300\n",
      "29/29 - 2s - loss: 0.6917 - acc: 0.4728 - f1_score: 0.3323 - val_loss: 0.6859 - val_acc: 0.3654 - val_f1_score: 0.2676\n",
      "Epoch 3/300\n",
      "29/29 - 2s - loss: 0.6913 - acc: 0.4804 - f1_score: 0.3642 - val_loss: 0.6842 - val_acc: 0.3942 - val_f1_score: 0.3352\n",
      "Epoch 4/300\n",
      "29/29 - 2s - loss: 0.6910 - acc: 0.4947 - f1_score: 0.4123 - val_loss: 0.6825 - val_acc: 0.4904 - val_f1_score: 0.4687\n",
      "Epoch 5/300\n",
      "29/29 - 2s - loss: 0.6906 - acc: 0.5065 - f1_score: 0.4546 - val_loss: 0.6808 - val_acc: 0.5865 - val_f1_score: 0.5847\n",
      "Epoch 6/300\n",
      "29/29 - 2s - loss: 0.6903 - acc: 0.5276 - f1_score: 0.4995 - val_loss: 0.6791 - val_acc: 0.6731 - val_f1_score: 0.6720\n",
      "Epoch 7/300\n",
      "29/29 - 2s - loss: 0.6900 - acc: 0.5442 - f1_score: 0.5320 - val_loss: 0.6774 - val_acc: 0.7115 - val_f1_score: 0.7088\n",
      "Epoch 8/300\n",
      "29/29 - 2s - loss: 0.6897 - acc: 0.5561 - f1_score: 0.5512 - val_loss: 0.6758 - val_acc: 0.7404 - val_f1_score: 0.7363\n",
      "Epoch 9/300\n",
      "29/29 - 2s - loss: 0.6894 - acc: 0.5555 - f1_score: 0.5541 - val_loss: 0.6742 - val_acc: 0.7596 - val_f1_score: 0.7530\n",
      "Epoch 10/300\n",
      "29/29 - 2s - loss: 0.6891 - acc: 0.5572 - f1_score: 0.5570 - val_loss: 0.6726 - val_acc: 0.7596 - val_f1_score: 0.7530\n",
      "Epoch 11/300\n",
      "29/29 - 2s - loss: 0.6888 - acc: 0.5583 - f1_score: 0.5583 - val_loss: 0.6711 - val_acc: 0.7596 - val_f1_score: 0.7530\n",
      "Epoch 12/300\n",
      "29/29 - 2s - loss: 0.6886 - acc: 0.5586 - f1_score: 0.5580 - val_loss: 0.6696 - val_acc: 0.7692 - val_f1_score: 0.7621\n",
      "Epoch 13/300\n",
      "29/29 - 2s - loss: 0.6883 - acc: 0.5570 - f1_score: 0.5559 - val_loss: 0.6681 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 14/300\n",
      "29/29 - 2s - loss: 0.6880 - acc: 0.5557 - f1_score: 0.5534 - val_loss: 0.6667 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 15/300\n",
      "29/29 - 2s - loss: 0.6878 - acc: 0.5554 - f1_score: 0.5525 - val_loss: 0.6653 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 16/300\n",
      "29/29 - 2s - loss: 0.6876 - acc: 0.5583 - f1_score: 0.5549 - val_loss: 0.6641 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 17/300\n",
      "29/29 - 2s - loss: 0.6874 - acc: 0.5595 - f1_score: 0.5554 - val_loss: 0.6629 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 18/300\n",
      "29/29 - 2s - loss: 0.6872 - acc: 0.5594 - f1_score: 0.5546 - val_loss: 0.6616 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 19/300\n",
      "29/29 - 2s - loss: 0.6870 - acc: 0.5575 - f1_score: 0.5520 - val_loss: 0.6605 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 20/300\n",
      "29/29 - 2s - loss: 0.6868 - acc: 0.5608 - f1_score: 0.5546 - val_loss: 0.6593 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 21/300\n",
      "29/29 - 2s - loss: 0.6866 - acc: 0.5601 - f1_score: 0.5535 - val_loss: 0.6583 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 22/300\n",
      "29/29 - 2s - loss: 0.6864 - acc: 0.5620 - f1_score: 0.5549 - val_loss: 0.6573 - val_acc: 0.7692 - val_f1_score: 0.7604\n",
      "Epoch 23/300\n",
      "29/29 - 2s - loss: 0.6862 - acc: 0.5613 - f1_score: 0.5537 - val_loss: 0.6564 - val_acc: 0.7692 - val_f1_score: 0.7584\n",
      "Epoch 24/300\n",
      "29/29 - 2s - loss: 0.6861 - acc: 0.5616 - f1_score: 0.5533 - val_loss: 0.6554 - val_acc: 0.7692 - val_f1_score: 0.7584\n",
      "Epoch 25/300\n",
      "29/29 - 2s - loss: 0.6859 - acc: 0.5617 - f1_score: 0.5528 - val_loss: 0.6546 - val_acc: 0.7692 - val_f1_score: 0.7584\n",
      "Epoch 26/300\n",
      "29/29 - 2s - loss: 0.6857 - acc: 0.5614 - f1_score: 0.5520 - val_loss: 0.6537 - val_acc: 0.7788 - val_f1_score: 0.7675\n",
      "Epoch 27/300\n",
      "29/29 - 2s - loss: 0.6856 - acc: 0.5620 - f1_score: 0.5518 - val_loss: 0.6528 - val_acc: 0.7788 - val_f1_score: 0.7675\n",
      "Epoch 28/300\n",
      "29/29 - 2s - loss: 0.6854 - acc: 0.5608 - f1_score: 0.5498 - val_loss: 0.6519 - val_acc: 0.7788 - val_f1_score: 0.7675\n",
      "Epoch 29/300\n",
      "29/29 - 2s - loss: 0.6853 - acc: 0.5609 - f1_score: 0.5496 - val_loss: 0.6512 - val_acc: 0.7692 - val_f1_score: 0.7563\n",
      "Epoch 30/300\n",
      "29/29 - 2s - loss: 0.6851 - acc: 0.5609 - f1_score: 0.5488 - val_loss: 0.6505 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 31/300\n",
      "29/29 - 2s - loss: 0.6850 - acc: 0.5610 - f1_score: 0.5488 - val_loss: 0.6496 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 32/300\n",
      "29/29 - 2s - loss: 0.6848 - acc: 0.5606 - f1_score: 0.5480 - val_loss: 0.6487 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 33/300\n",
      "29/29 - 2s - loss: 0.6847 - acc: 0.5602 - f1_score: 0.5469 - val_loss: 0.6479 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 34/300\n",
      "29/29 - 2s - loss: 0.6845 - acc: 0.5601 - f1_score: 0.5466 - val_loss: 0.6471 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 35/300\n",
      "29/29 - 2s - loss: 0.6844 - acc: 0.5597 - f1_score: 0.5452 - val_loss: 0.6461 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 36/300\n",
      "29/29 - 2s - loss: 0.6843 - acc: 0.5598 - f1_score: 0.5457 - val_loss: 0.6456 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 37/300\n",
      "29/29 - 2s - loss: 0.6841 - acc: 0.5602 - f1_score: 0.5456 - val_loss: 0.6450 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 38/300\n",
      "29/29 - 2s - loss: 0.6840 - acc: 0.5606 - f1_score: 0.5456 - val_loss: 0.6443 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 39/300\n",
      "29/29 - 2s - loss: 0.6839 - acc: 0.5612 - f1_score: 0.5456 - val_loss: 0.6437 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 40/300\n",
      "29/29 - 2s - loss: 0.6838 - acc: 0.5621 - f1_score: 0.5460 - val_loss: 0.6428 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 41/300\n",
      "29/29 - 2s - loss: 0.6836 - acc: 0.5610 - f1_score: 0.5451 - val_loss: 0.6425 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 42/300\n",
      "29/29 - 2s - loss: 0.6835 - acc: 0.5621 - f1_score: 0.5456 - val_loss: 0.6417 - val_acc: 0.7788 - val_f1_score: 0.7653\n",
      "Epoch 43/300\n",
      "29/29 - 2s - loss: 0.6834 - acc: 0.5628 - f1_score: 0.5463 - val_loss: 0.6414 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 44/300\n",
      "29/29 - 2s - loss: 0.6833 - acc: 0.5638 - f1_score: 0.5468 - val_loss: 0.6408 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 45/300\n",
      "29/29 - 2s - loss: 0.6832 - acc: 0.5642 - f1_score: 0.5468 - val_loss: 0.6401 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 46/300\n",
      "29/29 - 2s - loss: 0.6831 - acc: 0.5641 - f1_score: 0.5461 - val_loss: 0.6395 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 47/300\n",
      "29/29 - 2s - loss: 0.6829 - acc: 0.5641 - f1_score: 0.5464 - val_loss: 0.6392 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 48/300\n",
      "29/29 - 2s - loss: 0.6828 - acc: 0.5656 - f1_score: 0.5469 - val_loss: 0.6387 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 49/300\n",
      "29/29 - 2s - loss: 0.6827 - acc: 0.5658 - f1_score: 0.5467 - val_loss: 0.6380 - val_acc: 0.7885 - val_f1_score: 0.7744\n",
      "Epoch 50/300\n",
      "29/29 - 2s - loss: 0.6826 - acc: 0.5658 - f1_score: 0.5470 - val_loss: 0.6375 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 51/300\n",
      "29/29 - 2s - loss: 0.6825 - acc: 0.5668 - f1_score: 0.5472 - val_loss: 0.6368 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 52/300\n",
      "29/29 - 2s - loss: 0.6824 - acc: 0.5665 - f1_score: 0.5470 - val_loss: 0.6364 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 53/300\n",
      "29/29 - 2s - loss: 0.6823 - acc: 0.5669 - f1_score: 0.5467 - val_loss: 0.6360 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 54/300\n",
      "29/29 - 2s - loss: 0.6822 - acc: 0.5669 - f1_score: 0.5468 - val_loss: 0.6354 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 55/300\n",
      "29/29 - 2s - loss: 0.6821 - acc: 0.5672 - f1_score: 0.5468 - val_loss: 0.6352 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 56/300\n",
      "29/29 - 2s - loss: 0.6820 - acc: 0.5682 - f1_score: 0.5475 - val_loss: 0.6343 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 57/300\n",
      "29/29 - 2s - loss: 0.6819 - acc: 0.5679 - f1_score: 0.5465 - val_loss: 0.6338 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 58/300\n",
      "29/29 - 2s - loss: 0.6818 - acc: 0.5678 - f1_score: 0.5472 - val_loss: 0.6335 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 59/300\n",
      "29/29 - 2s - loss: 0.6817 - acc: 0.5683 - f1_score: 0.5467 - val_loss: 0.6331 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 60/300\n",
      "29/29 - 2s - loss: 0.6816 - acc: 0.5680 - f1_score: 0.5460 - val_loss: 0.6328 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 61/300\n",
      "29/29 - 2s - loss: 0.6815 - acc: 0.5674 - f1_score: 0.5451 - val_loss: 0.6321 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 62/300\n",
      "29/29 - 2s - loss: 0.6814 - acc: 0.5683 - f1_score: 0.5460 - val_loss: 0.6317 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Epoch 63/300\n",
      "29/29 - 2s - loss: 0.6813 - acc: 0.5682 - f1_score: 0.5456 - val_loss: 0.6312 - val_acc: 0.7788 - val_f1_score: 0.7629\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00063: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72        39\n",
      "           1       0.83      0.83      0.83        65\n",
      "\n",
      "    accuracy                           0.79       104\n",
      "   macro avg       0.77      0.77      0.77       104\n",
      "weighted avg       0.79      0.79      0.79       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1624\n",
      "Epoch 1/300\n",
      "28/28 - 7s - loss: 0.6923 - acc: 0.4556 - f1_score: 0.3130 - val_loss: 0.6853 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.4607 - f1_score: 0.3245 - val_loss: 0.6852 - val_acc: 0.6667 - val_f1_score: 0.4000\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6916 - acc: 0.4695 - f1_score: 0.3561 - val_loss: 0.6850 - val_acc: 0.6610 - val_f1_score: 0.4059\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.4812 - f1_score: 0.3993 - val_loss: 0.6848 - val_acc: 0.6439 - val_f1_score: 0.4622\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6910 - acc: 0.5050 - f1_score: 0.4598 - val_loss: 0.6847 - val_acc: 0.6011 - val_f1_score: 0.4808\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.5322 - f1_score: 0.5125 - val_loss: 0.6845 - val_acc: 0.5755 - val_f1_score: 0.4924\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.5523 - f1_score: 0.5451 - val_loss: 0.6844 - val_acc: 0.5413 - val_f1_score: 0.4806\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5568 - f1_score: 0.5558 - val_loss: 0.6843 - val_acc: 0.5071 - val_f1_score: 0.4577\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6897 - acc: 0.5624 - f1_score: 0.5624 - val_loss: 0.6842 - val_acc: 0.5043 - val_f1_score: 0.4702\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6894 - acc: 0.5621 - f1_score: 0.5616 - val_loss: 0.6840 - val_acc: 0.4843 - val_f1_score: 0.4561\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5635 - f1_score: 0.5620 - val_loss: 0.6839 - val_acc: 0.4701 - val_f1_score: 0.4536\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6888 - acc: 0.5644 - f1_score: 0.5616 - val_loss: 0.6838 - val_acc: 0.4558 - val_f1_score: 0.4446\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6885 - acc: 0.5638 - f1_score: 0.5600 - val_loss: 0.6836 - val_acc: 0.4615 - val_f1_score: 0.4521\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6883 - acc: 0.5648 - f1_score: 0.5600 - val_loss: 0.6835 - val_acc: 0.4615 - val_f1_score: 0.4537\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6880 - acc: 0.5668 - f1_score: 0.5610 - val_loss: 0.6834 - val_acc: 0.4587 - val_f1_score: 0.4533\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6877 - acc: 0.5672 - f1_score: 0.5602 - val_loss: 0.6835 - val_acc: 0.4587 - val_f1_score: 0.4555\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6875 - acc: 0.5678 - f1_score: 0.5604 - val_loss: 0.6833 - val_acc: 0.4701 - val_f1_score: 0.4678\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6872 - acc: 0.5687 - f1_score: 0.5605 - val_loss: 0.6834 - val_acc: 0.4729 - val_f1_score: 0.4715\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6870 - acc: 0.5685 - f1_score: 0.5594 - val_loss: 0.6834 - val_acc: 0.4672 - val_f1_score: 0.4664\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6867 - acc: 0.5678 - f1_score: 0.5582 - val_loss: 0.6835 - val_acc: 0.4672 - val_f1_score: 0.4664\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6865 - acc: 0.5695 - f1_score: 0.5590 - val_loss: 0.6836 - val_acc: 0.4672 - val_f1_score: 0.4666\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5684 - f1_score: 0.5571 - val_loss: 0.6836 - val_acc: 0.4644 - val_f1_score: 0.4642\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6860 - acc: 0.5684 - f1_score: 0.5563 - val_loss: 0.6838 - val_acc: 0.4758 - val_f1_score: 0.4758\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6858 - acc: 0.5679 - f1_score: 0.5553 - val_loss: 0.6840 - val_acc: 0.4758 - val_f1_score: 0.4758\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5681 - f1_score: 0.5545 - val_loss: 0.6841 - val_acc: 0.4729 - val_f1_score: 0.4729\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5670 - f1_score: 0.5530 - val_loss: 0.6844 - val_acc: 0.4672 - val_f1_score: 0.4671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D83C42280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.66      0.70       259\n",
      "           1       0.26      0.33      0.29        92\n",
      "\n",
      "    accuracy                           0.58       351\n",
      "   macro avg       0.50      0.50      0.49       351\n",
      "weighted avg       0.61      0.58      0.59       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1674\n",
      "Epoch 1/300\n",
      "28/28 - 6s - loss: 0.6911 - acc: 0.4852 - f1_score: 0.3267 - val_loss: 0.7131 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.4857 - f1_score: 0.3276 - val_loss: 0.7151 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6901 - acc: 0.4891 - f1_score: 0.3357 - val_loss: 0.7170 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6896 - acc: 0.4929 - f1_score: 0.3456 - val_loss: 0.7190 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.4972 - f1_score: 0.3618 - val_loss: 0.7208 - val_acc: 0.0400 - val_f1_score: 0.0385\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6887 - acc: 0.5027 - f1_score: 0.3869 - val_loss: 0.7227 - val_acc: 0.0431 - val_f1_score: 0.0418\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6883 - acc: 0.5088 - f1_score: 0.4095 - val_loss: 0.7246 - val_acc: 0.0431 - val_f1_score: 0.0418\n",
      "Epoch 8/300\n",
      "28/28 - 4s - loss: 0.6879 - acc: 0.5158 - f1_score: 0.4358 - val_loss: 0.7264 - val_acc: 0.0492 - val_f1_score: 0.0483\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6875 - acc: 0.5237 - f1_score: 0.4613 - val_loss: 0.7282 - val_acc: 0.0554 - val_f1_score: 0.0548\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6872 - acc: 0.5346 - f1_score: 0.4859 - val_loss: 0.7298 - val_acc: 0.0615 - val_f1_score: 0.0612\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5433 - f1_score: 0.5051 - val_loss: 0.7312 - val_acc: 0.0800 - val_f1_score: 0.0800\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6866 - acc: 0.5527 - f1_score: 0.5242 - val_loss: 0.7325 - val_acc: 0.0831 - val_f1_score: 0.0831\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6863 - acc: 0.5626 - f1_score: 0.5430 - val_loss: 0.7339 - val_acc: 0.0985 - val_f1_score: 0.0982\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6861 - acc: 0.5691 - f1_score: 0.5566 - val_loss: 0.7350 - val_acc: 0.1169 - val_f1_score: 0.1157\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6858 - acc: 0.5735 - f1_score: 0.5648 - val_loss: 0.7356 - val_acc: 0.1446 - val_f1_score: 0.1410\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5712 - f1_score: 0.5662 - val_loss: 0.7364 - val_acc: 0.1754 - val_f1_score: 0.1678\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5712 - f1_score: 0.5679 - val_loss: 0.7368 - val_acc: 0.2185 - val_f1_score: 0.2032\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5697 - f1_score: 0.5681 - val_loss: 0.7372 - val_acc: 0.2554 - val_f1_score: 0.2318\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5680 - f1_score: 0.5671 - val_loss: 0.7373 - val_acc: 0.2892 - val_f1_score: 0.2567\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6849 - acc: 0.5688 - f1_score: 0.5685 - val_loss: 0.7375 - val_acc: 0.3169 - val_f1_score: 0.2763\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6847 - acc: 0.5676 - f1_score: 0.5675 - val_loss: 0.7378 - val_acc: 0.3415 - val_f1_score: 0.2932\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5688 - f1_score: 0.5688 - val_loss: 0.7376 - val_acc: 0.3631 - val_f1_score: 0.3076\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5673 - f1_score: 0.5672 - val_loss: 0.7374 - val_acc: 0.3815 - val_f1_score: 0.3197\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5666 - f1_score: 0.5664 - val_loss: 0.7371 - val_acc: 0.3938 - val_f1_score: 0.3276\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5664 - f1_score: 0.5661 - val_loss: 0.7366 - val_acc: 0.4092 - val_f1_score: 0.3374\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6840 - acc: 0.5676 - f1_score: 0.5670 - val_loss: 0.7364 - val_acc: 0.4154 - val_f1_score: 0.3413\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6838 - acc: 0.5664 - f1_score: 0.5656 - val_loss: 0.7362 - val_acc: 0.4246 - val_f1_score: 0.3471\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5683 - f1_score: 0.5673 - val_loss: 0.7359 - val_acc: 0.4308 - val_f1_score: 0.3509\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6835 - acc: 0.5674 - f1_score: 0.5663 - val_loss: 0.7353 - val_acc: 0.4369 - val_f1_score: 0.3547\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6834 - acc: 0.5664 - f1_score: 0.5650 - val_loss: 0.7353 - val_acc: 0.4400 - val_f1_score: 0.3566\n",
      "Epoch 31/300\n",
      "28/28 - 2s - loss: 0.6833 - acc: 0.5667 - f1_score: 0.5650 - val_loss: 0.7349 - val_acc: 0.4554 - val_f1_score: 0.3661\n",
      "Epoch 32/300\n",
      "28/28 - 2s - loss: 0.6832 - acc: 0.5674 - f1_score: 0.5655 - val_loss: 0.7342 - val_acc: 0.4646 - val_f1_score: 0.3717\n",
      "Epoch 33/300\n",
      "28/28 - 2s - loss: 0.6831 - acc: 0.5663 - f1_score: 0.5642 - val_loss: 0.7337 - val_acc: 0.4677 - val_f1_score: 0.3735\n",
      "Epoch 34/300\n",
      "28/28 - 2s - loss: 0.6829 - acc: 0.5667 - f1_score: 0.5644 - val_loss: 0.7326 - val_acc: 0.4708 - val_f1_score: 0.3754\n",
      "Epoch 35/300\n",
      "28/28 - 2s - loss: 0.6828 - acc: 0.5663 - f1_score: 0.5635 - val_loss: 0.7318 - val_acc: 0.4800 - val_f1_score: 0.3810\n",
      "Epoch 36/300\n",
      "28/28 - 2s - loss: 0.6827 - acc: 0.5666 - f1_score: 0.5635 - val_loss: 0.7314 - val_acc: 0.4800 - val_f1_score: 0.3810\n",
      "Epoch 37/300\n",
      "28/28 - 2s - loss: 0.6826 - acc: 0.5671 - f1_score: 0.5640 - val_loss: 0.7307 - val_acc: 0.4800 - val_f1_score: 0.3810\n",
      "Epoch 38/300\n",
      "28/28 - 2s - loss: 0.6825 - acc: 0.5677 - f1_score: 0.5643 - val_loss: 0.7303 - val_acc: 0.4862 - val_f1_score: 0.3846\n",
      "Epoch 39/300\n",
      "28/28 - 2s - loss: 0.6824 - acc: 0.5671 - f1_score: 0.5634 - val_loss: 0.7303 - val_acc: 0.4862 - val_f1_score: 0.3846\n",
      "Epoch 40/300\n",
      "28/28 - 2s - loss: 0.6823 - acc: 0.5668 - f1_score: 0.5630 - val_loss: 0.7303 - val_acc: 0.4923 - val_f1_score: 0.3883\n",
      "Epoch 41/300\n",
      "28/28 - 2s - loss: 0.6822 - acc: 0.5670 - f1_score: 0.5631 - val_loss: 0.7300 - val_acc: 0.4954 - val_f1_score: 0.3902\n",
      "Epoch 42/300\n",
      "28/28 - 2s - loss: 0.6821 - acc: 0.5653 - f1_score: 0.5611 - val_loss: 0.7298 - val_acc: 0.4985 - val_f1_score: 0.3920\n",
      "Epoch 43/300\n",
      "28/28 - 2s - loss: 0.6820 - acc: 0.5660 - f1_score: 0.5618 - val_loss: 0.7299 - val_acc: 0.5015 - val_f1_score: 0.3938\n",
      "Epoch 44/300\n",
      "28/28 - 2s - loss: 0.6819 - acc: 0.5649 - f1_score: 0.5604 - val_loss: 0.7302 - val_acc: 0.5015 - val_f1_score: 0.3938\n",
      "Epoch 45/300\n",
      "28/28 - 2s - loss: 0.6818 - acc: 0.5650 - f1_score: 0.5606 - val_loss: 0.7291 - val_acc: 0.5077 - val_f1_score: 0.3975\n",
      "Epoch 46/300\n",
      "28/28 - 2s - loss: 0.6817 - acc: 0.5644 - f1_score: 0.5598 - val_loss: 0.7287 - val_acc: 0.5077 - val_f1_score: 0.3975\n",
      "Epoch 47/300\n",
      "28/28 - 2s - loss: 0.6816 - acc: 0.5651 - f1_score: 0.5603 - val_loss: 0.7282 - val_acc: 0.5138 - val_f1_score: 0.4011\n",
      "Epoch 48/300\n",
      "28/28 - 2s - loss: 0.6815 - acc: 0.5656 - f1_score: 0.5606 - val_loss: 0.7277 - val_acc: 0.5138 - val_f1_score: 0.4011\n",
      "Epoch 49/300\n",
      "28/28 - 2s - loss: 0.6814 - acc: 0.5664 - f1_score: 0.5613 - val_loss: 0.7272 - val_acc: 0.5200 - val_f1_score: 0.4048\n",
      "Epoch 50/300\n",
      "28/28 - 2s - loss: 0.6813 - acc: 0.5670 - f1_score: 0.5618 - val_loss: 0.7268 - val_acc: 0.5231 - val_f1_score: 0.4066\n",
      "Epoch 51/300\n",
      "28/28 - 2s - loss: 0.6812 - acc: 0.5670 - f1_score: 0.5613 - val_loss: 0.7268 - val_acc: 0.5231 - val_f1_score: 0.4066\n",
      "Epoch 52/300\n",
      "28/28 - 2s - loss: 0.6811 - acc: 0.5677 - f1_score: 0.5622 - val_loss: 0.7263 - val_acc: 0.5262 - val_f1_score: 0.4084\n",
      "Epoch 53/300\n",
      "28/28 - 2s - loss: 0.6810 - acc: 0.5673 - f1_score: 0.5616 - val_loss: 0.7257 - val_acc: 0.5292 - val_f1_score: 0.4102\n",
      "Epoch 54/300\n",
      "28/28 - 2s - loss: 0.6809 - acc: 0.5683 - f1_score: 0.5625 - val_loss: 0.7244 - val_acc: 0.5323 - val_f1_score: 0.4120\n",
      "Epoch 55/300\n",
      "28/28 - 2s - loss: 0.6808 - acc: 0.5687 - f1_score: 0.5626 - val_loss: 0.7236 - val_acc: 0.5323 - val_f1_score: 0.4120\n",
      "Epoch 56/300\n",
      "28/28 - 2s - loss: 0.6807 - acc: 0.5683 - f1_score: 0.5618 - val_loss: 0.7236 - val_acc: 0.5354 - val_f1_score: 0.4138\n",
      "Epoch 57/300\n",
      "28/28 - 2s - loss: 0.6806 - acc: 0.5681 - f1_score: 0.5618 - val_loss: 0.7226 - val_acc: 0.5385 - val_f1_score: 0.4156\n",
      "Epoch 58/300\n",
      "28/28 - 2s - loss: 0.6805 - acc: 0.5683 - f1_score: 0.5616 - val_loss: 0.7228 - val_acc: 0.5385 - val_f1_score: 0.4156\n",
      "Epoch 59/300\n",
      "28/28 - 2s - loss: 0.6804 - acc: 0.5678 - f1_score: 0.5610 - val_loss: 0.7232 - val_acc: 0.5385 - val_f1_score: 0.4156\n",
      "Epoch 60/300\n",
      "28/28 - 2s - loss: 0.6803 - acc: 0.5683 - f1_score: 0.5615 - val_loss: 0.7232 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 61/300\n",
      "28/28 - 2s - loss: 0.6802 - acc: 0.5684 - f1_score: 0.5616 - val_loss: 0.7226 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 62/300\n",
      "28/28 - 2s - loss: 0.6801 - acc: 0.5684 - f1_score: 0.5612 - val_loss: 0.7228 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 63/300\n",
      "28/28 - 2s - loss: 0.6800 - acc: 0.5678 - f1_score: 0.5608 - val_loss: 0.7229 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 64/300\n",
      "28/28 - 2s - loss: 0.6799 - acc: 0.5684 - f1_score: 0.5610 - val_loss: 0.7234 - val_acc: 0.5415 - val_f1_score: 0.4174\n",
      "Epoch 65/300\n",
      "28/28 - 2s - loss: 0.6798 - acc: 0.5685 - f1_score: 0.5616 - val_loss: 0.7224 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 66/300\n",
      "28/28 - 2s - loss: 0.6797 - acc: 0.5687 - f1_score: 0.5613 - val_loss: 0.7222 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 67/300\n",
      "28/28 - 2s - loss: 0.6797 - acc: 0.5691 - f1_score: 0.5615 - val_loss: 0.7224 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 68/300\n",
      "28/28 - 2s - loss: 0.6796 - acc: 0.5693 - f1_score: 0.5616 - val_loss: 0.7223 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 69/300\n",
      "28/28 - 2s - loss: 0.6795 - acc: 0.5688 - f1_score: 0.5611 - val_loss: 0.7215 - val_acc: 0.5446 - val_f1_score: 0.4193\n",
      "Epoch 70/300\n",
      "28/28 - 2s - loss: 0.6794 - acc: 0.5684 - f1_score: 0.5605 - val_loss: 0.7214 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 71/300\n",
      "28/28 - 2s - loss: 0.6793 - acc: 0.5693 - f1_score: 0.5612 - val_loss: 0.7218 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 72/300\n",
      "28/28 - 2s - loss: 0.6792 - acc: 0.5687 - f1_score: 0.5609 - val_loss: 0.7204 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 73/300\n",
      "28/28 - 2s - loss: 0.6791 - acc: 0.5695 - f1_score: 0.5612 - val_loss: 0.7207 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 74/300\n",
      "28/28 - 2s - loss: 0.6790 - acc: 0.5695 - f1_score: 0.5612 - val_loss: 0.7213 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 75/300\n",
      "28/28 - 2s - loss: 0.6790 - acc: 0.5697 - f1_score: 0.5615 - val_loss: 0.7205 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 76/300\n",
      "28/28 - 2s - loss: 0.6789 - acc: 0.5694 - f1_score: 0.5609 - val_loss: 0.7208 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 77/300\n",
      "28/28 - 2s - loss: 0.6788 - acc: 0.5698 - f1_score: 0.5617 - val_loss: 0.7201 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 78/300\n",
      "28/28 - 2s - loss: 0.6787 - acc: 0.5703 - f1_score: 0.5618 - val_loss: 0.7203 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 79/300\n",
      "28/28 - 2s - loss: 0.6786 - acc: 0.5711 - f1_score: 0.5625 - val_loss: 0.7212 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 80/300\n",
      "28/28 - 2s - loss: 0.6785 - acc: 0.5698 - f1_score: 0.5614 - val_loss: 0.7210 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 81/300\n",
      "28/28 - 2s - loss: 0.6785 - acc: 0.5704 - f1_score: 0.5619 - val_loss: 0.7213 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 82/300\n",
      "28/28 - 2s - loss: 0.6784 - acc: 0.5711 - f1_score: 0.5626 - val_loss: 0.7206 - val_acc: 0.5477 - val_f1_score: 0.4211\n",
      "Epoch 83/300\n",
      "28/28 - 2s - loss: 0.6783 - acc: 0.5720 - f1_score: 0.5632 - val_loss: 0.7207 - val_acc: 0.5508 - val_f1_score: 0.4229\n",
      "Epoch 84/300\n",
      "28/28 - 2s - loss: 0.6782 - acc: 0.5718 - f1_score: 0.5630 - val_loss: 0.7211 - val_acc: 0.5508 - val_f1_score: 0.4229\n",
      "Epoch 85/300\n",
      "28/28 - 2s - loss: 0.6782 - acc: 0.5728 - f1_score: 0.5639 - val_loss: 0.7205 - val_acc: 0.5508 - val_f1_score: 0.4229\n",
      "Epoch 86/300\n",
      "28/28 - 2s - loss: 0.6781 - acc: 0.5722 - f1_score: 0.5635 - val_loss: 0.7199 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 87/300\n",
      "28/28 - 2s - loss: 0.6780 - acc: 0.5725 - f1_score: 0.5636 - val_loss: 0.7201 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 88/300\n",
      "28/28 - 2s - loss: 0.6779 - acc: 0.5725 - f1_score: 0.5635 - val_loss: 0.7194 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 89/300\n",
      "28/28 - 2s - loss: 0.6778 - acc: 0.5734 - f1_score: 0.5640 - val_loss: 0.7210 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 90/300\n",
      "28/28 - 2s - loss: 0.6777 - acc: 0.5728 - f1_score: 0.5638 - val_loss: 0.7200 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 91/300\n",
      "28/28 - 2s - loss: 0.6777 - acc: 0.5729 - f1_score: 0.5639 - val_loss: 0.7203 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 92/300\n",
      "28/28 - 2s - loss: 0.6776 - acc: 0.5729 - f1_score: 0.5640 - val_loss: 0.7189 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 93/300\n",
      "28/28 - 2s - loss: 0.6775 - acc: 0.5737 - f1_score: 0.5643 - val_loss: 0.7197 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 94/300\n",
      "28/28 - 2s - loss: 0.6774 - acc: 0.5728 - f1_score: 0.5635 - val_loss: 0.7201 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 95/300\n",
      "28/28 - 2s - loss: 0.6774 - acc: 0.5729 - f1_score: 0.5639 - val_loss: 0.7183 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 96/300\n",
      "28/28 - 2s - loss: 0.6773 - acc: 0.5744 - f1_score: 0.5646 - val_loss: 0.7189 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 97/300\n",
      "28/28 - 2s - loss: 0.6772 - acc: 0.5738 - f1_score: 0.5643 - val_loss: 0.7188 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 98/300\n",
      "28/28 - 2s - loss: 0.6771 - acc: 0.5751 - f1_score: 0.5652 - val_loss: 0.7203 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 99/300\n",
      "28/28 - 2s - loss: 0.6770 - acc: 0.5739 - f1_score: 0.5643 - val_loss: 0.7208 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 100/300\n",
      "28/28 - 2s - loss: 0.6770 - acc: 0.5741 - f1_score: 0.5648 - val_loss: 0.7197 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 101/300\n",
      "28/28 - 2s - loss: 0.6769 - acc: 0.5742 - f1_score: 0.5649 - val_loss: 0.7176 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 102/300\n",
      "28/28 - 2s - loss: 0.6768 - acc: 0.5752 - f1_score: 0.5658 - val_loss: 0.7170 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 103/300\n",
      "28/28 - 2s - loss: 0.6767 - acc: 0.5764 - f1_score: 0.5663 - val_loss: 0.7185 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 104/300\n",
      "28/28 - 2s - loss: 0.6766 - acc: 0.5765 - f1_score: 0.5665 - val_loss: 0.7193 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 105/300\n",
      "28/28 - 2s - loss: 0.6766 - acc: 0.5762 - f1_score: 0.5665 - val_loss: 0.7187 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 106/300\n",
      "28/28 - 2s - loss: 0.6765 - acc: 0.5765 - f1_score: 0.5666 - val_loss: 0.7187 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 107/300\n",
      "28/28 - 2s - loss: 0.6764 - acc: 0.5766 - f1_score: 0.5667 - val_loss: 0.7192 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 108/300\n",
      "28/28 - 2s - loss: 0.6763 - acc: 0.5762 - f1_score: 0.5662 - val_loss: 0.7196 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Epoch 109/300\n",
      "28/28 - 2s - loss: 0.6763 - acc: 0.5766 - f1_score: 0.5670 - val_loss: 0.7179 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 110/300\n",
      "28/28 - 2s - loss: 0.6762 - acc: 0.5776 - f1_score: 0.5674 - val_loss: 0.7184 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 111/300\n",
      "28/28 - 2s - loss: 0.6761 - acc: 0.5775 - f1_score: 0.5673 - val_loss: 0.7182 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 112/300\n",
      "28/28 - 2s - loss: 0.6760 - acc: 0.5775 - f1_score: 0.5671 - val_loss: 0.7188 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 113/300\n",
      "28/28 - 2s - loss: 0.6760 - acc: 0.5775 - f1_score: 0.5673 - val_loss: 0.7193 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 114/300\n",
      "28/28 - 2s - loss: 0.6759 - acc: 0.5775 - f1_score: 0.5672 - val_loss: 0.7199 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 115/300\n",
      "28/28 - 2s - loss: 0.6758 - acc: 0.5778 - f1_score: 0.5678 - val_loss: 0.7179 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 116/300\n",
      "28/28 - 2s - loss: 0.6757 - acc: 0.5775 - f1_score: 0.5672 - val_loss: 0.7172 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 117/300\n",
      "28/28 - 2s - loss: 0.6756 - acc: 0.5776 - f1_score: 0.5672 - val_loss: 0.7176 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 118/300\n",
      "28/28 - 2s - loss: 0.6755 - acc: 0.5772 - f1_score: 0.5667 - val_loss: 0.7180 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 119/300\n",
      "28/28 - 2s - loss: 0.6755 - acc: 0.5773 - f1_score: 0.5669 - val_loss: 0.7181 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 120/300\n",
      "28/28 - 2s - loss: 0.6754 - acc: 0.5779 - f1_score: 0.5675 - val_loss: 0.7168 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 121/300\n",
      "28/28 - 2s - loss: 0.6753 - acc: 0.5785 - f1_score: 0.5674 - val_loss: 0.7192 - val_acc: 0.5569 - val_f1_score: 0.4265\n",
      "Epoch 122/300\n",
      "28/28 - 2s - loss: 0.6753 - acc: 0.5778 - f1_score: 0.5672 - val_loss: 0.7181 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 123/300\n",
      "28/28 - 2s - loss: 0.6752 - acc: 0.5782 - f1_score: 0.5676 - val_loss: 0.7178 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 124/300\n",
      "28/28 - 2s - loss: 0.6751 - acc: 0.5776 - f1_score: 0.5668 - val_loss: 0.7173 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 125/300\n",
      "28/28 - 2s - loss: 0.6750 - acc: 0.5773 - f1_score: 0.5664 - val_loss: 0.7186 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 126/300\n",
      "28/28 - 2s - loss: 0.6750 - acc: 0.5782 - f1_score: 0.5675 - val_loss: 0.7187 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 127/300\n",
      "28/28 - 2s - loss: 0.6749 - acc: 0.5781 - f1_score: 0.5673 - val_loss: 0.7181 - val_acc: 0.5631 - val_f1_score: 0.4301\n",
      "Epoch 128/300\n",
      "28/28 - 2s - loss: 0.6748 - acc: 0.5792 - f1_score: 0.5684 - val_loss: 0.7183 - val_acc: 0.5600 - val_f1_score: 0.4283\n",
      "Epoch 129/300\n",
      "28/28 - 2s - loss: 0.6747 - acc: 0.5788 - f1_score: 0.5679 - val_loss: 0.7196 - val_acc: 0.5538 - val_f1_score: 0.4247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00129: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D9A0108B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15       155\n",
      "           1       0.54      1.00      0.71       170\n",
      "\n",
      "    accuracy                           0.56       325\n",
      "   macro avg       0.77      0.54      0.43       325\n",
      "weighted avg       0.76      0.56      0.44       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1688\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6923 - acc: 0.4620 - f1_score: 0.3162 - val_loss: 0.6861 - val_acc: 0.5462 - val_f1_score: 0.3775\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6920 - acc: 0.4666 - f1_score: 0.3268 - val_loss: 0.6854 - val_acc: 0.5487 - val_f1_score: 0.4366\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6917 - acc: 0.4769 - f1_score: 0.3621 - val_loss: 0.6845 - val_acc: 0.5641 - val_f1_score: 0.4811\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6914 - acc: 0.4888 - f1_score: 0.4070 - val_loss: 0.6836 - val_acc: 0.5897 - val_f1_score: 0.5270\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6911 - acc: 0.5041 - f1_score: 0.4547 - val_loss: 0.6829 - val_acc: 0.6231 - val_f1_score: 0.5799\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.5230 - f1_score: 0.4987 - val_loss: 0.6821 - val_acc: 0.6667 - val_f1_score: 0.6411\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6906 - acc: 0.5409 - f1_score: 0.5309 - val_loss: 0.6813 - val_acc: 0.6846 - val_f1_score: 0.6656\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6903 - acc: 0.5525 - f1_score: 0.5501 - val_loss: 0.6805 - val_acc: 0.6897 - val_f1_score: 0.6743\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5557 - f1_score: 0.5555 - val_loss: 0.6799 - val_acc: 0.6897 - val_f1_score: 0.6803\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6898 - acc: 0.5558 - f1_score: 0.5557 - val_loss: 0.6792 - val_acc: 0.6974 - val_f1_score: 0.6924\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6895 - acc: 0.5535 - f1_score: 0.5525 - val_loss: 0.6782 - val_acc: 0.6795 - val_f1_score: 0.6772\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6893 - acc: 0.5544 - f1_score: 0.5525 - val_loss: 0.6779 - val_acc: 0.6769 - val_f1_score: 0.6764\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5519 - f1_score: 0.5485 - val_loss: 0.6773 - val_acc: 0.6846 - val_f1_score: 0.6846\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6889 - acc: 0.5546 - f1_score: 0.5502 - val_loss: 0.6766 - val_acc: 0.6897 - val_f1_score: 0.6897\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6887 - acc: 0.5551 - f1_score: 0.5500 - val_loss: 0.6758 - val_acc: 0.6923 - val_f1_score: 0.6923\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6884 - acc: 0.5561 - f1_score: 0.5503 - val_loss: 0.6753 - val_acc: 0.6821 - val_f1_score: 0.6816\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6882 - acc: 0.5552 - f1_score: 0.5484 - val_loss: 0.6745 - val_acc: 0.6744 - val_f1_score: 0.6737\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6880 - acc: 0.5559 - f1_score: 0.5486 - val_loss: 0.6738 - val_acc: 0.6821 - val_f1_score: 0.6812\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6878 - acc: 0.5565 - f1_score: 0.5482 - val_loss: 0.6730 - val_acc: 0.6795 - val_f1_score: 0.6786\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6876 - acc: 0.5562 - f1_score: 0.5479 - val_loss: 0.6728 - val_acc: 0.6846 - val_f1_score: 0.6833\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6874 - acc: 0.5571 - f1_score: 0.5474 - val_loss: 0.6725 - val_acc: 0.6821 - val_f1_score: 0.6806\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5582 - f1_score: 0.5475 - val_loss: 0.6720 - val_acc: 0.6846 - val_f1_score: 0.6831\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6871 - acc: 0.5585 - f1_score: 0.5478 - val_loss: 0.6717 - val_acc: 0.6872 - val_f1_score: 0.6856\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5565 - f1_score: 0.5446 - val_loss: 0.6711 - val_acc: 0.6821 - val_f1_score: 0.6802\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6867 - acc: 0.5582 - f1_score: 0.5456 - val_loss: 0.6709 - val_acc: 0.6795 - val_f1_score: 0.6772\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6865 - acc: 0.5578 - f1_score: 0.5445 - val_loss: 0.6707 - val_acc: 0.6795 - val_f1_score: 0.6772\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5577 - f1_score: 0.5433 - val_loss: 0.6704 - val_acc: 0.6744 - val_f1_score: 0.6714\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6862 - acc: 0.5581 - f1_score: 0.5431 - val_loss: 0.6701 - val_acc: 0.6769 - val_f1_score: 0.6738\n",
      "Epoch 29/300\n",
      "28/28 - 2s - loss: 0.6861 - acc: 0.5578 - f1_score: 0.5424 - val_loss: 0.6698 - val_acc: 0.6744 - val_f1_score: 0.6711\n",
      "Epoch 30/300\n",
      "28/28 - 2s - loss: 0.6859 - acc: 0.5577 - f1_score: 0.5417 - val_loss: 0.6692 - val_acc: 0.6744 - val_f1_score: 0.6711\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D86AFBA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.69      0.73       232\n",
      "           1       0.61      0.70      0.65       158\n",
      "\n",
      "    accuracy                           0.70       390\n",
      "   macro avg       0.69      0.70      0.69       390\n",
      "weighted avg       0.71      0.70      0.70       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1717\n",
      "Epoch 1/300\n",
      "28/28 - 8s - loss: 0.6918 - acc: 0.4663 - f1_score: 0.3180 - val_loss: 0.6975 - val_acc: 0.4444 - val_f1_score: 0.3077\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.4719 - f1_score: 0.3316 - val_loss: 0.6983 - val_acc: 0.4573 - val_f1_score: 0.3389\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6909 - acc: 0.4813 - f1_score: 0.3644 - val_loss: 0.6991 - val_acc: 0.5385 - val_f1_score: 0.4872\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.4919 - f1_score: 0.4102 - val_loss: 0.6999 - val_acc: 0.5983 - val_f1_score: 0.5720\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6900 - acc: 0.5074 - f1_score: 0.4572 - val_loss: 0.7007 - val_acc: 0.6154 - val_f1_score: 0.5954\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6896 - acc: 0.5243 - f1_score: 0.4972 - val_loss: 0.7016 - val_acc: 0.6325 - val_f1_score: 0.6202\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6892 - acc: 0.5418 - f1_score: 0.5295 - val_loss: 0.7024 - val_acc: 0.6197 - val_f1_score: 0.6119\n",
      "Epoch 8/300\n",
      "28/28 - 2s - loss: 0.6888 - acc: 0.5537 - f1_score: 0.5487 - val_loss: 0.7033 - val_acc: 0.6197 - val_f1_score: 0.6145\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6884 - acc: 0.5549 - f1_score: 0.5539 - val_loss: 0.7042 - val_acc: 0.6111 - val_f1_score: 0.6066\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6880 - acc: 0.5556 - f1_score: 0.5554 - val_loss: 0.7051 - val_acc: 0.5983 - val_f1_score: 0.5947\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6877 - acc: 0.5574 - f1_score: 0.5573 - val_loss: 0.7059 - val_acc: 0.5897 - val_f1_score: 0.5873\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5596 - f1_score: 0.5591 - val_loss: 0.7068 - val_acc: 0.5855 - val_f1_score: 0.5838\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6870 - acc: 0.5581 - f1_score: 0.5567 - val_loss: 0.7076 - val_acc: 0.5726 - val_f1_score: 0.5715\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6867 - acc: 0.5600 - f1_score: 0.5580 - val_loss: 0.7083 - val_acc: 0.5598 - val_f1_score: 0.5594\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5603 - f1_score: 0.5574 - val_loss: 0.7090 - val_acc: 0.5556 - val_f1_score: 0.5553\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6862 - acc: 0.5606 - f1_score: 0.5571 - val_loss: 0.7097 - val_acc: 0.5427 - val_f1_score: 0.5427\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6859 - acc: 0.5628 - f1_score: 0.5587 - val_loss: 0.7103 - val_acc: 0.5256 - val_f1_score: 0.5256\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6857 - acc: 0.5630 - f1_score: 0.5580 - val_loss: 0.7108 - val_acc: 0.5256 - val_f1_score: 0.5256\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5634 - f1_score: 0.5576 - val_loss: 0.7114 - val_acc: 0.5085 - val_f1_score: 0.5081\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5640 - f1_score: 0.5576 - val_loss: 0.7118 - val_acc: 0.4957 - val_f1_score: 0.4948\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5654 - f1_score: 0.5583 - val_loss: 0.7122 - val_acc: 0.4957 - val_f1_score: 0.4948\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6847 - acc: 0.5644 - f1_score: 0.5569 - val_loss: 0.7126 - val_acc: 0.4957 - val_f1_score: 0.4939\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6845 - acc: 0.5666 - f1_score: 0.5584 - val_loss: 0.7130 - val_acc: 0.4915 - val_f1_score: 0.4894\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6843 - acc: 0.5663 - f1_score: 0.5576 - val_loss: 0.7133 - val_acc: 0.4872 - val_f1_score: 0.4848\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5663 - f1_score: 0.5572 - val_loss: 0.7136 - val_acc: 0.4786 - val_f1_score: 0.4755\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6840 - acc: 0.5663 - f1_score: 0.5568 - val_loss: 0.7138 - val_acc: 0.4786 - val_f1_score: 0.4755\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.55      0.69       172\n",
      "           1       0.41      0.85      0.55        62\n",
      "\n",
      "    accuracy                           0.63       234\n",
      "   macro avg       0.66      0.70      0.62       234\n",
      "weighted avg       0.78      0.63      0.65       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1818\n",
      "Epoch 1/300\n",
      "27/27 - 6s - loss: 0.6919 - acc: 0.4728 - f1_score: 0.3213 - val_loss: 0.6940 - val_acc: 0.3611 - val_f1_score: 0.2653\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6915 - acc: 0.4756 - f1_score: 0.3275 - val_loss: 0.6935 - val_acc: 0.3632 - val_f1_score: 0.2691\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6912 - acc: 0.4801 - f1_score: 0.3420 - val_loss: 0.6930 - val_acc: 0.3718 - val_f1_score: 0.2887\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6909 - acc: 0.4862 - f1_score: 0.3655 - val_loss: 0.6925 - val_acc: 0.4038 - val_f1_score: 0.3480\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6905 - acc: 0.4980 - f1_score: 0.4038 - val_loss: 0.6920 - val_acc: 0.4167 - val_f1_score: 0.3832\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.5054 - f1_score: 0.4372 - val_loss: 0.6915 - val_acc: 0.4231 - val_f1_score: 0.4009\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6900 - acc: 0.5164 - f1_score: 0.4655 - val_loss: 0.6910 - val_acc: 0.4423 - val_f1_score: 0.4327\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6897 - acc: 0.5308 - f1_score: 0.4982 - val_loss: 0.6905 - val_acc: 0.4637 - val_f1_score: 0.4603\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6894 - acc: 0.5457 - f1_score: 0.5262 - val_loss: 0.6901 - val_acc: 0.4957 - val_f1_score: 0.4953\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6892 - acc: 0.5547 - f1_score: 0.5431 - val_loss: 0.6896 - val_acc: 0.5171 - val_f1_score: 0.5170\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6889 - acc: 0.5570 - f1_score: 0.5513 - val_loss: 0.6891 - val_acc: 0.5150 - val_f1_score: 0.5136\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6887 - acc: 0.5551 - f1_score: 0.5524 - val_loss: 0.6887 - val_acc: 0.5235 - val_f1_score: 0.5198\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6884 - acc: 0.5599 - f1_score: 0.5588 - val_loss: 0.6882 - val_acc: 0.5299 - val_f1_score: 0.5249\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6882 - acc: 0.5616 - f1_score: 0.5614 - val_loss: 0.6878 - val_acc: 0.5385 - val_f1_score: 0.5327\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6880 - acc: 0.5611 - f1_score: 0.5611 - val_loss: 0.6874 - val_acc: 0.5556 - val_f1_score: 0.5476\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5595 - f1_score: 0.5593 - val_loss: 0.6869 - val_acc: 0.5641 - val_f1_score: 0.5547\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6876 - acc: 0.5592 - f1_score: 0.5589 - val_loss: 0.6865 - val_acc: 0.5705 - val_f1_score: 0.5604\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6874 - acc: 0.5579 - f1_score: 0.5570 - val_loss: 0.6861 - val_acc: 0.5791 - val_f1_score: 0.5673\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6873 - acc: 0.5574 - f1_score: 0.5563 - val_loss: 0.6857 - val_acc: 0.5897 - val_f1_score: 0.5767\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6871 - acc: 0.5574 - f1_score: 0.5558 - val_loss: 0.6853 - val_acc: 0.5983 - val_f1_score: 0.5843\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6869 - acc: 0.5582 - f1_score: 0.5562 - val_loss: 0.6850 - val_acc: 0.6047 - val_f1_score: 0.5899\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6868 - acc: 0.5579 - f1_score: 0.5554 - val_loss: 0.6846 - val_acc: 0.6111 - val_f1_score: 0.5955\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5585 - f1_score: 0.5555 - val_loss: 0.6842 - val_acc: 0.6111 - val_f1_score: 0.5933\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6864 - acc: 0.5576 - f1_score: 0.5543 - val_loss: 0.6838 - val_acc: 0.6154 - val_f1_score: 0.5970\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6863 - acc: 0.5577 - f1_score: 0.5539 - val_loss: 0.6834 - val_acc: 0.6154 - val_f1_score: 0.5954\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6862 - acc: 0.5579 - f1_score: 0.5535 - val_loss: 0.6830 - val_acc: 0.6218 - val_f1_score: 0.6009\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6860 - acc: 0.5573 - f1_score: 0.5527 - val_loss: 0.6826 - val_acc: 0.6303 - val_f1_score: 0.6083\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6859 - acc: 0.5589 - f1_score: 0.5537 - val_loss: 0.6823 - val_acc: 0.6303 - val_f1_score: 0.6083\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6857 - acc: 0.5605 - f1_score: 0.5549 - val_loss: 0.6819 - val_acc: 0.6325 - val_f1_score: 0.6102\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6856 - acc: 0.5598 - f1_score: 0.5535 - val_loss: 0.6816 - val_acc: 0.6346 - val_f1_score: 0.6120\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6855 - acc: 0.5592 - f1_score: 0.5526 - val_loss: 0.6812 - val_acc: 0.6368 - val_f1_score: 0.6138\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6853 - acc: 0.5598 - f1_score: 0.5531 - val_loss: 0.6809 - val_acc: 0.6389 - val_f1_score: 0.6148\n",
      "Epoch 33/300\n",
      "27/27 - 2s - loss: 0.6852 - acc: 0.5608 - f1_score: 0.5538 - val_loss: 0.6806 - val_acc: 0.6389 - val_f1_score: 0.6148\n",
      "Epoch 34/300\n",
      "27/27 - 2s - loss: 0.6851 - acc: 0.5614 - f1_score: 0.5540 - val_loss: 0.6802 - val_acc: 0.6389 - val_f1_score: 0.6148\n",
      "Epoch 35/300\n",
      "27/27 - 2s - loss: 0.6850 - acc: 0.5603 - f1_score: 0.5524 - val_loss: 0.6799 - val_acc: 0.6368 - val_f1_score: 0.6121\n",
      "Epoch 36/300\n",
      "27/27 - 2s - loss: 0.6848 - acc: 0.5600 - f1_score: 0.5519 - val_loss: 0.6796 - val_acc: 0.6368 - val_f1_score: 0.6121\n",
      "Epoch 37/300\n",
      "27/27 - 2s - loss: 0.6847 - acc: 0.5615 - f1_score: 0.5528 - val_loss: 0.6793 - val_acc: 0.6389 - val_f1_score: 0.6139\n",
      "Epoch 38/300\n",
      "27/27 - 2s - loss: 0.6846 - acc: 0.5605 - f1_score: 0.5516 - val_loss: 0.6790 - val_acc: 0.6389 - val_f1_score: 0.6139\n",
      "Epoch 39/300\n",
      "27/27 - 2s - loss: 0.6845 - acc: 0.5614 - f1_score: 0.5522 - val_loss: 0.6786 - val_acc: 0.6389 - val_f1_score: 0.6130\n",
      "Epoch 40/300\n",
      "27/27 - 2s - loss: 0.6844 - acc: 0.5605 - f1_score: 0.5509 - val_loss: 0.6784 - val_acc: 0.6389 - val_f1_score: 0.6121\n",
      "Epoch 41/300\n",
      "27/27 - 2s - loss: 0.6843 - acc: 0.5602 - f1_score: 0.5500 - val_loss: 0.6781 - val_acc: 0.6389 - val_f1_score: 0.6121\n",
      "Epoch 42/300\n",
      "27/27 - 2s - loss: 0.6842 - acc: 0.5599 - f1_score: 0.5498 - val_loss: 0.6778 - val_acc: 0.6368 - val_f1_score: 0.6093\n",
      "Epoch 43/300\n",
      "27/27 - 2s - loss: 0.6841 - acc: 0.5595 - f1_score: 0.5490 - val_loss: 0.6775 - val_acc: 0.6346 - val_f1_score: 0.6056\n",
      "Epoch 44/300\n",
      "27/27 - 2s - loss: 0.6839 - acc: 0.5596 - f1_score: 0.5486 - val_loss: 0.6772 - val_acc: 0.6325 - val_f1_score: 0.6028\n",
      "Epoch 45/300\n",
      "27/27 - 2s - loss: 0.6838 - acc: 0.5606 - f1_score: 0.5497 - val_loss: 0.6769 - val_acc: 0.6346 - val_f1_score: 0.6046\n",
      "Epoch 46/300\n",
      "27/27 - 2s - loss: 0.6837 - acc: 0.5595 - f1_score: 0.5482 - val_loss: 0.6766 - val_acc: 0.6346 - val_f1_score: 0.6046\n",
      "Epoch 47/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5590 - f1_score: 0.5473 - val_loss: 0.6764 - val_acc: 0.6368 - val_f1_score: 0.6064\n",
      "Epoch 48/300\n",
      "27/27 - 2s - loss: 0.6835 - acc: 0.5585 - f1_score: 0.5466 - val_loss: 0.6761 - val_acc: 0.6368 - val_f1_score: 0.6064\n",
      "Epoch 49/300\n",
      "27/27 - 2s - loss: 0.6834 - acc: 0.5585 - f1_score: 0.5461 - val_loss: 0.6758 - val_acc: 0.6368 - val_f1_score: 0.6064\n",
      "Epoch 50/300\n",
      "27/27 - 2s - loss: 0.6833 - acc: 0.5595 - f1_score: 0.5468 - val_loss: 0.6756 - val_acc: 0.6368 - val_f1_score: 0.6064\n",
      "Epoch 51/300\n",
      "27/27 - 2s - loss: 0.6832 - acc: 0.5592 - f1_score: 0.5465 - val_loss: 0.6753 - val_acc: 0.6389 - val_f1_score: 0.6072\n",
      "Epoch 52/300\n",
      "27/27 - 2s - loss: 0.6831 - acc: 0.5606 - f1_score: 0.5470 - val_loss: 0.6751 - val_acc: 0.6389 - val_f1_score: 0.6072\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00052: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D975D48B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52       182\n",
      "           1       0.70      0.73      0.71       286\n",
      "\n",
      "    accuracy                           0.64       468\n",
      "   macro avg       0.62      0.61      0.61       468\n",
      "weighted avg       0.63      0.64      0.64       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1892\n",
      "Epoch 1/300\n",
      "27/27 - 5s - loss: 0.6932 - acc: 0.4320 - f1_score: 0.3027 - val_loss: 0.6759 - val_acc: 0.9722 - val_f1_score: 0.4930\n",
      "Epoch 2/300\n",
      "27/27 - 1s - loss: 0.6928 - acc: 0.4400 - f1_score: 0.3261 - val_loss: 0.6772 - val_acc: 0.9658 - val_f1_score: 0.4913\n",
      "Epoch 3/300\n",
      "27/27 - 1s - loss: 0.6924 - acc: 0.4597 - f1_score: 0.3837 - val_loss: 0.6785 - val_acc: 0.9380 - val_f1_score: 0.5162\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6920 - acc: 0.4921 - f1_score: 0.4603 - val_loss: 0.6797 - val_acc: 0.8162 - val_f1_score: 0.4915\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6916 - acc: 0.5357 - f1_score: 0.5302 - val_loss: 0.6809 - val_acc: 0.6816 - val_f1_score: 0.4411\n",
      "Epoch 6/300\n",
      "27/27 - 1s - loss: 0.6913 - acc: 0.5580 - f1_score: 0.5580 - val_loss: 0.6820 - val_acc: 0.5748 - val_f1_score: 0.3952\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6909 - acc: 0.5595 - f1_score: 0.5572 - val_loss: 0.6832 - val_acc: 0.5256 - val_f1_score: 0.3780\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6906 - acc: 0.5625 - f1_score: 0.5556 - val_loss: 0.6844 - val_acc: 0.4893 - val_f1_score: 0.3621\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.5658 - f1_score: 0.5547 - val_loss: 0.6855 - val_acc: 0.4679 - val_f1_score: 0.3534\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6899 - acc: 0.5687 - f1_score: 0.5540 - val_loss: 0.6866 - val_acc: 0.4380 - val_f1_score: 0.3392\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6895 - acc: 0.5724 - f1_score: 0.5544 - val_loss: 0.6877 - val_acc: 0.4274 - val_f1_score: 0.3355\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6892 - acc: 0.5758 - f1_score: 0.5552 - val_loss: 0.6887 - val_acc: 0.4188 - val_f1_score: 0.3305\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6888 - acc: 0.5763 - f1_score: 0.5535 - val_loss: 0.6898 - val_acc: 0.4038 - val_f1_score: 0.3215\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6885 - acc: 0.5767 - f1_score: 0.5514 - val_loss: 0.6908 - val_acc: 0.3846 - val_f1_score: 0.3099\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6882 - acc: 0.5792 - f1_score: 0.5520 - val_loss: 0.6918 - val_acc: 0.3739 - val_f1_score: 0.3033\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5812 - f1_score: 0.5524 - val_loss: 0.6927 - val_acc: 0.3568 - val_f1_score: 0.2926\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6875 - acc: 0.5824 - f1_score: 0.5528 - val_loss: 0.6937 - val_acc: 0.3504 - val_f1_score: 0.2886\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6872 - acc: 0.5824 - f1_score: 0.5503 - val_loss: 0.6946 - val_acc: 0.3419 - val_f1_score: 0.2831\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6869 - acc: 0.5834 - f1_score: 0.5509 - val_loss: 0.6955 - val_acc: 0.3333 - val_f1_score: 0.2776\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5847 - f1_score: 0.5508 - val_loss: 0.6964 - val_acc: 0.3312 - val_f1_score: 0.2762\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6863 - acc: 0.5845 - f1_score: 0.5497 - val_loss: 0.6973 - val_acc: 0.3248 - val_f1_score: 0.2720\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6859 - acc: 0.5848 - f1_score: 0.5491 - val_loss: 0.6983 - val_acc: 0.3226 - val_f1_score: 0.2706\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6856 - acc: 0.5868 - f1_score: 0.5502 - val_loss: 0.6992 - val_acc: 0.3162 - val_f1_score: 0.2664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       450\n",
      "           1       0.08      0.06      0.06        18\n",
      "\n",
      "    accuracy                           0.94       468\n",
      "   macro avg       0.52      0.51      0.52       468\n",
      "weighted avg       0.93      0.94      0.93       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1929\n",
      "Epoch 1/300\n",
      "27/27 - 5s - loss: 0.6913 - acc: 0.4821 - f1_score: 0.3253 - val_loss: 0.7027 - val_acc: 0.2244 - val_f1_score: 0.1849\n",
      "Epoch 2/300\n",
      "27/27 - 2s - loss: 0.6907 - acc: 0.4830 - f1_score: 0.3272 - val_loss: 0.7043 - val_acc: 0.2350 - val_f1_score: 0.2000\n",
      "Epoch 3/300\n",
      "27/27 - 2s - loss: 0.6902 - acc: 0.4844 - f1_score: 0.3312 - val_loss: 0.7059 - val_acc: 0.2863 - val_f1_score: 0.2680\n",
      "Epoch 4/300\n",
      "27/27 - 2s - loss: 0.6897 - acc: 0.4862 - f1_score: 0.3408 - val_loss: 0.7075 - val_acc: 0.3162 - val_f1_score: 0.3048\n",
      "Epoch 5/300\n",
      "27/27 - 2s - loss: 0.6892 - acc: 0.4899 - f1_score: 0.3609 - val_loss: 0.7093 - val_acc: 0.3184 - val_f1_score: 0.3114\n",
      "Epoch 6/300\n",
      "27/27 - 2s - loss: 0.6887 - acc: 0.4969 - f1_score: 0.3880 - val_loss: 0.7111 - val_acc: 0.3419 - val_f1_score: 0.3401\n",
      "Epoch 7/300\n",
      "27/27 - 2s - loss: 0.6882 - acc: 0.5040 - f1_score: 0.4131 - val_loss: 0.7129 - val_acc: 0.3718 - val_f1_score: 0.3718\n",
      "Epoch 8/300\n",
      "27/27 - 2s - loss: 0.6878 - acc: 0.5078 - f1_score: 0.4331 - val_loss: 0.7147 - val_acc: 0.4252 - val_f1_score: 0.4233\n",
      "Epoch 9/300\n",
      "27/27 - 2s - loss: 0.6873 - acc: 0.5166 - f1_score: 0.4603 - val_loss: 0.7166 - val_acc: 0.4679 - val_f1_score: 0.4621\n",
      "Epoch 10/300\n",
      "27/27 - 2s - loss: 0.6869 - acc: 0.5275 - f1_score: 0.4861 - val_loss: 0.7184 - val_acc: 0.4872 - val_f1_score: 0.4761\n",
      "Epoch 11/300\n",
      "27/27 - 2s - loss: 0.6866 - acc: 0.5348 - f1_score: 0.5057 - val_loss: 0.7201 - val_acc: 0.5128 - val_f1_score: 0.4914\n",
      "Epoch 12/300\n",
      "27/27 - 2s - loss: 0.6862 - acc: 0.5464 - f1_score: 0.5278 - val_loss: 0.7217 - val_acc: 0.5449 - val_f1_score: 0.5123\n",
      "Epoch 13/300\n",
      "27/27 - 2s - loss: 0.6859 - acc: 0.5538 - f1_score: 0.5429 - val_loss: 0.7232 - val_acc: 0.5235 - val_f1_score: 0.4713\n",
      "Epoch 14/300\n",
      "27/27 - 2s - loss: 0.6856 - acc: 0.5556 - f1_score: 0.5491 - val_loss: 0.7246 - val_acc: 0.5064 - val_f1_score: 0.4402\n",
      "Epoch 15/300\n",
      "27/27 - 2s - loss: 0.6853 - acc: 0.5570 - f1_score: 0.5536 - val_loss: 0.7258 - val_acc: 0.5085 - val_f1_score: 0.4416\n",
      "Epoch 16/300\n",
      "27/27 - 2s - loss: 0.6851 - acc: 0.5624 - f1_score: 0.5608 - val_loss: 0.7269 - val_acc: 0.5085 - val_f1_score: 0.4397\n",
      "Epoch 17/300\n",
      "27/27 - 2s - loss: 0.6848 - acc: 0.5605 - f1_score: 0.5597 - val_loss: 0.7277 - val_acc: 0.5064 - val_f1_score: 0.4363\n",
      "Epoch 18/300\n",
      "27/27 - 2s - loss: 0.6846 - acc: 0.5629 - f1_score: 0.5629 - val_loss: 0.7286 - val_acc: 0.5107 - val_f1_score: 0.4393\n",
      "Epoch 19/300\n",
      "27/27 - 2s - loss: 0.6844 - acc: 0.5632 - f1_score: 0.5632 - val_loss: 0.7294 - val_acc: 0.5107 - val_f1_score: 0.4373\n",
      "Epoch 20/300\n",
      "27/27 - 2s - loss: 0.6842 - acc: 0.5614 - f1_score: 0.5613 - val_loss: 0.7301 - val_acc: 0.5107 - val_f1_score: 0.4373\n",
      "Epoch 21/300\n",
      "27/27 - 2s - loss: 0.6840 - acc: 0.5602 - f1_score: 0.5599 - val_loss: 0.7306 - val_acc: 0.5107 - val_f1_score: 0.4373\n",
      "Epoch 22/300\n",
      "27/27 - 2s - loss: 0.6838 - acc: 0.5608 - f1_score: 0.5602 - val_loss: 0.7313 - val_acc: 0.5107 - val_f1_score: 0.4353\n",
      "Epoch 23/300\n",
      "27/27 - 2s - loss: 0.6836 - acc: 0.5616 - f1_score: 0.5608 - val_loss: 0.7317 - val_acc: 0.5107 - val_f1_score: 0.4353\n",
      "Epoch 24/300\n",
      "27/27 - 2s - loss: 0.6835 - acc: 0.5634 - f1_score: 0.5622 - val_loss: 0.7317 - val_acc: 0.5107 - val_f1_score: 0.4332\n",
      "Epoch 25/300\n",
      "27/27 - 2s - loss: 0.6833 - acc: 0.5653 - f1_score: 0.5636 - val_loss: 0.7319 - val_acc: 0.5107 - val_f1_score: 0.4332\n",
      "Epoch 26/300\n",
      "27/27 - 2s - loss: 0.6831 - acc: 0.5654 - f1_score: 0.5631 - val_loss: 0.7321 - val_acc: 0.5107 - val_f1_score: 0.4332\n",
      "Epoch 27/300\n",
      "27/27 - 2s - loss: 0.6830 - acc: 0.5642 - f1_score: 0.5617 - val_loss: 0.7319 - val_acc: 0.5128 - val_f1_score: 0.4347\n",
      "Epoch 28/300\n",
      "27/27 - 2s - loss: 0.6828 - acc: 0.5647 - f1_score: 0.5616 - val_loss: 0.7318 - val_acc: 0.5150 - val_f1_score: 0.4361\n",
      "Epoch 29/300\n",
      "27/27 - 2s - loss: 0.6827 - acc: 0.5671 - f1_score: 0.5636 - val_loss: 0.7318 - val_acc: 0.5128 - val_f1_score: 0.4326\n",
      "Epoch 30/300\n",
      "27/27 - 2s - loss: 0.6825 - acc: 0.5669 - f1_score: 0.5630 - val_loss: 0.7321 - val_acc: 0.5128 - val_f1_score: 0.4326\n",
      "Epoch 31/300\n",
      "27/27 - 2s - loss: 0.6824 - acc: 0.5660 - f1_score: 0.5616 - val_loss: 0.7325 - val_acc: 0.5107 - val_f1_score: 0.4290\n",
      "Epoch 32/300\n",
      "27/27 - 2s - loss: 0.6822 - acc: 0.5666 - f1_score: 0.5622 - val_loss: 0.7321 - val_acc: 0.5128 - val_f1_score: 0.4304\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.28      0.39       243\n",
      "           1       0.52      0.84      0.64       225\n",
      "\n",
      "    accuracy                           0.54       468\n",
      "   macro avg       0.58      0.56      0.51       468\n",
      "weighted avg       0.58      0.54      0.51       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 3)\n",
      "Testing on 1933\n",
      "Epoch 1/300\n",
      "28/28 - 6s - loss: 0.6917 - acc: 0.4701 - f1_score: 0.3200 - val_loss: 0.6987 - val_acc: 0.3333 - val_f1_score: 0.2500\n",
      "Epoch 2/300\n",
      "28/28 - 2s - loss: 0.6913 - acc: 0.4775 - f1_score: 0.3387 - val_loss: 0.6991 - val_acc: 0.3419 - val_f1_score: 0.2643\n",
      "Epoch 3/300\n",
      "28/28 - 2s - loss: 0.6908 - acc: 0.4876 - f1_score: 0.3791 - val_loss: 0.6995 - val_acc: 0.3376 - val_f1_score: 0.2662\n",
      "Epoch 4/300\n",
      "28/28 - 2s - loss: 0.6904 - acc: 0.4991 - f1_score: 0.4277 - val_loss: 0.7000 - val_acc: 0.3504 - val_f1_score: 0.3047\n",
      "Epoch 5/300\n",
      "28/28 - 2s - loss: 0.6899 - acc: 0.5188 - f1_score: 0.4767 - val_loss: 0.7004 - val_acc: 0.3718 - val_f1_score: 0.3454\n",
      "Epoch 6/300\n",
      "28/28 - 2s - loss: 0.6895 - acc: 0.5404 - f1_score: 0.5203 - val_loss: 0.7009 - val_acc: 0.4658 - val_f1_score: 0.4653\n",
      "Epoch 7/300\n",
      "28/28 - 2s - loss: 0.6891 - acc: 0.5522 - f1_score: 0.5432 - val_loss: 0.7015 - val_acc: 0.4786 - val_f1_score: 0.4785\n",
      "Epoch 8/300\n",
      "28/28 - 5s - loss: 0.6887 - acc: 0.5554 - f1_score: 0.5522 - val_loss: 0.7020 - val_acc: 0.4872 - val_f1_score: 0.4826\n",
      "Epoch 9/300\n",
      "28/28 - 2s - loss: 0.6883 - acc: 0.5599 - f1_score: 0.5593 - val_loss: 0.7026 - val_acc: 0.4786 - val_f1_score: 0.4699\n",
      "Epoch 10/300\n",
      "28/28 - 2s - loss: 0.6879 - acc: 0.5641 - f1_score: 0.5641 - val_loss: 0.7032 - val_acc: 0.4487 - val_f1_score: 0.4330\n",
      "Epoch 11/300\n",
      "28/28 - 2s - loss: 0.6876 - acc: 0.5624 - f1_score: 0.5623 - val_loss: 0.7037 - val_acc: 0.4487 - val_f1_score: 0.4330\n",
      "Epoch 12/300\n",
      "28/28 - 2s - loss: 0.6873 - acc: 0.5640 - f1_score: 0.5635 - val_loss: 0.7043 - val_acc: 0.4359 - val_f1_score: 0.4171\n",
      "Epoch 13/300\n",
      "28/28 - 2s - loss: 0.6869 - acc: 0.5628 - f1_score: 0.5616 - val_loss: 0.7048 - val_acc: 0.4444 - val_f1_score: 0.4221\n",
      "Epoch 14/300\n",
      "28/28 - 2s - loss: 0.6867 - acc: 0.5624 - f1_score: 0.5604 - val_loss: 0.7052 - val_acc: 0.4444 - val_f1_score: 0.4221\n",
      "Epoch 15/300\n",
      "28/28 - 2s - loss: 0.6864 - acc: 0.5638 - f1_score: 0.5611 - val_loss: 0.7056 - val_acc: 0.4444 - val_f1_score: 0.4200\n",
      "Epoch 16/300\n",
      "28/28 - 2s - loss: 0.6861 - acc: 0.5642 - f1_score: 0.5609 - val_loss: 0.7060 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 17/300\n",
      "28/28 - 2s - loss: 0.6859 - acc: 0.5654 - f1_score: 0.5614 - val_loss: 0.7063 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 18/300\n",
      "28/28 - 2s - loss: 0.6856 - acc: 0.5645 - f1_score: 0.5598 - val_loss: 0.7066 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 19/300\n",
      "28/28 - 2s - loss: 0.6854 - acc: 0.5659 - f1_score: 0.5606 - val_loss: 0.7068 - val_acc: 0.4402 - val_f1_score: 0.4145\n",
      "Epoch 20/300\n",
      "28/28 - 2s - loss: 0.6852 - acc: 0.5669 - f1_score: 0.5612 - val_loss: 0.7070 - val_acc: 0.4487 - val_f1_score: 0.4212\n",
      "Epoch 21/300\n",
      "28/28 - 2s - loss: 0.6850 - acc: 0.5670 - f1_score: 0.5608 - val_loss: 0.7071 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 22/300\n",
      "28/28 - 2s - loss: 0.6848 - acc: 0.5679 - f1_score: 0.5612 - val_loss: 0.7070 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 23/300\n",
      "28/28 - 2s - loss: 0.6846 - acc: 0.5677 - f1_score: 0.5603 - val_loss: 0.7071 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 24/300\n",
      "28/28 - 2s - loss: 0.6844 - acc: 0.5677 - f1_score: 0.5597 - val_loss: 0.7072 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 25/300\n",
      "28/28 - 2s - loss: 0.6842 - acc: 0.5670 - f1_score: 0.5587 - val_loss: 0.7071 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 26/300\n",
      "28/28 - 2s - loss: 0.6841 - acc: 0.5675 - f1_score: 0.5586 - val_loss: 0.7070 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 27/300\n",
      "28/28 - 2s - loss: 0.6839 - acc: 0.5669 - f1_score: 0.5574 - val_loss: 0.7070 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Epoch 28/300\n",
      "28/28 - 2s - loss: 0.6837 - acc: 0.5679 - f1_score: 0.5579 - val_loss: 0.7068 - val_acc: 0.4444 - val_f1_score: 0.4156\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.34      0.43       134\n",
      "           1       0.44      0.68      0.53       100\n",
      "\n",
      "    accuracy                           0.49       234\n",
      "   macro avg       0.51      0.51      0.48       234\n",
      "weighted avg       0.52      0.49      0.48       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-134518\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Ablation eda\n",
      "Average Accuracy:  0.6594777062018441\n",
      "F1 score for Baseline:  0.5655243280667068\n",
      "F1 score for Stress:  0.570667615615759\n",
      "Macro F1:  0.568095971841233\n",
      "Weighted F1:  0.6431575318550505\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "# opt = Adam(learning_rate = 0.001)\n",
    "# model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['FF']\n",
    "attx_st = ['FF']\n",
    "modType = ['ecg', 'eda']\n",
    "\n",
    "\n",
    "for mod in modType:\n",
    "\n",
    "    main_path = r\"X:\\Thesis\\matb2\\Processed_Data\"\n",
    "    with open(os.path.join(main_path, 'cola_labels.pickle'), 'rb') as handle:\n",
    "        sub_label = pickle.load(handle)\n",
    "\n",
    "    if mod == 'ecg':\n",
    "        ipShape = [(2560, 1)]\n",
    "        with open(os.path.join(main_path, 'cola_ecg.pickle'), 'rb') as handle:\n",
    "            sub_dict = pickle.load(handle)\n",
    "    else:\n",
    "        ipShape = [(2560, 3)]\n",
    "        with open(os.path.join(main_path, 'cola_eda.pickle'), 'rb') as handle:\n",
    "            sub_dict = pickle.load(handle)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Training for Modality {}\".format(mod))\n",
    "    print(\"--------------------------------------------------------------------------\\n\")        \n",
    "    \n",
    "    hs, preds, clr = {}, {}, {}\n",
    "\n",
    "    path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "    tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "    for i in sub_dict.keys():\n",
    "\n",
    "        if i in ['1765']:\n",
    "            continue\n",
    "\n",
    "        opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "        tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "        X_test = sub_dict[i]\n",
    "        y_test = sub_label[i]\n",
    "\n",
    "        X_test = vstack(X_test)\n",
    "        y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "        X = [vstack(v) for k, v in sub_dict.items() if k != i]\n",
    "        y_train = [hstack(np.asarray(v)) for k, v in sub_label.items() if k != i]\n",
    "\n",
    "        X = vstack(X)\n",
    "        y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "        y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "        y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "        \n",
    "        y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "        y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "        callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                        patience=20, verbose=1, mode='max', \n",
    "                                                        restore_best_weights=True)\n",
    "\n",
    "        class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "        model = mega_model_ecg(input_shape=ipShape, classes = num_classes)\n",
    "        mod_1 = inspect.getsource(mega_model_ecg)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "        print('Testing on {}'.format(i))\n",
    "\n",
    "        hist = model.fit([X], y, epochs=300, verbose=2, shuffle=True,\n",
    "                        batch_size = 256, validation_data = ([X_test], y_test),\n",
    "                        callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "        y_pred_i = model.predict([X_test], batch_size = 128)\n",
    "\n",
    "        pred_list = list()\n",
    "        test_y = list()\n",
    "\n",
    "        for n in range(len(y_pred_i)):\n",
    "            pred_list.append(np.argmax(y_pred_i[n]))\n",
    "            test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        print(classification_report(pred_list, test_y))\n",
    "        a = classification_report(pred_list, test_y,\n",
    "                                target_names = ['Baseline', 'Stress'],\n",
    "                                output_dict=True)\n",
    "\n",
    "        clr[i] = a\n",
    "        hs[i] = hist\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "        scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                    'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "        model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "        model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "        model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "        with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "        K.clear_session()\n",
    "        \n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print('Classfication report for Ablation {}'.format(mod))    \n",
    "    score_class(clr)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = 'experiment_20220607-132454'\n",
    "eda = 'experiment_20220607-134518'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_a (Conv1D)  (None, 366, 32)      64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_a (Conv1D)  (None, 366, 32)      128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_a (BatchNorm (None, 366, 32)      128         conv_res_stage1_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_a (BatchNorm (None, 366, 32)      128         conv_res_stage1_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 366, 32)      0           conv_bn_stage1_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 366, 32)      0           conv_bn_stage1_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_b (Conv1D)  (None, 366, 32)      65568       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_b (Conv1D)  (None, 366, 32)      65568       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_b (BatchNorm (None, 366, 32)      128         conv_res_stage1_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_b (BatchNorm (None, 366, 32)      128         conv_res_stage1_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 366, 32)      0           conv_bn_stage1_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 366, 32)      0           conv_bn_stage1_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_c (Conv1D)  (None, 366, 64)      2112        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_short (Conv (None, 366, 64)      128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_c (Conv1D)  (None, 366, 64)      2112        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_short (Conv (None, 366, 64)      256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_c (BatchNorm (None, 366, 64)      256         conv_res_stage1_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_short (Batch (None, 366, 64)      256         conv_res_stage1_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_c (BatchNorm (None, 366, 64)      256         conv_res_stage1_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_short (Batch (None, 366, 64)      256         conv_res_stage1_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 366, 64)      0           conv_bn_stage1_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage1_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 366, 64)      0           conv_bn_stage1_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage1_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 366, 64)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 366, 64)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_a (Conv1D)  (None, 366, 32)      2080        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_a (Conv1D)  (None, 366, 32)      2080        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_a (BatchNorm (None, 366, 32)      128         iden_res_stage1_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_a (BatchNorm (None, 366, 32)      128         iden_res_stage1_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 366, 32)      0           iden_bn_stage1_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 366, 32)      0           iden_bn_stage1_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_b (Conv1D)  (None, 366, 32)      65568       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_b (Conv1D)  (None, 366, 32)      65568       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_b (BatchNorm (None, 366, 32)      128         iden_res_stage1_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_b (BatchNorm (None, 366, 32)      128         iden_res_stage1_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 366, 32)      0           iden_bn_stage1_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 366, 32)      0           iden_bn_stage1_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_c (Conv1D)  (None, 366, 64)      2112        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_c (Conv1D)  (None, 366, 64)      2112        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_c (BatchNorm (None, 366, 64)      256         iden_res_stage1_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_c (BatchNorm (None, 366, 64)      256         iden_res_stage1_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 366, 64)      0           iden_bn_stage1_ecg_c[0][0]       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 366, 64)      0           iden_bn_stage1_eda_c[0][0]       \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 366, 64)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 366, 64)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_a (Conv1D)  (None, 122, 64)      4160        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_a (Conv1D)  (None, 122, 64)      4160        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_a (BatchNorm (None, 122, 64)      256         conv_res_stage2_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_a (BatchNorm (None, 122, 64)      256         conv_res_stage2_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 122, 64)      0           conv_bn_stage2_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 122, 64)      0           conv_bn_stage2_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_b (Conv1D)  (None, 122, 64)      131136      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_b (Conv1D)  (None, 122, 64)      131136      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_b (BatchNorm (None, 122, 64)      256         conv_res_stage2_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_b (BatchNorm (None, 122, 64)      256         conv_res_stage2_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 122, 64)      0           conv_bn_stage2_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 122, 64)      0           conv_bn_stage2_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_c (Conv1D)  (None, 122, 128)     8320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_short (Conv (None, 122, 128)     8320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_c (Conv1D)  (None, 122, 128)     8320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_short (Conv (None, 122, 128)     8320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_c (BatchNorm (None, 122, 128)     512         conv_res_stage2_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_short (Batch (None, 122, 128)     512         conv_res_stage2_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_c (BatchNorm (None, 122, 128)     512         conv_res_stage2_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_short (Batch (None, 122, 128)     512         conv_res_stage2_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 122, 128)     0           conv_bn_stage2_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage2_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 122, 128)     0           conv_bn_stage2_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage2_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 122, 128)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 122, 128)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_a (Conv1D)  (None, 122, 64)      8256        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_a (Conv1D)  (None, 122, 64)      8256        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_a (BatchNorm (None, 122, 64)      256         iden_res_stage2_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_a (BatchNorm (None, 122, 64)      256         iden_res_stage2_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 122, 64)      0           iden_bn_stage2_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 122, 64)      0           iden_bn_stage2_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_b (Conv1D)  (None, 122, 64)      131136      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_b (Conv1D)  (None, 122, 64)      131136      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_b (BatchNorm (None, 122, 64)      256         iden_res_stage2_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_b (BatchNorm (None, 122, 64)      256         iden_res_stage2_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 122, 64)      0           iden_bn_stage2_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 122, 64)      0           iden_bn_stage2_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_c (Conv1D)  (None, 122, 128)     8320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_c (Conv1D)  (None, 122, 128)     8320        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_c (BatchNorm (None, 122, 128)     512         iden_res_stage2_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_c (BatchNorm (None, 122, 128)     512         iden_res_stage2_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 122, 128)     0           iden_bn_stage2_ecg_c[0][0]       \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 122, 128)     0           iden_bn_stage2_eda_c[0][0]       \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 122, 128)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 122, 128)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 122, 1, 128)  0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 122, 1, 128)  0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 122, 2, 128)  0           tf.expand_dims[0][0]             \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 122, 128, 2)  0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 122, 128, 2)  6           tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 122, 128, 2)  0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 122, 2, 128)  0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Attention_laye ((None, 122, 1), (No 128         tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 122, 128)     0           activation_23[0][0]              \n",
      "                                                                 attention_layer[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 122, 256)     0           activation_17[0][0]              \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_a (Conv1D)  (None, 41, 128)      32896       tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_a (Conv1D)  (None, 41, 128)      16512       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_a (BatchNorm (None, 41, 128)      512         conv_res_stage3_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_a (BatchNorm (None, 41, 128)      512         conv_res_stage3_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 41, 128)      0           conv_bn_stage3_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 41, 128)      0           conv_bn_stage3_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_b (Conv1D)  (None, 41, 128)      278656      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_b (Conv1D)  (None, 41, 128)      278656      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_b (BatchNorm (None, 41, 128)      512         conv_res_stage3_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_b (BatchNorm (None, 41, 128)      512         conv_res_stage3_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 41, 128)      0           conv_bn_stage3_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 41, 128)      0           conv_bn_stage3_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_c (Conv1D)  (None, 41, 256)      33024       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_short (Conv (None, 41, 256)      65792       tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_c (Conv1D)  (None, 41, 256)      33024       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_short (Conv (None, 41, 256)      33024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_c (BatchNorm (None, 41, 256)      1024        conv_res_stage3_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_short (Batch (None, 41, 256)      1024        conv_res_stage3_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_c (BatchNorm (None, 41, 256)      1024        conv_res_stage3_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_short (Batch (None, 41, 256)      1024        conv_res_stage3_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 41, 256)      0           conv_bn_stage3_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage3_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 41, 256)      0           conv_bn_stage3_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage3_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 41, 256)      0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 41, 256)      0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_a (Conv1D)  (None, 41, 128)      32896       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_a (Conv1D)  (None, 41, 128)      32896       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_a (BatchNorm (None, 41, 128)      512         iden_res_stage3_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_a (BatchNorm (None, 41, 128)      512         iden_res_stage3_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 41, 128)      0           iden_bn_stage3_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 41, 128)      0           iden_bn_stage3_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_b (Conv1D)  (None, 41, 128)      278656      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_b (Conv1D)  (None, 41, 128)      278656      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_b (BatchNorm (None, 41, 128)      512         iden_res_stage3_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_b (BatchNorm (None, 41, 128)      512         iden_res_stage3_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 41, 128)      0           iden_bn_stage3_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 41, 128)      0           iden_bn_stage3_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_c (Conv1D)  (None, 41, 256)      33024       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_c (Conv1D)  (None, 41, 256)      33024       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_c (BatchNorm (None, 41, 256)      1024        iden_res_stage3_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_c (BatchNorm (None, 41, 256)      1024        iden_res_stage3_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 41, 256)      0           iden_bn_stage3_ecg_c[0][0]       \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 41, 256)      0           iden_bn_stage3_eda_c[0][0]       \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 41, 256)      0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 41, 256)      0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 41, 1, 256)   0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 41, 1, 256)   0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 41, 2, 256)   0           tf.expand_dims_2[0][0]           \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 41, 256, 2)   0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 41, 256, 2)   6           tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 41, 256, 2)   0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 41, 2, 256)   0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (Attention_la ((None, 41, 1), (Non 256         tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 41, 256)      0           activation_35[0][0]              \n",
      "                                                                 attention_layer_1[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (None, 41, 512)      0           activation_29[0][0]              \n",
      "                                                                 tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_a (Conv1D)  (None, 14, 256)      131328      tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_a (Conv1D)  (None, 14, 256)      65792       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_a (BatchNorm (None, 14, 256)      1024        conv_res_stage4_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_a (BatchNorm (None, 14, 256)      1024        conv_res_stage4_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 256)      0           conv_bn_stage4_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 256)      0           conv_bn_stage4_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_b (Conv1D)  (None, 14, 256)      459008      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_b (Conv1D)  (None, 14, 256)      459008      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_b (BatchNorm (None, 14, 256)      1024        conv_res_stage4_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_b (BatchNorm (None, 14, 256)      1024        conv_res_stage4_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 256)      0           conv_bn_stage4_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 256)      0           conv_bn_stage4_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_c (Conv1D)  (None, 14, 512)      131584      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_short (Conv (None, 14, 512)      262656      tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_c (Conv1D)  (None, 14, 512)      131584      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_short (Conv (None, 14, 512)      131584      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_c (BatchNorm (None, 14, 512)      2048        conv_res_stage4_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_short (Batch (None, 14, 512)      2048        conv_res_stage4_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_c (BatchNorm (None, 14, 512)      2048        conv_res_stage4_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_short (Batch (None, 14, 512)      2048        conv_res_stage4_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 512)      0           conv_bn_stage4_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage4_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 14, 512)      0           conv_bn_stage4_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage4_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 512)      0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 512)      0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_a (Conv1D)  (None, 14, 256)      131328      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_a (Conv1D)  (None, 14, 256)      131328      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_a (BatchNorm (None, 14, 256)      1024        iden_res_stage4_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_a (BatchNorm (None, 14, 256)      1024        iden_res_stage4_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 256)      0           iden_bn_stage4_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 256)      0           iden_bn_stage4_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_b (Conv1D)  (None, 14, 256)      459008      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_b (Conv1D)  (None, 14, 256)      459008      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_b (BatchNorm (None, 14, 256)      1024        iden_res_stage4_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_b (BatchNorm (None, 14, 256)      1024        iden_res_stage4_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 256)      0           iden_bn_stage4_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 256)      0           iden_bn_stage4_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_c (Conv1D)  (None, 14, 512)      131584      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_c (Conv1D)  (None, 14, 512)      131584      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_c (BatchNorm (None, 14, 512)      2048        iden_res_stage4_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_c (BatchNorm (None, 14, 512)      2048        iden_res_stage4_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 512)      0           iden_bn_stage4_ecg_c[0][0]       \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 14, 512)      0           iden_bn_stage4_eda_c[0][0]       \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 512)      0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 512)      0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 7168)         0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7168)         0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         7341056     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         7341056     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 256)          262400      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 256)          262400      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            1026        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 20,798,606\n",
      "Trainable params: 20,779,406\n",
      "Non-trainable params: 19,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type II, Stage two_three\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-152134\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1105\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 68s - loss: 14.4078 - acc: 0.4795 - f1_score: 0.4766 - val_loss: 13.3870 - val_acc: 0.4017 - val_f1_score: 0.3108\n",
      "Epoch 2/300\n",
      "27/27 - 7s - loss: 14.0059 - acc: 0.5105 - f1_score: 0.5083 - val_loss: 13.4005 - val_acc: 0.4167 - val_f1_score: 0.3547\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.8888 - acc: 0.5277 - f1_score: 0.5243 - val_loss: 13.4077 - val_acc: 0.4060 - val_f1_score: 0.3627\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.8235 - acc: 0.5331 - f1_score: 0.5302 - val_loss: 13.4054 - val_acc: 0.4295 - val_f1_score: 0.4110\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7660 - acc: 0.5392 - f1_score: 0.5371 - val_loss: 13.3855 - val_acc: 0.5000 - val_f1_score: 0.4942\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.7321 - acc: 0.5547 - f1_score: 0.5518 - val_loss: 13.3937 - val_acc: 0.5192 - val_f1_score: 0.5125\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6912 - acc: 0.5519 - f1_score: 0.5492 - val_loss: 13.3952 - val_acc: 0.5577 - val_f1_score: 0.5395\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6504 - acc: 0.5671 - f1_score: 0.5645 - val_loss: 13.3980 - val_acc: 0.5684 - val_f1_score: 0.5391\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.5968 - acc: 0.5734 - f1_score: 0.5705 - val_loss: 13.4011 - val_acc: 0.5620 - val_f1_score: 0.5317\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5754 - acc: 0.5952 - f1_score: 0.5922 - val_loss: 13.4075 - val_acc: 0.5791 - val_f1_score: 0.5618\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5682 - acc: 0.5821 - f1_score: 0.5799 - val_loss: 13.4267 - val_acc: 0.5662 - val_f1_score: 0.5423\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5457 - acc: 0.5947 - f1_score: 0.5923 - val_loss: 13.4350 - val_acc: 0.5620 - val_f1_score: 0.5295\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5202 - acc: 0.5941 - f1_score: 0.5912 - val_loss: 13.4911 - val_acc: 0.5556 - val_f1_score: 0.5286\n",
      "Epoch 14/300\n",
      "27/27 - 6s - loss: 13.4966 - acc: 0.6093 - f1_score: 0.6070 - val_loss: 13.4974 - val_acc: 0.5769 - val_f1_score: 0.5482\n",
      "Epoch 15/300\n",
      "27/27 - 6s - loss: 13.4671 - acc: 0.6157 - f1_score: 0.6129 - val_loss: 13.4925 - val_acc: 0.5919 - val_f1_score: 0.5736\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4608 - acc: 0.6189 - f1_score: 0.6164 - val_loss: 13.5167 - val_acc: 0.5662 - val_f1_score: 0.5451\n",
      "Epoch 17/300\n",
      "27/27 - 6s - loss: 13.4378 - acc: 0.6289 - f1_score: 0.6264 - val_loss: 13.5577 - val_acc: 0.5556 - val_f1_score: 0.5334\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4309 - acc: 0.6280 - f1_score: 0.6253 - val_loss: 13.5512 - val_acc: 0.5748 - val_f1_score: 0.5531\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.4088 - acc: 0.6342 - f1_score: 0.6320 - val_loss: 13.5550 - val_acc: 0.5769 - val_f1_score: 0.5531\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.3968 - acc: 0.6396 - f1_score: 0.6368 - val_loss: 13.5579 - val_acc: 0.5726 - val_f1_score: 0.5562\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.4028 - acc: 0.6391 - f1_score: 0.6364 - val_loss: 13.5432 - val_acc: 0.5556 - val_f1_score: 0.5454\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3643 - acc: 0.6471 - f1_score: 0.6447 - val_loss: 13.5464 - val_acc: 0.5876 - val_f1_score: 0.5691\n",
      "Epoch 23/300\n",
      "27/27 - 6s - loss: 13.3619 - acc: 0.6516 - f1_score: 0.6493 - val_loss: 13.5404 - val_acc: 0.5833 - val_f1_score: 0.5556\n",
      "Epoch 24/300\n",
      "27/27 - 6s - loss: 13.3307 - acc: 0.6678 - f1_score: 0.6652 - val_loss: 13.5237 - val_acc: 0.5833 - val_f1_score: 0.5691\n",
      "Epoch 25/300\n",
      "27/27 - 6s - loss: 13.3371 - acc: 0.6651 - f1_score: 0.6630 - val_loss: 13.5147 - val_acc: 0.5962 - val_f1_score: 0.5824\n",
      "Epoch 26/300\n",
      "27/27 - 6s - loss: 13.3166 - acc: 0.6730 - f1_score: 0.6709 - val_loss: 13.5234 - val_acc: 0.5919 - val_f1_score: 0.5766\n",
      "Epoch 27/300\n",
      "27/27 - 7s - loss: 13.2889 - acc: 0.6869 - f1_score: 0.6846 - val_loss: 13.5301 - val_acc: 0.5726 - val_f1_score: 0.5611\n",
      "Epoch 28/300\n",
      "27/27 - 7s - loss: 13.2858 - acc: 0.6872 - f1_score: 0.6852 - val_loss: 13.5306 - val_acc: 0.5791 - val_f1_score: 0.5640\n",
      "Epoch 29/300\n",
      "27/27 - 7s - loss: 13.2738 - acc: 0.6955 - f1_score: 0.6934 - val_loss: 13.5149 - val_acc: 0.5791 - val_f1_score: 0.5633\n",
      "Epoch 30/300\n",
      "27/27 - 7s - loss: 13.2866 - acc: 0.6864 - f1_score: 0.6839 - val_loss: 13.5022 - val_acc: 0.5684 - val_f1_score: 0.5585\n",
      "Epoch 31/300\n",
      "27/27 - 7s - loss: 13.2656 - acc: 0.6990 - f1_score: 0.6968 - val_loss: 13.5093 - val_acc: 0.5684 - val_f1_score: 0.5518\n",
      "Epoch 32/300\n",
      "27/27 - 7s - loss: 13.2478 - acc: 0.7082 - f1_score: 0.7058 - val_loss: 13.5021 - val_acc: 0.5726 - val_f1_score: 0.5611\n",
      "Epoch 33/300\n",
      "27/27 - 6s - loss: 13.2527 - acc: 0.7048 - f1_score: 0.7026 - val_loss: 13.5012 - val_acc: 0.5769 - val_f1_score: 0.5689\n",
      "Epoch 34/300\n",
      "27/27 - 7s - loss: 13.2360 - acc: 0.7161 - f1_score: 0.7144 - val_loss: 13.4962 - val_acc: 0.5769 - val_f1_score: 0.5599\n",
      "Epoch 35/300\n",
      "27/27 - 6s - loss: 13.2274 - acc: 0.7200 - f1_score: 0.7180 - val_loss: 13.5027 - val_acc: 0.5705 - val_f1_score: 0.5551\n",
      "Epoch 36/300\n",
      "27/27 - 6s - loss: 13.2221 - acc: 0.7171 - f1_score: 0.7147 - val_loss: 13.5111 - val_acc: 0.5705 - val_f1_score: 0.5536\n",
      "Epoch 37/300\n",
      "27/27 - 6s - loss: 13.2106 - acc: 0.7308 - f1_score: 0.7292 - val_loss: 13.5044 - val_acc: 0.5726 - val_f1_score: 0.5555\n",
      "Epoch 38/300\n",
      "27/27 - 6s - loss: 13.2202 - acc: 0.7194 - f1_score: 0.7173 - val_loss: 13.5103 - val_acc: 0.5833 - val_f1_score: 0.5724\n",
      "Epoch 39/300\n",
      "27/27 - 6s - loss: 13.2104 - acc: 0.7306 - f1_score: 0.7287 - val_loss: 13.5029 - val_acc: 0.5769 - val_f1_score: 0.5667\n",
      "Epoch 40/300\n",
      "27/27 - 6s - loss: 13.1681 - acc: 0.7549 - f1_score: 0.7535 - val_loss: 13.5106 - val_acc: 0.5748 - val_f1_score: 0.5659\n",
      "Epoch 41/300\n",
      "27/27 - 6s - loss: 13.1738 - acc: 0.7510 - f1_score: 0.7493 - val_loss: 13.5112 - val_acc: 0.5833 - val_f1_score: 0.5698\n",
      "Epoch 42/300\n",
      "27/27 - 6s - loss: 13.1617 - acc: 0.7588 - f1_score: 0.7572 - val_loss: 13.5143 - val_acc: 0.5791 - val_f1_score: 0.5633\n",
      "Epoch 43/300\n",
      "27/27 - 6s - loss: 13.1731 - acc: 0.7508 - f1_score: 0.7492 - val_loss: 13.5175 - val_acc: 0.5769 - val_f1_score: 0.5607\n",
      "Epoch 44/300\n",
      "27/27 - 6s - loss: 13.1562 - acc: 0.7602 - f1_score: 0.7586 - val_loss: 13.5098 - val_acc: 0.5940 - val_f1_score: 0.5754\n",
      "Epoch 45/300\n",
      "27/27 - 6s - loss: 13.1406 - acc: 0.7721 - f1_score: 0.7707 - val_loss: 13.5082 - val_acc: 0.5962 - val_f1_score: 0.5780\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00045: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51       188\n",
      "           1       0.67      0.65      0.66       280\n",
      "\n",
      "    accuracy                           0.60       468\n",
      "   macro avg       0.58      0.58      0.58       468\n",
      "weighted avg       0.60      0.60      0.60       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1106\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 57s - loss: 14.2200 - acc: 0.4889 - f1_score: 0.4828 - val_loss: 13.4675 - val_acc: 0.0940 - val_f1_score: 0.0892\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 13.9308 - acc: 0.5172 - f1_score: 0.5170 - val_loss: 13.5010 - val_acc: 0.1004 - val_f1_score: 0.0970\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.8556 - acc: 0.5295 - f1_score: 0.5291 - val_loss: 13.5201 - val_acc: 0.1261 - val_f1_score: 0.1252\n",
      "Epoch 4/300\n",
      "27/27 - 7s - loss: 13.7779 - acc: 0.5293 - f1_score: 0.5287 - val_loss: 13.5228 - val_acc: 0.1453 - val_f1_score: 0.1452\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7294 - acc: 0.5493 - f1_score: 0.5492 - val_loss: 13.4305 - val_acc: 0.3697 - val_f1_score: 0.3143\n",
      "Epoch 6/300\n",
      "27/27 - 7s - loss: 13.6922 - acc: 0.5577 - f1_score: 0.5571 - val_loss: 13.4845 - val_acc: 0.3098 - val_f1_score: 0.2815\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6577 - acc: 0.5600 - f1_score: 0.5598 - val_loss: 13.4328 - val_acc: 0.4530 - val_f1_score: 0.3698\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.5854 - acc: 0.5758 - f1_score: 0.5756 - val_loss: 13.4351 - val_acc: 0.4915 - val_f1_score: 0.3885\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.5796 - acc: 0.5767 - f1_score: 0.5763 - val_loss: 13.4989 - val_acc: 0.4744 - val_f1_score: 0.3900\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5529 - acc: 0.5942 - f1_score: 0.5938 - val_loss: 13.5609 - val_acc: 0.4274 - val_f1_score: 0.3618\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5335 - acc: 0.5884 - f1_score: 0.5880 - val_loss: 13.6241 - val_acc: 0.4145 - val_f1_score: 0.3552\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5208 - acc: 0.5952 - f1_score: 0.5948 - val_loss: 13.6665 - val_acc: 0.4487 - val_f1_score: 0.3778\n",
      "Epoch 13/300\n",
      "27/27 - 7s - loss: 13.4961 - acc: 0.6064 - f1_score: 0.6061 - val_loss: 13.7022 - val_acc: 0.4551 - val_f1_score: 0.3820\n",
      "Epoch 14/300\n",
      "27/27 - 7s - loss: 13.4853 - acc: 0.6077 - f1_score: 0.6074 - val_loss: 13.7571 - val_acc: 0.4594 - val_f1_score: 0.3889\n",
      "Epoch 15/300\n",
      "27/27 - 7s - loss: 13.4539 - acc: 0.6274 - f1_score: 0.6270 - val_loss: 13.8358 - val_acc: 0.4038 - val_f1_score: 0.3515\n",
      "Epoch 16/300\n",
      "27/27 - 7s - loss: 13.4458 - acc: 0.6119 - f1_score: 0.6115 - val_loss: 13.8430 - val_acc: 0.4209 - val_f1_score: 0.3667\n",
      "Epoch 17/300\n",
      "27/27 - 7s - loss: 13.4129 - acc: 0.6341 - f1_score: 0.6339 - val_loss: 13.8453 - val_acc: 0.4487 - val_f1_score: 0.3856\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4109 - acc: 0.6370 - f1_score: 0.6367 - val_loss: 13.8597 - val_acc: 0.4722 - val_f1_score: 0.4034\n",
      "Epoch 19/300\n",
      "27/27 - 7s - loss: 13.4019 - acc: 0.6290 - f1_score: 0.6287 - val_loss: 13.8427 - val_acc: 0.4872 - val_f1_score: 0.4113\n",
      "Epoch 20/300\n",
      "27/27 - 7s - loss: 13.3827 - acc: 0.6381 - f1_score: 0.6378 - val_loss: 13.9203 - val_acc: 0.4658 - val_f1_score: 0.4009\n",
      "Epoch 21/300\n",
      "27/27 - 7s - loss: 13.3891 - acc: 0.6397 - f1_score: 0.6394 - val_loss: 13.9755 - val_acc: 0.4594 - val_f1_score: 0.3966\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3484 - acc: 0.6529 - f1_score: 0.6526 - val_loss: 13.9538 - val_acc: 0.4722 - val_f1_score: 0.4034\n",
      "Epoch 23/300\n",
      "27/27 - 6s - loss: 13.3539 - acc: 0.6588 - f1_score: 0.6587 - val_loss: 13.8196 - val_acc: 0.5427 - val_f1_score: 0.4430\n",
      "Epoch 24/300\n",
      "27/27 - 6s - loss: 13.3326 - acc: 0.6638 - f1_score: 0.6634 - val_loss: 13.9377 - val_acc: 0.4765 - val_f1_score: 0.4062\n",
      "Epoch 25/300\n",
      "27/27 - 6s - loss: 13.3224 - acc: 0.6704 - f1_score: 0.6702 - val_loss: 13.9316 - val_acc: 0.5043 - val_f1_score: 0.4182\n",
      "Epoch 26/300\n",
      "27/27 - 6s - loss: 13.3061 - acc: 0.6793 - f1_score: 0.6789 - val_loss: 13.9394 - val_acc: 0.4872 - val_f1_score: 0.4092\n",
      "Epoch 27/300\n",
      "27/27 - 7s - loss: 13.2896 - acc: 0.6801 - f1_score: 0.6798 - val_loss: 14.0172 - val_acc: 0.4679 - val_f1_score: 0.3945\n",
      "Epoch 28/300\n",
      "27/27 - 7s - loss: 13.2757 - acc: 0.6948 - f1_score: 0.6946 - val_loss: 13.9806 - val_acc: 0.4936 - val_f1_score: 0.4091\n",
      "Epoch 29/300\n",
      "27/27 - 7s - loss: 13.2750 - acc: 0.6949 - f1_score: 0.6947 - val_loss: 13.9723 - val_acc: 0.5192 - val_f1_score: 0.4279\n",
      "Epoch 30/300\n",
      "27/27 - 7s - loss: 13.2739 - acc: 0.6845 - f1_score: 0.6841 - val_loss: 14.0716 - val_acc: 0.4679 - val_f1_score: 0.4024\n",
      "Epoch 31/300\n",
      "27/27 - 7s - loss: 13.2578 - acc: 0.7024 - f1_score: 0.7023 - val_loss: 13.9678 - val_acc: 0.5085 - val_f1_score: 0.4164\n",
      "Epoch 32/300\n",
      "27/27 - 6s - loss: 13.2423 - acc: 0.7114 - f1_score: 0.7111 - val_loss: 14.0047 - val_acc: 0.4829 - val_f1_score: 0.4064\n",
      "Epoch 33/300\n",
      "27/27 - 6s - loss: 13.2575 - acc: 0.7020 - f1_score: 0.7018 - val_loss: 14.0805 - val_acc: 0.4509 - val_f1_score: 0.3870\n",
      "Epoch 34/300\n",
      "27/27 - 6s - loss: 13.2362 - acc: 0.7117 - f1_score: 0.7115 - val_loss: 14.0305 - val_acc: 0.4658 - val_f1_score: 0.3846\n",
      "Epoch 35/300\n",
      "27/27 - 6s - loss: 13.2176 - acc: 0.7250 - f1_score: 0.7247 - val_loss: 14.0674 - val_acc: 0.4423 - val_f1_score: 0.3716\n",
      "Epoch 36/300\n",
      "27/27 - 6s - loss: 13.2189 - acc: 0.7249 - f1_score: 0.7247 - val_loss: 13.9779 - val_acc: 0.4893 - val_f1_score: 0.3995\n",
      "Epoch 37/300\n",
      "27/27 - 6s - loss: 13.2015 - acc: 0.7317 - f1_score: 0.7314 - val_loss: 13.9858 - val_acc: 0.4872 - val_f1_score: 0.4049\n",
      "Epoch 38/300\n",
      "27/27 - 6s - loss: 13.2062 - acc: 0.7271 - f1_score: 0.7269 - val_loss: 14.0995 - val_acc: 0.4423 - val_f1_score: 0.3736\n",
      "Epoch 39/300\n",
      "27/27 - 6s - loss: 13.2082 - acc: 0.7291 - f1_score: 0.7288 - val_loss: 14.1188 - val_acc: 0.4295 - val_f1_score: 0.3689\n",
      "Epoch 40/300\n",
      "27/27 - 6s - loss: 13.1717 - acc: 0.7542 - f1_score: 0.7539 - val_loss: 14.0160 - val_acc: 0.4615 - val_f1_score: 0.3862\n",
      "Epoch 41/300\n",
      "27/27 - 6s - loss: 13.1750 - acc: 0.7500 - f1_score: 0.7497 - val_loss: 14.0859 - val_acc: 0.4530 - val_f1_score: 0.3785\n",
      "Epoch 42/300\n",
      "27/27 - 6s - loss: 13.1674 - acc: 0.7504 - f1_score: 0.7502 - val_loss: 13.9950 - val_acc: 0.4829 - val_f1_score: 0.3955\n",
      "Epoch 43/300\n",
      "27/27 - 6s - loss: 13.1658 - acc: 0.7546 - f1_score: 0.7544 - val_loss: 14.0066 - val_acc: 0.4872 - val_f1_score: 0.3958\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00043: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.12      0.21       231\n",
      "           1       0.53      0.95      0.68       237\n",
      "\n",
      "    accuracy                           0.54       468\n",
      "   macro avg       0.62      0.54      0.44       468\n",
      "weighted avg       0.62      0.54      0.45       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1175\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 51s - loss: 14.3671 - acc: 0.4798 - f1_score: 0.4777 - val_loss: 13.3288 - val_acc: 0.7082 - val_f1_score: 0.4233\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 14.0342 - acc: 0.5217 - f1_score: 0.5167 - val_loss: 13.3294 - val_acc: 0.6950 - val_f1_score: 0.4751\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.9083 - acc: 0.5360 - f1_score: 0.5309 - val_loss: 13.3567 - val_acc: 0.5411 - val_f1_score: 0.4748\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.8454 - acc: 0.5492 - f1_score: 0.5431 - val_loss: 13.3655 - val_acc: 0.5570 - val_f1_score: 0.5052\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.7630 - acc: 0.5560 - f1_score: 0.5501 - val_loss: 13.3801 - val_acc: 0.5332 - val_f1_score: 0.4986\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7232 - acc: 0.5611 - f1_score: 0.5548 - val_loss: 13.4300 - val_acc: 0.4483 - val_f1_score: 0.4417\n",
      "Epoch 7/300\n",
      "28/28 - 7s - loss: 13.6689 - acc: 0.5739 - f1_score: 0.5675 - val_loss: 13.4645 - val_acc: 0.4854 - val_f1_score: 0.4665\n",
      "Epoch 8/300\n",
      "28/28 - 6s - loss: 13.6458 - acc: 0.5771 - f1_score: 0.5717 - val_loss: 13.6056 - val_acc: 0.4164 - val_f1_score: 0.4164\n",
      "Epoch 9/300\n",
      "28/28 - 7s - loss: 13.6218 - acc: 0.5842 - f1_score: 0.5779 - val_loss: 13.6437 - val_acc: 0.4138 - val_f1_score: 0.4132\n",
      "Epoch 10/300\n",
      "28/28 - 7s - loss: 13.5947 - acc: 0.5855 - f1_score: 0.5793 - val_loss: 13.6398 - val_acc: 0.4324 - val_f1_score: 0.4290\n",
      "Epoch 11/300\n",
      "28/28 - 8s - loss: 13.5716 - acc: 0.5928 - f1_score: 0.5876 - val_loss: 13.6952 - val_acc: 0.4536 - val_f1_score: 0.4503\n",
      "Epoch 12/300\n",
      "28/28 - 7s - loss: 13.5351 - acc: 0.6025 - f1_score: 0.5968 - val_loss: 13.7477 - val_acc: 0.4058 - val_f1_score: 0.4032\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5158 - acc: 0.6071 - f1_score: 0.6005 - val_loss: 13.8023 - val_acc: 0.3979 - val_f1_score: 0.3958\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.5070 - acc: 0.6034 - f1_score: 0.5972 - val_loss: 13.7837 - val_acc: 0.4058 - val_f1_score: 0.4018\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4789 - acc: 0.6145 - f1_score: 0.6094 - val_loss: 13.8151 - val_acc: 0.4138 - val_f1_score: 0.4105\n",
      "Epoch 16/300\n",
      "28/28 - 7s - loss: 13.4729 - acc: 0.6218 - f1_score: 0.6167 - val_loss: 14.0276 - val_acc: 0.3581 - val_f1_score: 0.3579\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.4477 - acc: 0.6267 - f1_score: 0.6207 - val_loss: 13.9068 - val_acc: 0.4244 - val_f1_score: 0.4202\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.4315 - acc: 0.6311 - f1_score: 0.6252 - val_loss: 13.9635 - val_acc: 0.4403 - val_f1_score: 0.4357\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.3970 - acc: 0.6446 - f1_score: 0.6391 - val_loss: 13.9839 - val_acc: 0.4377 - val_f1_score: 0.4333\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.3949 - acc: 0.6464 - f1_score: 0.6411 - val_loss: 14.0240 - val_acc: 0.4244 - val_f1_score: 0.4217\n",
      "Epoch 21/300\n",
      "28/28 - 7s - loss: 13.3903 - acc: 0.6428 - f1_score: 0.6375 - val_loss: 14.1116 - val_acc: 0.4164 - val_f1_score: 0.4150\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3827 - acc: 0.6466 - f1_score: 0.6414 - val_loss: 14.0657 - val_acc: 0.4403 - val_f1_score: 0.4363\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3679 - acc: 0.6544 - f1_score: 0.6487 - val_loss: 14.0474 - val_acc: 0.4271 - val_f1_score: 0.4221\n",
      "Epoch 24/300\n",
      "28/28 - 6s - loss: 13.3425 - acc: 0.6613 - f1_score: 0.6557 - val_loss: 14.1187 - val_acc: 0.4244 - val_f1_score: 0.4217\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.67       226\n",
      "           1       0.42      0.29      0.35       151\n",
      "\n",
      "    accuracy                           0.56       377\n",
      "   macro avg       0.52      0.51      0.51       377\n",
      "weighted avg       0.53      0.56      0.54       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1194\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 54s - loss: 14.2602 - acc: 0.4835 - f1_score: 0.4770 - val_loss: 13.4310 - val_acc: 0.2154 - val_f1_score: 0.1877\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 13.9953 - acc: 0.5181 - f1_score: 0.5175 - val_loss: 13.4301 - val_acc: 0.2791 - val_f1_score: 0.2713\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.9364 - acc: 0.5236 - f1_score: 0.5221 - val_loss: 13.4266 - val_acc: 0.3143 - val_f1_score: 0.3128\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.8509 - acc: 0.5335 - f1_score: 0.5325 - val_loss: 13.3980 - val_acc: 0.4374 - val_f1_score: 0.4249\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.8242 - acc: 0.5380 - f1_score: 0.5355 - val_loss: 13.4477 - val_acc: 0.3626 - val_f1_score: 0.3600\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.8022 - acc: 0.5347 - f1_score: 0.5339 - val_loss: 13.4362 - val_acc: 0.4549 - val_f1_score: 0.4405\n",
      "Epoch 7/300\n",
      "28/28 - 7s - loss: 13.7725 - acc: 0.5341 - f1_score: 0.5333 - val_loss: 13.3883 - val_acc: 0.5253 - val_f1_score: 0.4824\n",
      "Epoch 8/300\n",
      "28/28 - 7s - loss: 13.7449 - acc: 0.5408 - f1_score: 0.5395 - val_loss: 13.3789 - val_acc: 0.5560 - val_f1_score: 0.4704\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.7587 - acc: 0.5451 - f1_score: 0.5427 - val_loss: 13.5132 - val_acc: 0.4703 - val_f1_score: 0.4200\n",
      "Epoch 10/300\n",
      "28/28 - 7s - loss: 13.7177 - acc: 0.5515 - f1_score: 0.5504 - val_loss: 13.5901 - val_acc: 0.4352 - val_f1_score: 0.3914\n",
      "Epoch 11/300\n",
      "28/28 - 7s - loss: 13.6993 - acc: 0.5513 - f1_score: 0.5506 - val_loss: 13.5631 - val_acc: 0.4637 - val_f1_score: 0.3870\n",
      "Epoch 12/300\n",
      "28/28 - 7s - loss: 13.6740 - acc: 0.5523 - f1_score: 0.5511 - val_loss: 13.5585 - val_acc: 0.5099 - val_f1_score: 0.4165\n",
      "Epoch 13/300\n",
      "28/28 - 8s - loss: 13.6512 - acc: 0.5625 - f1_score: 0.5609 - val_loss: 13.7228 - val_acc: 0.4440 - val_f1_score: 0.3858\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.6602 - acc: 0.5570 - f1_score: 0.5565 - val_loss: 13.6694 - val_acc: 0.5429 - val_f1_score: 0.4523\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.6151 - acc: 0.5680 - f1_score: 0.5669 - val_loss: 13.7646 - val_acc: 0.5187 - val_f1_score: 0.4468\n",
      "Epoch 16/300\n",
      "28/28 - 6s - loss: 13.6221 - acc: 0.5748 - f1_score: 0.5733 - val_loss: 13.8702 - val_acc: 0.4945 - val_f1_score: 0.4408\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.5818 - acc: 0.5732 - f1_score: 0.5728 - val_loss: 13.8105 - val_acc: 0.5363 - val_f1_score: 0.4651\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.5944 - acc: 0.5737 - f1_score: 0.5726 - val_loss: 13.8608 - val_acc: 0.4945 - val_f1_score: 0.4441\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.5531 - acc: 0.5821 - f1_score: 0.5812 - val_loss: 13.8474 - val_acc: 0.5407 - val_f1_score: 0.4758\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.5837 - acc: 0.5779 - f1_score: 0.5768 - val_loss: 13.8155 - val_acc: 0.5407 - val_f1_score: 0.4701\n",
      "Epoch 21/300\n",
      "28/28 - 6s - loss: 13.5669 - acc: 0.5891 - f1_score: 0.5880 - val_loss: 13.8067 - val_acc: 0.5495 - val_f1_score: 0.4680\n",
      "Epoch 22/300\n",
      "28/28 - 7s - loss: 13.5216 - acc: 0.6016 - f1_score: 0.5999 - val_loss: 13.8589 - val_acc: 0.5209 - val_f1_score: 0.4561\n",
      "Epoch 23/300\n",
      "28/28 - 7s - loss: 13.5291 - acc: 0.5946 - f1_score: 0.5936 - val_loss: 13.9340 - val_acc: 0.4791 - val_f1_score: 0.4373\n",
      "Epoch 24/300\n",
      "28/28 - 7s - loss: 13.5182 - acc: 0.5976 - f1_score: 0.5970 - val_loss: 13.8372 - val_acc: 0.5055 - val_f1_score: 0.4415\n",
      "Epoch 25/300\n",
      "28/28 - 7s - loss: 13.5149 - acc: 0.5843 - f1_score: 0.5828 - val_loss: 13.9453 - val_acc: 0.4549 - val_f1_score: 0.4178\n",
      "Epoch 26/300\n",
      "28/28 - 6s - loss: 13.5069 - acc: 0.5957 - f1_score: 0.5952 - val_loss: 13.9029 - val_acc: 0.4791 - val_f1_score: 0.4344\n",
      "Epoch 27/300\n",
      "28/28 - 6s - loss: 13.5024 - acc: 0.6005 - f1_score: 0.5998 - val_loss: 13.8259 - val_acc: 0.5077 - val_f1_score: 0.4486\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.23      0.33       233\n",
      "           1       0.51      0.83      0.63       222\n",
      "\n",
      "    accuracy                           0.53       455\n",
      "   macro avg       0.55      0.53      0.48       455\n",
      "weighted avg       0.55      0.53      0.48       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1337\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 54s - loss: 14.2008 - acc: 0.4914 - f1_score: 0.4911 - val_loss: 13.3398 - val_acc: 0.6859 - val_f1_score: 0.4788\n",
      "Epoch 2/300\n",
      "27/27 - 7s - loss: 13.9535 - acc: 0.5254 - f1_score: 0.5194 - val_loss: 13.3527 - val_acc: 0.5833 - val_f1_score: 0.5100\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.8474 - acc: 0.5360 - f1_score: 0.5282 - val_loss: 13.3741 - val_acc: 0.4701 - val_f1_score: 0.4612\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.7963 - acc: 0.5380 - f1_score: 0.5301 - val_loss: 13.4341 - val_acc: 0.3483 - val_f1_score: 0.3370\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7069 - acc: 0.5625 - f1_score: 0.5563 - val_loss: 13.5851 - val_acc: 0.2949 - val_f1_score: 0.2470\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.6767 - acc: 0.5640 - f1_score: 0.5552 - val_loss: 13.6199 - val_acc: 0.3056 - val_f1_score: 0.2657\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6624 - acc: 0.5631 - f1_score: 0.5572 - val_loss: 13.7662 - val_acc: 0.2906 - val_f1_score: 0.2459\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6003 - acc: 0.5831 - f1_score: 0.5761 - val_loss: 13.8909 - val_acc: 0.2949 - val_f1_score: 0.2504\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.5838 - acc: 0.5831 - f1_score: 0.5760 - val_loss: 13.9523 - val_acc: 0.2949 - val_f1_score: 0.2567\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5385 - acc: 0.5970 - f1_score: 0.5893 - val_loss: 13.9144 - val_acc: 0.3226 - val_f1_score: 0.3017\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5251 - acc: 0.6029 - f1_score: 0.5956 - val_loss: 13.8951 - val_acc: 0.3355 - val_f1_score: 0.3216\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5288 - acc: 0.6032 - f1_score: 0.5965 - val_loss: 14.0283 - val_acc: 0.3419 - val_f1_score: 0.3259\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5130 - acc: 0.5984 - f1_score: 0.5908 - val_loss: 13.8868 - val_acc: 0.3846 - val_f1_score: 0.3791\n",
      "Epoch 14/300\n",
      "27/27 - 6s - loss: 13.4820 - acc: 0.6112 - f1_score: 0.6051 - val_loss: 13.9405 - val_acc: 0.3868 - val_f1_score: 0.3825\n",
      "Epoch 15/300\n",
      "27/27 - 6s - loss: 13.4630 - acc: 0.6209 - f1_score: 0.6141 - val_loss: 13.9776 - val_acc: 0.3953 - val_f1_score: 0.3930\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4341 - acc: 0.6290 - f1_score: 0.6213 - val_loss: 13.9201 - val_acc: 0.4316 - val_f1_score: 0.4311\n",
      "Epoch 17/300\n",
      "27/27 - 6s - loss: 13.4225 - acc: 0.6319 - f1_score: 0.6261 - val_loss: 14.0098 - val_acc: 0.4167 - val_f1_score: 0.4147\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4203 - acc: 0.6335 - f1_score: 0.6274 - val_loss: 13.9658 - val_acc: 0.4231 - val_f1_score: 0.4220\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.4146 - acc: 0.6377 - f1_score: 0.6314 - val_loss: 13.9111 - val_acc: 0.4380 - val_f1_score: 0.4377\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.3803 - acc: 0.6506 - f1_score: 0.6436 - val_loss: 13.8712 - val_acc: 0.4380 - val_f1_score: 0.4380\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.3772 - acc: 0.6442 - f1_score: 0.6372 - val_loss: 13.7985 - val_acc: 0.4423 - val_f1_score: 0.4422\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3545 - acc: 0.6584 - f1_score: 0.6517 - val_loss: 13.8291 - val_acc: 0.4722 - val_f1_score: 0.4722\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70       311\n",
      "           1       0.35      0.29      0.32       157\n",
      "\n",
      "    accuracy                           0.58       468\n",
      "   macro avg       0.51      0.51      0.51       468\n",
      "weighted avg       0.57      0.58      0.57       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1390\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 58s - loss: 14.2029 - acc: 0.4941 - f1_score: 0.4930 - val_loss: 13.3840 - val_acc: 0.4359 - val_f1_score: 0.3129\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 13.9514 - acc: 0.5098 - f1_score: 0.5068 - val_loss: 13.3845 - val_acc: 0.4295 - val_f1_score: 0.3343\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.8589 - acc: 0.5301 - f1_score: 0.5266 - val_loss: 13.3825 - val_acc: 0.4658 - val_f1_score: 0.4116\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.7866 - acc: 0.5272 - f1_score: 0.5241 - val_loss: 13.3772 - val_acc: 0.5235 - val_f1_score: 0.5102\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7235 - acc: 0.5557 - f1_score: 0.5528 - val_loss: 13.3585 - val_acc: 0.5513 - val_f1_score: 0.5378\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.6881 - acc: 0.5554 - f1_score: 0.5510 - val_loss: 13.3660 - val_acc: 0.5513 - val_f1_score: 0.5505\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6561 - acc: 0.5509 - f1_score: 0.5483 - val_loss: 13.3621 - val_acc: 0.5833 - val_f1_score: 0.5705\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6114 - acc: 0.5699 - f1_score: 0.5668 - val_loss: 13.3753 - val_acc: 0.5855 - val_f1_score: 0.5648\n",
      "Epoch 9/300\n",
      "27/27 - 7s - loss: 13.5986 - acc: 0.5742 - f1_score: 0.5716 - val_loss: 13.4041 - val_acc: 0.5748 - val_f1_score: 0.5540\n",
      "Epoch 10/300\n",
      "27/27 - 7s - loss: 13.5680 - acc: 0.5899 - f1_score: 0.5857 - val_loss: 13.4263 - val_acc: 0.5684 - val_f1_score: 0.5596\n",
      "Epoch 11/300\n",
      "27/27 - 7s - loss: 13.5344 - acc: 0.5935 - f1_score: 0.5907 - val_loss: 13.4484 - val_acc: 0.6047 - val_f1_score: 0.5931\n",
      "Epoch 12/300\n",
      "27/27 - 7s - loss: 13.5178 - acc: 0.5997 - f1_score: 0.5970 - val_loss: 13.4891 - val_acc: 0.5962 - val_f1_score: 0.5764\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5247 - acc: 0.5908 - f1_score: 0.5874 - val_loss: 13.4963 - val_acc: 0.5983 - val_f1_score: 0.5856\n",
      "Epoch 14/300\n",
      "27/27 - 7s - loss: 13.5062 - acc: 0.6018 - f1_score: 0.5986 - val_loss: 13.5176 - val_acc: 0.5620 - val_f1_score: 0.5455\n",
      "Epoch 15/300\n",
      "27/27 - 6s - loss: 13.4590 - acc: 0.6093 - f1_score: 0.6065 - val_loss: 13.5490 - val_acc: 0.5962 - val_f1_score: 0.5810\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4490 - acc: 0.6197 - f1_score: 0.6166 - val_loss: 13.5874 - val_acc: 0.5919 - val_f1_score: 0.5766\n",
      "Epoch 17/300\n",
      "27/27 - 6s - loss: 13.4353 - acc: 0.6239 - f1_score: 0.6210 - val_loss: 13.6015 - val_acc: 0.5748 - val_f1_score: 0.5573\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4248 - acc: 0.6264 - f1_score: 0.6235 - val_loss: 13.6565 - val_acc: 0.5705 - val_f1_score: 0.5429\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.3955 - acc: 0.6355 - f1_score: 0.6323 - val_loss: 13.6927 - val_acc: 0.5748 - val_f1_score: 0.5588\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.3883 - acc: 0.6364 - f1_score: 0.6336 - val_loss: 13.7581 - val_acc: 0.5769 - val_f1_score: 0.5607\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.3653 - acc: 0.6493 - f1_score: 0.6462 - val_loss: 13.7962 - val_acc: 0.5684 - val_f1_score: 0.5573\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3602 - acc: 0.6487 - f1_score: 0.6456 - val_loss: 13.8058 - val_acc: 0.5684 - val_f1_score: 0.5602\n",
      "Epoch 23/300\n",
      "27/27 - 6s - loss: 13.3437 - acc: 0.6507 - f1_score: 0.6482 - val_loss: 13.8424 - val_acc: 0.5598 - val_f1_score: 0.5459\n",
      "Epoch 24/300\n",
      "27/27 - 6s - loss: 13.3439 - acc: 0.6607 - f1_score: 0.6583 - val_loss: 13.8647 - val_acc: 0.5662 - val_f1_score: 0.5522\n",
      "Epoch 25/300\n",
      "27/27 - 6s - loss: 13.3364 - acc: 0.6575 - f1_score: 0.6549 - val_loss: 13.8761 - val_acc: 0.5662 - val_f1_score: 0.5542\n",
      "Epoch 26/300\n",
      "27/27 - 6s - loss: 13.3164 - acc: 0.6649 - f1_score: 0.6620 - val_loss: 13.8877 - val_acc: 0.5620 - val_f1_score: 0.5504\n",
      "Epoch 27/300\n",
      "27/27 - 6s - loss: 13.3130 - acc: 0.6712 - f1_score: 0.6682 - val_loss: 13.8779 - val_acc: 0.5791 - val_f1_score: 0.5708\n",
      "Epoch 28/300\n",
      "27/27 - 6s - loss: 13.2978 - acc: 0.6833 - f1_score: 0.6808 - val_loss: 13.8949 - val_acc: 0.5897 - val_f1_score: 0.5774\n",
      "Epoch 29/300\n",
      "27/27 - 6s - loss: 13.2808 - acc: 0.6872 - f1_score: 0.6846 - val_loss: 13.9199 - val_acc: 0.5812 - val_f1_score: 0.5658\n",
      "Epoch 30/300\n",
      "27/27 - 6s - loss: 13.2843 - acc: 0.6748 - f1_score: 0.6717 - val_loss: 13.8811 - val_acc: 0.5769 - val_f1_score: 0.5716\n",
      "Epoch 31/300\n",
      "27/27 - 6s - loss: 13.2681 - acc: 0.6900 - f1_score: 0.6876 - val_loss: 13.8839 - val_acc: 0.5962 - val_f1_score: 0.5837\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.56      0.52       181\n",
      "           1       0.70      0.63      0.66       287\n",
      "\n",
      "    accuracy                           0.60       468\n",
      "   macro avg       0.59      0.60      0.59       468\n",
      "weighted avg       0.62      0.60      0.61       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1400\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 60s - loss: 14.3654 - acc: 0.4960 - f1_score: 0.4898 - val_loss: 13.4418 - val_acc: 0.1667 - val_f1_score: 0.1578\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 14.0076 - acc: 0.5124 - f1_score: 0.5120 - val_loss: 13.4781 - val_acc: 0.1816 - val_f1_score: 0.1753\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.8756 - acc: 0.5215 - f1_score: 0.5208 - val_loss: 13.5090 - val_acc: 0.1944 - val_f1_score: 0.1904\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.8318 - acc: 0.5322 - f1_score: 0.5316 - val_loss: 13.4660 - val_acc: 0.2842 - val_f1_score: 0.2802\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7672 - acc: 0.5503 - f1_score: 0.5499 - val_loss: 13.4085 - val_acc: 0.4808 - val_f1_score: 0.4256\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.7454 - acc: 0.5451 - f1_score: 0.5439 - val_loss: 13.5020 - val_acc: 0.3632 - val_f1_score: 0.3399\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.7144 - acc: 0.5650 - f1_score: 0.5644 - val_loss: 13.4674 - val_acc: 0.4808 - val_f1_score: 0.4091\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6227 - acc: 0.5747 - f1_score: 0.5740 - val_loss: 13.4853 - val_acc: 0.5085 - val_f1_score: 0.4232\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.6237 - acc: 0.5831 - f1_score: 0.5825 - val_loss: 13.4902 - val_acc: 0.5962 - val_f1_score: 0.4690\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5999 - acc: 0.5780 - f1_score: 0.5771 - val_loss: 13.6344 - val_acc: 0.5107 - val_f1_score: 0.4268\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5701 - acc: 0.5897 - f1_score: 0.5891 - val_loss: 13.6421 - val_acc: 0.5726 - val_f1_score: 0.4623\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5415 - acc: 0.5971 - f1_score: 0.5967 - val_loss: 13.6712 - val_acc: 0.6068 - val_f1_score: 0.4787\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5172 - acc: 0.6034 - f1_score: 0.6027 - val_loss: 13.8092 - val_acc: 0.5513 - val_f1_score: 0.4582\n",
      "Epoch 14/300\n",
      "27/27 - 6s - loss: 13.5051 - acc: 0.6058 - f1_score: 0.6054 - val_loss: 13.8295 - val_acc: 0.5662 - val_f1_score: 0.4681\n",
      "Epoch 15/300\n",
      "27/27 - 7s - loss: 13.4900 - acc: 0.6080 - f1_score: 0.6076 - val_loss: 13.8867 - val_acc: 0.5983 - val_f1_score: 0.4946\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4596 - acc: 0.6186 - f1_score: 0.6180 - val_loss: 14.0360 - val_acc: 0.5705 - val_f1_score: 0.4889\n",
      "Epoch 17/300\n",
      "27/27 - 7s - loss: 13.4353 - acc: 0.6357 - f1_score: 0.6353 - val_loss: 14.0094 - val_acc: 0.6047 - val_f1_score: 0.5062\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4143 - acc: 0.6313 - f1_score: 0.6307 - val_loss: 14.0343 - val_acc: 0.6090 - val_f1_score: 0.5068\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.4040 - acc: 0.6374 - f1_score: 0.6368 - val_loss: 14.0835 - val_acc: 0.5897 - val_f1_score: 0.4958\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.4019 - acc: 0.6367 - f1_score: 0.6361 - val_loss: 14.2092 - val_acc: 0.5684 - val_f1_score: 0.4766\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.3739 - acc: 0.6477 - f1_score: 0.6473 - val_loss: 14.2378 - val_acc: 0.5705 - val_f1_score: 0.4757\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3505 - acc: 0.6604 - f1_score: 0.6600 - val_loss: 14.3390 - val_acc: 0.5278 - val_f1_score: 0.4490\n",
      "Epoch 23/300\n",
      "27/27 - 6s - loss: 13.3549 - acc: 0.6581 - f1_score: 0.6574 - val_loss: 14.2598 - val_acc: 0.5342 - val_f1_score: 0.4512\n",
      "Epoch 24/300\n",
      "27/27 - 6s - loss: 13.3403 - acc: 0.6655 - f1_score: 0.6652 - val_loss: 14.1976 - val_acc: 0.5641 - val_f1_score: 0.4619\n",
      "Epoch 25/300\n",
      "27/27 - 6s - loss: 13.3377 - acc: 0.6623 - f1_score: 0.6618 - val_loss: 14.2456 - val_acc: 0.5491 - val_f1_score: 0.4568\n",
      "Epoch 26/300\n",
      "27/27 - 6s - loss: 13.3050 - acc: 0.6777 - f1_score: 0.6772 - val_loss: 14.3105 - val_acc: 0.5363 - val_f1_score: 0.4548\n",
      "Epoch 27/300\n",
      "27/27 - 6s - loss: 13.3045 - acc: 0.6796 - f1_score: 0.6790 - val_loss: 14.4157 - val_acc: 0.4957 - val_f1_score: 0.4232\n",
      "Epoch 28/300\n",
      "27/27 - 6s - loss: 13.2980 - acc: 0.6893 - f1_score: 0.6888 - val_loss: 14.2679 - val_acc: 0.5556 - val_f1_score: 0.4610\n",
      "Epoch 29/300\n",
      "27/27 - 6s - loss: 13.2751 - acc: 0.6907 - f1_score: 0.6903 - val_loss: 14.2645 - val_acc: 0.5577 - val_f1_score: 0.4648\n",
      "Epoch 30/300\n",
      "27/27 - 7s - loss: 13.2830 - acc: 0.6907 - f1_score: 0.6903 - val_loss: 14.3973 - val_acc: 0.5278 - val_f1_score: 0.4550\n",
      "Epoch 31/300\n",
      "27/27 - 7s - loss: 13.2504 - acc: 0.7088 - f1_score: 0.7084 - val_loss: 14.3501 - val_acc: 0.5363 - val_f1_score: 0.4548\n",
      "Epoch 32/300\n",
      "27/27 - 7s - loss: 13.2429 - acc: 0.7106 - f1_score: 0.7103 - val_loss: 14.3147 - val_acc: 0.5726 - val_f1_score: 0.4772\n",
      "Epoch 33/300\n",
      "27/27 - 8s - loss: 13.2410 - acc: 0.7119 - f1_score: 0.7113 - val_loss: 14.4253 - val_acc: 0.5150 - val_f1_score: 0.4402\n",
      "Epoch 34/300\n",
      "27/27 - 6s - loss: 13.2324 - acc: 0.7213 - f1_score: 0.7208 - val_loss: 14.3639 - val_acc: 0.5299 - val_f1_score: 0.4483\n",
      "Epoch 35/300\n",
      "27/27 - 7s - loss: 13.2202 - acc: 0.7214 - f1_score: 0.7211 - val_loss: 14.4308 - val_acc: 0.5256 - val_f1_score: 0.4496\n",
      "Epoch 36/300\n",
      "27/27 - 8s - loss: 13.2159 - acc: 0.7274 - f1_score: 0.7270 - val_loss: 14.2857 - val_acc: 0.5534 - val_f1_score: 0.4642\n",
      "Epoch 37/300\n",
      "27/27 - 6s - loss: 13.2183 - acc: 0.7213 - f1_score: 0.7208 - val_loss: 14.3511 - val_acc: 0.5406 - val_f1_score: 0.4598\n",
      "Epoch 38/300\n",
      "27/27 - 7s - loss: 13.1986 - acc: 0.7358 - f1_score: 0.7353 - val_loss: 14.3942 - val_acc: 0.5342 - val_f1_score: 0.4554\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00038: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.19      0.28       190\n",
      "           1       0.62      0.90      0.73       278\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.59      0.54      0.51       468\n",
      "weighted avg       0.59      0.61      0.55       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1419\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 60s - loss: 14.2891 - acc: 0.4816 - f1_score: 0.4763 - val_loss: 13.4002 - val_acc: 0.3253 - val_f1_score: 0.2730\n",
      "Epoch 2/300\n",
      "28/28 - 7s - loss: 13.9860 - acc: 0.5139 - f1_score: 0.5129 - val_loss: 13.4087 - val_acc: 0.3253 - val_f1_score: 0.2938\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.9141 - acc: 0.5301 - f1_score: 0.5285 - val_loss: 13.3917 - val_acc: 0.4198 - val_f1_score: 0.4196\n",
      "Epoch 4/300\n",
      "28/28 - 7s - loss: 13.9013 - acc: 0.5325 - f1_score: 0.5300 - val_loss: 13.3937 - val_acc: 0.4527 - val_f1_score: 0.4522\n",
      "Epoch 5/300\n",
      "28/28 - 7s - loss: 13.8303 - acc: 0.5429 - f1_score: 0.5404 - val_loss: 13.4167 - val_acc: 0.4242 - val_f1_score: 0.4238\n",
      "Epoch 6/300\n",
      "28/28 - 7s - loss: 13.8192 - acc: 0.5353 - f1_score: 0.5329 - val_loss: 13.4070 - val_acc: 0.4835 - val_f1_score: 0.4772\n",
      "Epoch 7/300\n",
      "28/28 - 7s - loss: 13.7833 - acc: 0.5455 - f1_score: 0.5439 - val_loss: 13.4015 - val_acc: 0.5560 - val_f1_score: 0.5340\n",
      "Epoch 8/300\n",
      "28/28 - 7s - loss: 13.7745 - acc: 0.5388 - f1_score: 0.5368 - val_loss: 13.3761 - val_acc: 0.5890 - val_f1_score: 0.5418\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.7748 - acc: 0.5431 - f1_score: 0.5401 - val_loss: 13.4441 - val_acc: 0.5385 - val_f1_score: 0.5209\n",
      "Epoch 10/300\n",
      "28/28 - 7s - loss: 13.7156 - acc: 0.5560 - f1_score: 0.5541 - val_loss: 13.4711 - val_acc: 0.5385 - val_f1_score: 0.5226\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.7136 - acc: 0.5548 - f1_score: 0.5538 - val_loss: 13.4361 - val_acc: 0.5626 - val_f1_score: 0.5124\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.6993 - acc: 0.5500 - f1_score: 0.5482 - val_loss: 13.4238 - val_acc: 0.5868 - val_f1_score: 0.5277\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.6638 - acc: 0.5583 - f1_score: 0.5555 - val_loss: 13.4684 - val_acc: 0.6066 - val_f1_score: 0.5297\n",
      "Epoch 14/300\n",
      "28/28 - 7s - loss: 13.6458 - acc: 0.5664 - f1_score: 0.5637 - val_loss: 13.5632 - val_acc: 0.5780 - val_f1_score: 0.5346\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.6132 - acc: 0.5682 - f1_score: 0.5673 - val_loss: 13.5529 - val_acc: 0.5978 - val_f1_score: 0.5458\n",
      "Epoch 16/300\n",
      "28/28 - 7s - loss: 13.6030 - acc: 0.5720 - f1_score: 0.5703 - val_loss: 13.6666 - val_acc: 0.5604 - val_f1_score: 0.5326\n",
      "Epoch 17/300\n",
      "28/28 - 7s - loss: 13.5638 - acc: 0.5758 - f1_score: 0.5745 - val_loss: 13.6390 - val_acc: 0.5495 - val_f1_score: 0.5067\n",
      "Epoch 18/300\n",
      "28/28 - 7s - loss: 13.5776 - acc: 0.5745 - f1_score: 0.5729 - val_loss: 13.6172 - val_acc: 0.5846 - val_f1_score: 0.5355\n",
      "Epoch 19/300\n",
      "28/28 - 7s - loss: 13.5584 - acc: 0.5811 - f1_score: 0.5790 - val_loss: 13.6480 - val_acc: 0.5692 - val_f1_score: 0.5290\n",
      "Epoch 20/300\n",
      "28/28 - 7s - loss: 13.5585 - acc: 0.5881 - f1_score: 0.5860 - val_loss: 13.6239 - val_acc: 0.5890 - val_f1_score: 0.5404\n",
      "Epoch 21/300\n",
      "28/28 - 7s - loss: 13.5493 - acc: 0.5866 - f1_score: 0.5844 - val_loss: 13.6380 - val_acc: 0.5692 - val_f1_score: 0.5249\n",
      "Epoch 22/300\n",
      "28/28 - 8s - loss: 13.5193 - acc: 0.5902 - f1_score: 0.5875 - val_loss: 13.7153 - val_acc: 0.5275 - val_f1_score: 0.5064\n",
      "Epoch 23/300\n",
      "28/28 - 7s - loss: 13.5110 - acc: 0.5967 - f1_score: 0.5957 - val_loss: 13.6828 - val_acc: 0.5516 - val_f1_score: 0.5233\n",
      "Epoch 24/300\n",
      "28/28 - 7s - loss: 13.5055 - acc: 0.5905 - f1_score: 0.5893 - val_loss: 13.6417 - val_acc: 0.5736 - val_f1_score: 0.5351\n",
      "Epoch 25/300\n",
      "28/28 - 8s - loss: 13.5058 - acc: 0.5989 - f1_score: 0.5964 - val_loss: 13.6864 - val_acc: 0.5385 - val_f1_score: 0.5135\n",
      "Epoch 26/300\n",
      "28/28 - 8s - loss: 13.4923 - acc: 0.6001 - f1_score: 0.5988 - val_loss: 13.7254 - val_acc: 0.5275 - val_f1_score: 0.5064\n",
      "Epoch 27/300\n",
      "28/28 - 7s - loss: 13.4941 - acc: 0.5973 - f1_score: 0.5958 - val_loss: 13.6629 - val_acc: 0.5670 - val_f1_score: 0.5347\n",
      "Epoch 28/300\n",
      "28/28 - 7s - loss: 13.4663 - acc: 0.6087 - f1_score: 0.6065 - val_loss: 13.7053 - val_acc: 0.5275 - val_f1_score: 0.5082\n",
      "Epoch 29/300\n",
      "28/28 - 7s - loss: 13.4654 - acc: 0.6060 - f1_score: 0.6048 - val_loss: 13.6873 - val_acc: 0.5407 - val_f1_score: 0.5143\n",
      "Epoch 30/300\n",
      "28/28 - 6s - loss: 13.4587 - acc: 0.6141 - f1_score: 0.6126 - val_loss: 13.6857 - val_acc: 0.5231 - val_f1_score: 0.4968\n",
      "Epoch 31/300\n",
      "28/28 - 6s - loss: 13.4515 - acc: 0.6158 - f1_score: 0.6143 - val_loss: 13.6773 - val_acc: 0.5451 - val_f1_score: 0.5146\n",
      "Epoch 32/300\n",
      "28/28 - 6s - loss: 13.4368 - acc: 0.6241 - f1_score: 0.6225 - val_loss: 13.6769 - val_acc: 0.5451 - val_f1_score: 0.5134\n",
      "Epoch 33/300\n",
      "28/28 - 6s - loss: 13.4445 - acc: 0.6187 - f1_score: 0.6168 - val_loss: 13.6977 - val_acc: 0.5275 - val_f1_score: 0.5055\n",
      "Epoch 34/300\n",
      "28/28 - 6s - loss: 13.4170 - acc: 0.6203 - f1_score: 0.6186 - val_loss: 13.6433 - val_acc: 0.5736 - val_f1_score: 0.5338\n",
      "Epoch 35/300\n",
      "28/28 - 6s - loss: 13.4256 - acc: 0.6180 - f1_score: 0.6159 - val_loss: 13.6384 - val_acc: 0.5802 - val_f1_score: 0.5363\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00035: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.37      0.39       158\n",
      "           1       0.68      0.72      0.70       297\n",
      "\n",
      "    accuracy                           0.60       455\n",
      "   macro avg       0.55      0.55      0.55       455\n",
      "weighted avg       0.59      0.60      0.59       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1517\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 63s - loss: 14.3097 - acc: 0.4895 - f1_score: 0.4882 - val_loss: 13.3075 - val_acc: 0.8889 - val_f1_score: 0.4949\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 13.9877 - acc: 0.5328 - f1_score: 0.5257 - val_loss: 13.3097 - val_acc: 0.7379 - val_f1_score: 0.4808\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.8481 - acc: 0.5434 - f1_score: 0.5340 - val_loss: 13.2877 - val_acc: 0.7578 - val_f1_score: 0.5085\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.8009 - acc: 0.5479 - f1_score: 0.5402 - val_loss: 13.2963 - val_acc: 0.7037 - val_f1_score: 0.5032\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.7749 - acc: 0.5558 - f1_score: 0.5486 - val_loss: 13.4046 - val_acc: 0.4330 - val_f1_score: 0.3688\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7063 - acc: 0.5627 - f1_score: 0.5539 - val_loss: 13.3838 - val_acc: 0.5413 - val_f1_score: 0.4284\n",
      "Epoch 7/300\n",
      "28/28 - 6s - loss: 13.6435 - acc: 0.5766 - f1_score: 0.5696 - val_loss: 13.4600 - val_acc: 0.4672 - val_f1_score: 0.3855\n",
      "Epoch 8/300\n",
      "28/28 - 6s - loss: 13.6424 - acc: 0.5793 - f1_score: 0.5709 - val_loss: 13.4819 - val_acc: 0.4957 - val_f1_score: 0.3902\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.6087 - acc: 0.5801 - f1_score: 0.5716 - val_loss: 13.5051 - val_acc: 0.4872 - val_f1_score: 0.3707\n",
      "Epoch 10/300\n",
      "28/28 - 6s - loss: 13.5908 - acc: 0.5879 - f1_score: 0.5802 - val_loss: 13.7006 - val_acc: 0.3875 - val_f1_score: 0.3234\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.5411 - acc: 0.5940 - f1_score: 0.5866 - val_loss: 13.7661 - val_acc: 0.3903 - val_f1_score: 0.3279\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.5475 - acc: 0.5912 - f1_score: 0.5833 - val_loss: 13.8372 - val_acc: 0.3675 - val_f1_score: 0.3152\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5086 - acc: 0.6060 - f1_score: 0.5987 - val_loss: 13.9928 - val_acc: 0.3048 - val_f1_score: 0.2677\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.5022 - acc: 0.6098 - f1_score: 0.6013 - val_loss: 13.8176 - val_acc: 0.3903 - val_f1_score: 0.3170\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4750 - acc: 0.6175 - f1_score: 0.6102 - val_loss: 14.0574 - val_acc: 0.3134 - val_f1_score: 0.2695\n",
      "Epoch 16/300\n",
      "28/28 - 7s - loss: 13.4377 - acc: 0.6221 - f1_score: 0.6134 - val_loss: 13.9490 - val_acc: 0.3561 - val_f1_score: 0.2874\n",
      "Epoch 17/300\n",
      "28/28 - 8s - loss: 13.4402 - acc: 0.6309 - f1_score: 0.6238 - val_loss: 14.0323 - val_acc: 0.3618 - val_f1_score: 0.2991\n",
      "Epoch 18/300\n",
      "28/28 - 8s - loss: 13.4431 - acc: 0.6265 - f1_score: 0.6189 - val_loss: 14.0608 - val_acc: 0.3675 - val_f1_score: 0.3000\n",
      "Epoch 19/300\n",
      "28/28 - 8s - loss: 13.4036 - acc: 0.6427 - f1_score: 0.6346 - val_loss: 13.9670 - val_acc: 0.4245 - val_f1_score: 0.3439\n",
      "Epoch 20/300\n",
      "28/28 - 7s - loss: 13.3895 - acc: 0.6449 - f1_score: 0.6375 - val_loss: 13.8458 - val_acc: 0.4729 - val_f1_score: 0.3698\n",
      "Epoch 21/300\n",
      "28/28 - 6s - loss: 13.3944 - acc: 0.6454 - f1_score: 0.6391 - val_loss: 14.0264 - val_acc: 0.3903 - val_f1_score: 0.3252\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3645 - acc: 0.6520 - f1_score: 0.6439 - val_loss: 13.8823 - val_acc: 0.4701 - val_f1_score: 0.3646\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3582 - acc: 0.6587 - f1_score: 0.6521 - val_loss: 14.0795 - val_acc: 0.3903 - val_f1_score: 0.3304\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.86       276\n",
      "           1       0.31      0.11      0.16        75\n",
      "\n",
      "    accuracy                           0.76       351\n",
      "   macro avg       0.55      0.52      0.51       351\n",
      "weighted avg       0.69      0.76      0.71       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1544\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "29/29 - 60s - loss: 14.4093 - acc: 0.4895 - f1_score: 0.4878 - val_loss: 13.3958 - val_acc: 0.3654 - val_f1_score: 0.2790\n",
      "Epoch 2/300\n",
      "29/29 - 7s - loss: 13.9903 - acc: 0.5122 - f1_score: 0.5099 - val_loss: 13.4078 - val_acc: 0.3462 - val_f1_score: 0.2778\n",
      "Epoch 3/300\n",
      "29/29 - 6s - loss: 13.9109 - acc: 0.5218 - f1_score: 0.5183 - val_loss: 13.4167 - val_acc: 0.3750 - val_f1_score: 0.3429\n",
      "Epoch 4/300\n",
      "29/29 - 6s - loss: 13.8162 - acc: 0.5408 - f1_score: 0.5380 - val_loss: 13.4080 - val_acc: 0.4038 - val_f1_score: 0.4030\n",
      "Epoch 5/300\n",
      "29/29 - 6s - loss: 13.7730 - acc: 0.5482 - f1_score: 0.5449 - val_loss: 13.4043 - val_acc: 0.4519 - val_f1_score: 0.4507\n",
      "Epoch 6/300\n",
      "29/29 - 6s - loss: 13.7151 - acc: 0.5475 - f1_score: 0.5446 - val_loss: 13.4186 - val_acc: 0.4615 - val_f1_score: 0.4607\n",
      "Epoch 7/300\n",
      "29/29 - 6s - loss: 13.7026 - acc: 0.5575 - f1_score: 0.5546 - val_loss: 13.4002 - val_acc: 0.6058 - val_f1_score: 0.5922\n",
      "Epoch 8/300\n",
      "29/29 - 6s - loss: 13.6358 - acc: 0.5660 - f1_score: 0.5636 - val_loss: 13.3520 - val_acc: 0.6635 - val_f1_score: 0.6205\n",
      "Epoch 9/300\n",
      "29/29 - 7s - loss: 13.6225 - acc: 0.5769 - f1_score: 0.5739 - val_loss: 13.3299 - val_acc: 0.6442 - val_f1_score: 0.5927\n",
      "Epoch 10/300\n",
      "29/29 - 7s - loss: 13.5776 - acc: 0.5876 - f1_score: 0.5846 - val_loss: 13.3215 - val_acc: 0.7019 - val_f1_score: 0.6587\n",
      "Epoch 11/300\n",
      "29/29 - 6s - loss: 13.5609 - acc: 0.5863 - f1_score: 0.5832 - val_loss: 13.3443 - val_acc: 0.6731 - val_f1_score: 0.6340\n",
      "Epoch 12/300\n",
      "29/29 - 6s - loss: 13.5387 - acc: 0.5919 - f1_score: 0.5886 - val_loss: 13.3676 - val_acc: 0.6827 - val_f1_score: 0.6472\n",
      "Epoch 13/300\n",
      "29/29 - 7s - loss: 13.5035 - acc: 0.6107 - f1_score: 0.6085 - val_loss: 13.4063 - val_acc: 0.6827 - val_f1_score: 0.6308\n",
      "Epoch 14/300\n",
      "29/29 - 6s - loss: 13.5007 - acc: 0.6071 - f1_score: 0.6037 - val_loss: 13.5115 - val_acc: 0.6346 - val_f1_score: 0.6015\n",
      "Epoch 15/300\n",
      "29/29 - 7s - loss: 13.4749 - acc: 0.6104 - f1_score: 0.6074 - val_loss: 13.4897 - val_acc: 0.6058 - val_f1_score: 0.5890\n",
      "Epoch 16/300\n",
      "29/29 - 6s - loss: 13.4545 - acc: 0.6162 - f1_score: 0.6135 - val_loss: 13.4872 - val_acc: 0.6346 - val_f1_score: 0.6061\n",
      "Epoch 17/300\n",
      "29/29 - 7s - loss: 13.4288 - acc: 0.6238 - f1_score: 0.6208 - val_loss: 13.4627 - val_acc: 0.6442 - val_f1_score: 0.6185\n",
      "Epoch 18/300\n",
      "29/29 - 7s - loss: 13.4194 - acc: 0.6287 - f1_score: 0.6260 - val_loss: 13.4575 - val_acc: 0.6923 - val_f1_score: 0.6504\n",
      "Epoch 19/300\n",
      "29/29 - 7s - loss: 13.4173 - acc: 0.6346 - f1_score: 0.6314 - val_loss: 13.4053 - val_acc: 0.6442 - val_f1_score: 0.6224\n",
      "Epoch 20/300\n",
      "29/29 - 7s - loss: 13.3825 - acc: 0.6441 - f1_score: 0.6410 - val_loss: 13.4439 - val_acc: 0.6346 - val_f1_score: 0.6206\n",
      "Epoch 21/300\n",
      "29/29 - 7s - loss: 13.3825 - acc: 0.6410 - f1_score: 0.6390 - val_loss: 13.4529 - val_acc: 0.6538 - val_f1_score: 0.6268\n",
      "Epoch 22/300\n",
      "29/29 - 7s - loss: 13.3795 - acc: 0.6437 - f1_score: 0.6407 - val_loss: 13.4804 - val_acc: 0.6538 - val_f1_score: 0.6344\n",
      "Epoch 23/300\n",
      "29/29 - 7s - loss: 13.3555 - acc: 0.6608 - f1_score: 0.6585 - val_loss: 13.5160 - val_acc: 0.6250 - val_f1_score: 0.5979\n",
      "Epoch 24/300\n",
      "29/29 - 7s - loss: 13.3333 - acc: 0.6682 - f1_score: 0.6656 - val_loss: 13.4875 - val_acc: 0.6442 - val_f1_score: 0.6185\n",
      "Epoch 25/300\n",
      "29/29 - 7s - loss: 13.3182 - acc: 0.6763 - f1_score: 0.6737 - val_loss: 13.4876 - val_acc: 0.6923 - val_f1_score: 0.6601\n",
      "Epoch 26/300\n",
      "29/29 - 7s - loss: 13.3024 - acc: 0.6757 - f1_score: 0.6727 - val_loss: 13.4709 - val_acc: 0.6731 - val_f1_score: 0.6475\n",
      "Epoch 27/300\n",
      "29/29 - 7s - loss: 13.3103 - acc: 0.6733 - f1_score: 0.6710 - val_loss: 13.4438 - val_acc: 0.6731 - val_f1_score: 0.6513\n",
      "Epoch 28/300\n",
      "29/29 - 7s - loss: 13.2993 - acc: 0.6841 - f1_score: 0.6816 - val_loss: 13.4414 - val_acc: 0.6827 - val_f1_score: 0.6632\n",
      "Epoch 29/300\n",
      "29/29 - 7s - loss: 13.2801 - acc: 0.6907 - f1_score: 0.6883 - val_loss: 13.4186 - val_acc: 0.7019 - val_f1_score: 0.6768\n",
      "Epoch 30/300\n",
      "29/29 - 7s - loss: 13.2755 - acc: 0.6953 - f1_score: 0.6930 - val_loss: 13.4218 - val_acc: 0.6731 - val_f1_score: 0.6513\n",
      "Epoch 31/300\n",
      "29/29 - 7s - loss: 13.2647 - acc: 0.7011 - f1_score: 0.6989 - val_loss: 13.4139 - val_acc: 0.6538 - val_f1_score: 0.6308\n",
      "Epoch 32/300\n",
      "29/29 - 7s - loss: 13.2555 - acc: 0.7035 - f1_score: 0.7013 - val_loss: 13.3887 - val_acc: 0.6442 - val_f1_score: 0.6224\n",
      "Epoch 33/300\n",
      "29/29 - 6s - loss: 13.2612 - acc: 0.7029 - f1_score: 0.7006 - val_loss: 13.3877 - val_acc: 0.6635 - val_f1_score: 0.6428\n",
      "Epoch 34/300\n",
      "29/29 - 7s - loss: 13.2530 - acc: 0.7076 - f1_score: 0.7055 - val_loss: 13.3750 - val_acc: 0.6923 - val_f1_score: 0.6683\n",
      "Epoch 35/300\n",
      "29/29 - 7s - loss: 13.2398 - acc: 0.7110 - f1_score: 0.7087 - val_loss: 13.3788 - val_acc: 0.6731 - val_f1_score: 0.6547\n",
      "Epoch 36/300\n",
      "29/29 - 6s - loss: 13.2235 - acc: 0.7222 - f1_score: 0.7202 - val_loss: 13.3733 - val_acc: 0.6827 - val_f1_score: 0.6632\n",
      "Epoch 37/300\n",
      "29/29 - 7s - loss: 13.2171 - acc: 0.7200 - f1_score: 0.7177 - val_loss: 13.3877 - val_acc: 0.6731 - val_f1_score: 0.6475\n",
      "Epoch 38/300\n",
      "29/29 - 7s - loss: 13.2000 - acc: 0.7351 - f1_score: 0.7332 - val_loss: 13.3750 - val_acc: 0.6635 - val_f1_score: 0.6428\n",
      "Epoch 39/300\n",
      "29/29 - 6s - loss: 13.2141 - acc: 0.7273 - f1_score: 0.7251 - val_loss: 13.3971 - val_acc: 0.6442 - val_f1_score: 0.6259\n",
      "Epoch 40/300\n",
      "29/29 - 7s - loss: 13.1933 - acc: 0.7383 - f1_score: 0.7363 - val_loss: 13.3980 - val_acc: 0.6442 - val_f1_score: 0.6259\n",
      "Epoch 41/300\n",
      "29/29 - 7s - loss: 13.1795 - acc: 0.7465 - f1_score: 0.7449 - val_loss: 13.3939 - val_acc: 0.6923 - val_f1_score: 0.6644\n",
      "Epoch 42/300\n",
      "29/29 - 7s - loss: 13.1807 - acc: 0.7457 - f1_score: 0.7438 - val_loss: 13.3962 - val_acc: 0.6635 - val_f1_score: 0.6391\n",
      "Epoch 43/300\n",
      "29/29 - 7s - loss: 13.1781 - acc: 0.7453 - f1_score: 0.7437 - val_loss: 13.3881 - val_acc: 0.6923 - val_f1_score: 0.6601\n",
      "Epoch 44/300\n",
      "29/29 - 6s - loss: 13.1641 - acc: 0.7584 - f1_score: 0.7565 - val_loss: 13.3742 - val_acc: 0.6827 - val_f1_score: 0.6472\n",
      "Epoch 45/300\n",
      "29/29 - 7s - loss: 13.1688 - acc: 0.7533 - f1_score: 0.7514 - val_loss: 13.3763 - val_acc: 0.6635 - val_f1_score: 0.6306\n",
      "Epoch 46/300\n",
      "29/29 - 7s - loss: 13.1612 - acc: 0.7601 - f1_score: 0.7581 - val_loss: 13.4346 - val_acc: 0.6538 - val_f1_score: 0.6124\n",
      "Epoch 47/300\n",
      "29/29 - 7s - loss: 13.1551 - acc: 0.7666 - f1_score: 0.7643 - val_loss: 13.3931 - val_acc: 0.6827 - val_f1_score: 0.6559\n",
      "Epoch 48/300\n",
      "29/29 - 7s - loss: 13.1396 - acc: 0.7683 - f1_score: 0.7666 - val_loss: 13.3990 - val_acc: 0.6923 - val_f1_score: 0.6601\n",
      "Epoch 49/300\n",
      "29/29 - 7s - loss: 13.1276 - acc: 0.7816 - f1_score: 0.7798 - val_loss: 13.3889 - val_acc: 0.6827 - val_f1_score: 0.6559\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00049: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.59        36\n",
      "           1       0.78      0.75      0.77        68\n",
      "\n",
      "    accuracy                           0.70       104\n",
      "   macro avg       0.67      0.68      0.68       104\n",
      "weighted avg       0.71      0.70      0.70       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1624\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 64s - loss: 14.4905 - acc: 0.4843 - f1_score: 0.4812 - val_loss: 13.3465 - val_acc: 0.6296 - val_f1_score: 0.4072\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 14.0616 - acc: 0.5251 - f1_score: 0.5221 - val_loss: 13.3458 - val_acc: 0.5926 - val_f1_score: 0.4638\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.9048 - acc: 0.5360 - f1_score: 0.5299 - val_loss: 13.3332 - val_acc: 0.6239 - val_f1_score: 0.4793\n",
      "Epoch 4/300\n",
      "28/28 - 7s - loss: 13.8761 - acc: 0.5340 - f1_score: 0.5294 - val_loss: 13.3525 - val_acc: 0.5840 - val_f1_score: 0.5321\n",
      "Epoch 5/300\n",
      "28/28 - 7s - loss: 13.7997 - acc: 0.5454 - f1_score: 0.5409 - val_loss: 13.3888 - val_acc: 0.5185 - val_f1_score: 0.5093\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7838 - acc: 0.5514 - f1_score: 0.5463 - val_loss: 13.4104 - val_acc: 0.5128 - val_f1_score: 0.5050\n",
      "Epoch 7/300\n",
      "28/28 - 6s - loss: 13.7131 - acc: 0.5722 - f1_score: 0.5669 - val_loss: 13.4668 - val_acc: 0.4786 - val_f1_score: 0.4731\n",
      "Epoch 8/300\n",
      "28/28 - 6s - loss: 13.6695 - acc: 0.5749 - f1_score: 0.5697 - val_loss: 13.4811 - val_acc: 0.4758 - val_f1_score: 0.4705\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.6322 - acc: 0.5776 - f1_score: 0.5730 - val_loss: 13.4774 - val_acc: 0.4758 - val_f1_score: 0.4626\n",
      "Epoch 10/300\n",
      "28/28 - 6s - loss: 13.5973 - acc: 0.5853 - f1_score: 0.5809 - val_loss: 13.5385 - val_acc: 0.4758 - val_f1_score: 0.4685\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.5709 - acc: 0.5947 - f1_score: 0.5907 - val_loss: 13.6469 - val_acc: 0.4530 - val_f1_score: 0.4492\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.5691 - acc: 0.5833 - f1_score: 0.5785 - val_loss: 13.6869 - val_acc: 0.4758 - val_f1_score: 0.4685\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5182 - acc: 0.6013 - f1_score: 0.5966 - val_loss: 13.7539 - val_acc: 0.4729 - val_f1_score: 0.4721\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.5084 - acc: 0.6027 - f1_score: 0.5973 - val_loss: 13.6554 - val_acc: 0.4986 - val_f1_score: 0.4946\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4632 - acc: 0.6170 - f1_score: 0.6131 - val_loss: 13.7880 - val_acc: 0.4701 - val_f1_score: 0.4701\n",
      "Epoch 16/300\n",
      "28/28 - 7s - loss: 13.4593 - acc: 0.6192 - f1_score: 0.6135 - val_loss: 13.6956 - val_acc: 0.4929 - val_f1_score: 0.4819\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.4425 - acc: 0.6335 - f1_score: 0.6289 - val_loss: 13.7060 - val_acc: 0.4558 - val_f1_score: 0.4524\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.4350 - acc: 0.6349 - f1_score: 0.6307 - val_loss: 13.7251 - val_acc: 0.4615 - val_f1_score: 0.4590\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.4014 - acc: 0.6392 - f1_score: 0.6337 - val_loss: 13.6818 - val_acc: 0.4729 - val_f1_score: 0.4685\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.3906 - acc: 0.6417 - f1_score: 0.6377 - val_loss: 13.6596 - val_acc: 0.4815 - val_f1_score: 0.4750\n",
      "Epoch 21/300\n",
      "28/28 - 6s - loss: 13.3760 - acc: 0.6521 - f1_score: 0.6481 - val_loss: 13.7207 - val_acc: 0.4644 - val_f1_score: 0.4612\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3606 - acc: 0.6530 - f1_score: 0.6483 - val_loss: 13.6999 - val_acc: 0.4615 - val_f1_score: 0.4570\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3556 - acc: 0.6593 - f1_score: 0.6553 - val_loss: 13.7494 - val_acc: 0.4615 - val_f1_score: 0.4598\n",
      "Epoch 24/300\n",
      "28/28 - 6s - loss: 13.3495 - acc: 0.6640 - f1_score: 0.6597 - val_loss: 13.7402 - val_acc: 0.4558 - val_f1_score: 0.4544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001519D05C820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       234\n",
      "           1       0.38      0.38      0.38       117\n",
      "\n",
      "    accuracy                           0.58       351\n",
      "   macro avg       0.53      0.53      0.53       351\n",
      "weighted avg       0.58      0.58      0.58       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1674\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 68s - loss: 14.3563 - acc: 0.4955 - f1_score: 0.4914 - val_loss: 13.4566 - val_acc: 0.0677 - val_f1_score: 0.0676\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 14.0208 - acc: 0.5143 - f1_score: 0.5138 - val_loss: 13.4496 - val_acc: 0.1508 - val_f1_score: 0.1449\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.9223 - acc: 0.5251 - f1_score: 0.5245 - val_loss: 13.3885 - val_acc: 0.4523 - val_f1_score: 0.3499\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.8084 - acc: 0.5372 - f1_score: 0.5361 - val_loss: 13.3529 - val_acc: 0.5846 - val_f1_score: 0.4177\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.7538 - acc: 0.5593 - f1_score: 0.5584 - val_loss: 13.3068 - val_acc: 0.6677 - val_f1_score: 0.4551\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7009 - acc: 0.5552 - f1_score: 0.5540 - val_loss: 13.3641 - val_acc: 0.5908 - val_f1_score: 0.4263\n",
      "Epoch 7/300\n",
      "28/28 - 6s - loss: 13.6570 - acc: 0.5659 - f1_score: 0.5652 - val_loss: 13.2898 - val_acc: 0.6985 - val_f1_score: 0.4714\n",
      "Epoch 8/300\n",
      "28/28 - 7s - loss: 13.6507 - acc: 0.5785 - f1_score: 0.5774 - val_loss: 13.3643 - val_acc: 0.6092 - val_f1_score: 0.4249\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.5985 - acc: 0.5765 - f1_score: 0.5757 - val_loss: 13.3723 - val_acc: 0.5969 - val_f1_score: 0.4297\n",
      "Epoch 10/300\n",
      "28/28 - 6s - loss: 13.5706 - acc: 0.5901 - f1_score: 0.5892 - val_loss: 13.3416 - val_acc: 0.6123 - val_f1_score: 0.4204\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.5412 - acc: 0.5924 - f1_score: 0.5913 - val_loss: 13.3963 - val_acc: 0.5938 - val_f1_score: 0.4112\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.5534 - acc: 0.5911 - f1_score: 0.5900 - val_loss: 13.4826 - val_acc: 0.5785 - val_f1_score: 0.4091\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5423 - acc: 0.5982 - f1_score: 0.5973 - val_loss: 13.5341 - val_acc: 0.5477 - val_f1_score: 0.3828\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.4766 - acc: 0.6121 - f1_score: 0.6113 - val_loss: 13.6049 - val_acc: 0.5231 - val_f1_score: 0.3601\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4724 - acc: 0.6150 - f1_score: 0.6140 - val_loss: 13.5827 - val_acc: 0.5292 - val_f1_score: 0.3631\n",
      "Epoch 16/300\n",
      "28/28 - 6s - loss: 13.4455 - acc: 0.6127 - f1_score: 0.6117 - val_loss: 13.5839 - val_acc: 0.5538 - val_f1_score: 0.3858\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.4394 - acc: 0.6211 - f1_score: 0.6205 - val_loss: 13.5159 - val_acc: 0.5723 - val_f1_score: 0.4005\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.4093 - acc: 0.6405 - f1_score: 0.6397 - val_loss: 13.6783 - val_acc: 0.5108 - val_f1_score: 0.3738\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.4016 - acc: 0.6438 - f1_score: 0.6430 - val_loss: 13.6551 - val_acc: 0.5046 - val_f1_score: 0.3705\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.3904 - acc: 0.6387 - f1_score: 0.6377 - val_loss: 13.7121 - val_acc: 0.4954 - val_f1_score: 0.3700\n",
      "Epoch 21/300\n",
      "28/28 - 6s - loss: 13.3964 - acc: 0.6422 - f1_score: 0.6411 - val_loss: 13.7972 - val_acc: 0.4308 - val_f1_score: 0.3263\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3457 - acc: 0.6645 - f1_score: 0.6638 - val_loss: 13.7217 - val_acc: 0.4554 - val_f1_score: 0.3438\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3477 - acc: 0.6670 - f1_score: 0.6662 - val_loss: 13.7178 - val_acc: 0.4554 - val_f1_score: 0.3438\n",
      "Epoch 24/300\n",
      "28/28 - 6s - loss: 13.3552 - acc: 0.6587 - f1_score: 0.6580 - val_loss: 13.7038 - val_acc: 0.4708 - val_f1_score: 0.3523\n",
      "Epoch 25/300\n",
      "28/28 - 6s - loss: 13.3237 - acc: 0.6668 - f1_score: 0.6660 - val_loss: 13.8178 - val_acc: 0.4338 - val_f1_score: 0.3318\n",
      "Epoch 26/300\n",
      "28/28 - 6s - loss: 13.3201 - acc: 0.6740 - f1_score: 0.6735 - val_loss: 13.7138 - val_acc: 0.4646 - val_f1_score: 0.3489\n",
      "Epoch 27/300\n",
      "28/28 - 6s - loss: 13.3253 - acc: 0.6714 - f1_score: 0.6710 - val_loss: 13.6737 - val_acc: 0.4892 - val_f1_score: 0.3623\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000151A36A3280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.07      0.12        99\n",
      "           1       0.71      0.97      0.82       226\n",
      "\n",
      "    accuracy                           0.70       325\n",
      "   macro avg       0.62      0.52      0.47       325\n",
      "weighted avg       0.65      0.70      0.61       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1688\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 81s - loss: 14.3572 - acc: 0.4870 - f1_score: 0.4855 - val_loss: 13.3639 - val_acc: 0.5154 - val_f1_score: 0.4000\n",
      "Epoch 2/300\n",
      "28/28 - 7s - loss: 14.0127 - acc: 0.5239 - f1_score: 0.5192 - val_loss: 13.3623 - val_acc: 0.5564 - val_f1_score: 0.5158\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.8976 - acc: 0.5363 - f1_score: 0.5314 - val_loss: 13.3672 - val_acc: 0.5513 - val_f1_score: 0.5117\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.8302 - acc: 0.5357 - f1_score: 0.5323 - val_loss: 13.3739 - val_acc: 0.5436 - val_f1_score: 0.5214\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.7801 - acc: 0.5387 - f1_score: 0.5353 - val_loss: 13.3816 - val_acc: 0.5308 - val_f1_score: 0.5198\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7211 - acc: 0.5589 - f1_score: 0.5554 - val_loss: 13.3714 - val_acc: 0.5667 - val_f1_score: 0.5518\n",
      "Epoch 7/300\n",
      "28/28 - 6s - loss: 13.6828 - acc: 0.5670 - f1_score: 0.5632 - val_loss: 13.3757 - val_acc: 0.5769 - val_f1_score: 0.5598\n",
      "Epoch 8/300\n",
      "28/28 - 6s - loss: 13.6463 - acc: 0.5726 - f1_score: 0.5690 - val_loss: 13.4445 - val_acc: 0.5436 - val_f1_score: 0.5009\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.6291 - acc: 0.5791 - f1_score: 0.5752 - val_loss: 13.4696 - val_acc: 0.5282 - val_f1_score: 0.4921\n",
      "Epoch 10/300\n",
      "28/28 - 6s - loss: 13.5805 - acc: 0.5916 - f1_score: 0.5881 - val_loss: 13.4956 - val_acc: 0.5077 - val_f1_score: 0.4773\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.5519 - acc: 0.5839 - f1_score: 0.5792 - val_loss: 13.4954 - val_acc: 0.5564 - val_f1_score: 0.5504\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.5516 - acc: 0.5910 - f1_score: 0.5881 - val_loss: 13.6368 - val_acc: 0.5487 - val_f1_score: 0.5268\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5012 - acc: 0.6075 - f1_score: 0.6034 - val_loss: 13.5706 - val_acc: 0.5615 - val_f1_score: 0.5580\n",
      "Epoch 14/300\n",
      "28/28 - 7s - loss: 13.5063 - acc: 0.5978 - f1_score: 0.5947 - val_loss: 13.6385 - val_acc: 0.5641 - val_f1_score: 0.5585\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4630 - acc: 0.6188 - f1_score: 0.6153 - val_loss: 13.6773 - val_acc: 0.5436 - val_f1_score: 0.5360\n",
      "Epoch 16/300\n",
      "28/28 - 6s - loss: 13.4820 - acc: 0.6124 - f1_score: 0.6084 - val_loss: 13.6586 - val_acc: 0.5872 - val_f1_score: 0.5838\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.4441 - acc: 0.6162 - f1_score: 0.6118 - val_loss: 13.6381 - val_acc: 0.5974 - val_f1_score: 0.5970\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.4521 - acc: 0.6200 - f1_score: 0.6162 - val_loss: 13.6658 - val_acc: 0.6154 - val_f1_score: 0.6153\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.4089 - acc: 0.6354 - f1_score: 0.6320 - val_loss: 13.7061 - val_acc: 0.5949 - val_f1_score: 0.5936\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.4008 - acc: 0.6462 - f1_score: 0.6428 - val_loss: 13.7317 - val_acc: 0.6051 - val_f1_score: 0.6025\n",
      "Epoch 21/300\n",
      "28/28 - 7s - loss: 13.3886 - acc: 0.6466 - f1_score: 0.6428 - val_loss: 13.7501 - val_acc: 0.5974 - val_f1_score: 0.5945\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3798 - acc: 0.6446 - f1_score: 0.6413 - val_loss: 13.7735 - val_acc: 0.5974 - val_f1_score: 0.5958\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3743 - acc: 0.6460 - f1_score: 0.6431 - val_loss: 13.8002 - val_acc: 0.5769 - val_f1_score: 0.5722\n",
      "Epoch 24/300\n",
      "28/28 - 6s - loss: 13.3505 - acc: 0.6488 - f1_score: 0.6448 - val_loss: 13.7868 - val_acc: 0.5718 - val_f1_score: 0.5705\n",
      "Epoch 25/300\n",
      "28/28 - 6s - loss: 13.3479 - acc: 0.6589 - f1_score: 0.6555 - val_loss: 13.8498 - val_acc: 0.5564 - val_f1_score: 0.5487\n",
      "Epoch 26/300\n",
      "28/28 - 6s - loss: 13.3335 - acc: 0.6602 - f1_score: 0.6566 - val_loss: 13.8182 - val_acc: 0.5718 - val_f1_score: 0.5703\n",
      "Epoch 27/300\n",
      "28/28 - 7s - loss: 13.3310 - acc: 0.6660 - f1_score: 0.6626 - val_loss: 13.8596 - val_acc: 0.5538 - val_f1_score: 0.5458\n",
      "Epoch 28/300\n",
      "28/28 - 6s - loss: 13.3143 - acc: 0.6678 - f1_score: 0.6641 - val_loss: 13.8083 - val_acc: 0.5641 - val_f1_score: 0.5611\n",
      "Epoch 29/300\n",
      "28/28 - 6s - loss: 13.2993 - acc: 0.6774 - f1_score: 0.6738 - val_loss: 13.7899 - val_acc: 0.5795 - val_f1_score: 0.5793\n",
      "Epoch 30/300\n",
      "28/28 - 6s - loss: 13.2956 - acc: 0.6816 - f1_score: 0.6781 - val_loss: 13.7853 - val_acc: 0.5744 - val_f1_score: 0.5732\n",
      "Epoch 31/300\n",
      "28/28 - 6s - loss: 13.2863 - acc: 0.6883 - f1_score: 0.6852 - val_loss: 13.8012 - val_acc: 0.5744 - val_f1_score: 0.5732\n",
      "Epoch 32/300\n",
      "28/28 - 6s - loss: 13.2645 - acc: 0.6963 - f1_score: 0.6935 - val_loss: 13.7830 - val_acc: 0.5641 - val_f1_score: 0.5627\n",
      "Epoch 33/300\n",
      "28/28 - 6s - loss: 13.2793 - acc: 0.6849 - f1_score: 0.6820 - val_loss: 13.8883 - val_acc: 0.5564 - val_f1_score: 0.5445\n",
      "Epoch 34/300\n",
      "28/28 - 7s - loss: 13.2704 - acc: 0.6979 - f1_score: 0.6943 - val_loss: 13.8416 - val_acc: 0.5667 - val_f1_score: 0.5591\n",
      "Epoch 35/300\n",
      "28/28 - 6s - loss: 13.2609 - acc: 0.7019 - f1_score: 0.6987 - val_loss: 13.7840 - val_acc: 0.5538 - val_f1_score: 0.5512\n",
      "Epoch 36/300\n",
      "28/28 - 6s - loss: 13.2418 - acc: 0.7116 - f1_score: 0.7086 - val_loss: 13.7599 - val_acc: 0.5641 - val_f1_score: 0.5630\n",
      "Epoch 37/300\n",
      "28/28 - 6s - loss: 13.2448 - acc: 0.7045 - f1_score: 0.7014 - val_loss: 13.7777 - val_acc: 0.5667 - val_f1_score: 0.5649\n",
      "Epoch 38/300\n",
      "28/28 - 6s - loss: 13.2272 - acc: 0.7148 - f1_score: 0.7120 - val_loss: 13.7889 - val_acc: 0.5615 - val_f1_score: 0.5584\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000151B92B11F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61       178\n",
      "           1       0.67      0.58      0.62       212\n",
      "\n",
      "    accuracy                           0.62       390\n",
      "   macro avg       0.62      0.62      0.62       390\n",
      "weighted avg       0.62      0.62      0.62       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1717\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "28/28 - 89s - loss: 14.2608 - acc: 0.4977 - f1_score: 0.4961 - val_loss: 13.3600 - val_acc: 0.4829 - val_f1_score: 0.3955\n",
      "Epoch 2/300\n",
      "28/28 - 6s - loss: 13.9584 - acc: 0.5288 - f1_score: 0.5261 - val_loss: 13.3531 - val_acc: 0.5128 - val_f1_score: 0.4734\n",
      "Epoch 3/300\n",
      "28/28 - 6s - loss: 13.8634 - acc: 0.5362 - f1_score: 0.5326 - val_loss: 13.3560 - val_acc: 0.5043 - val_f1_score: 0.4908\n",
      "Epoch 4/300\n",
      "28/28 - 6s - loss: 13.7892 - acc: 0.5359 - f1_score: 0.5330 - val_loss: 13.3583 - val_acc: 0.4872 - val_f1_score: 0.4834\n",
      "Epoch 5/300\n",
      "28/28 - 6s - loss: 13.7524 - acc: 0.5457 - f1_score: 0.5427 - val_loss: 13.3772 - val_acc: 0.4487 - val_f1_score: 0.4413\n",
      "Epoch 6/300\n",
      "28/28 - 6s - loss: 13.7165 - acc: 0.5494 - f1_score: 0.5462 - val_loss: 13.3962 - val_acc: 0.5000 - val_f1_score: 0.4989\n",
      "Epoch 7/300\n",
      "28/28 - 6s - loss: 13.6670 - acc: 0.5619 - f1_score: 0.5589 - val_loss: 13.4355 - val_acc: 0.4786 - val_f1_score: 0.4762\n",
      "Epoch 8/300\n",
      "28/28 - 6s - loss: 13.6283 - acc: 0.5731 - f1_score: 0.5700 - val_loss: 13.4554 - val_acc: 0.5000 - val_f1_score: 0.4922\n",
      "Epoch 9/300\n",
      "28/28 - 6s - loss: 13.6127 - acc: 0.5773 - f1_score: 0.5744 - val_loss: 13.5046 - val_acc: 0.5214 - val_f1_score: 0.5038\n",
      "Epoch 10/300\n",
      "28/28 - 6s - loss: 13.5769 - acc: 0.5829 - f1_score: 0.5801 - val_loss: 13.5498 - val_acc: 0.5128 - val_f1_score: 0.4832\n",
      "Epoch 11/300\n",
      "28/28 - 6s - loss: 13.5708 - acc: 0.5900 - f1_score: 0.5873 - val_loss: 13.6033 - val_acc: 0.4872 - val_f1_score: 0.4336\n",
      "Epoch 12/300\n",
      "28/28 - 6s - loss: 13.5473 - acc: 0.5909 - f1_score: 0.5871 - val_loss: 13.5477 - val_acc: 0.5598 - val_f1_score: 0.5379\n",
      "Epoch 13/300\n",
      "28/28 - 6s - loss: 13.5026 - acc: 0.6091 - f1_score: 0.6058 - val_loss: 13.5635 - val_acc: 0.5556 - val_f1_score: 0.5422\n",
      "Epoch 14/300\n",
      "28/28 - 6s - loss: 13.4760 - acc: 0.6115 - f1_score: 0.6089 - val_loss: 13.6382 - val_acc: 0.5470 - val_f1_score: 0.5319\n",
      "Epoch 15/300\n",
      "28/28 - 6s - loss: 13.4592 - acc: 0.6224 - f1_score: 0.6186 - val_loss: 13.6236 - val_acc: 0.5256 - val_f1_score: 0.5231\n",
      "Epoch 16/300\n",
      "28/28 - 6s - loss: 13.4648 - acc: 0.6133 - f1_score: 0.6107 - val_loss: 13.6178 - val_acc: 0.5299 - val_f1_score: 0.5291\n",
      "Epoch 17/300\n",
      "28/28 - 6s - loss: 13.4392 - acc: 0.6350 - f1_score: 0.6323 - val_loss: 13.6594 - val_acc: 0.5043 - val_f1_score: 0.5041\n",
      "Epoch 18/300\n",
      "28/28 - 6s - loss: 13.4053 - acc: 0.6381 - f1_score: 0.6353 - val_loss: 13.7068 - val_acc: 0.5043 - val_f1_score: 0.5037\n",
      "Epoch 19/300\n",
      "28/28 - 6s - loss: 13.3915 - acc: 0.6385 - f1_score: 0.6358 - val_loss: 13.7863 - val_acc: 0.5085 - val_f1_score: 0.5070\n",
      "Epoch 20/300\n",
      "28/28 - 6s - loss: 13.4032 - acc: 0.6305 - f1_score: 0.6278 - val_loss: 13.8293 - val_acc: 0.4872 - val_f1_score: 0.4862\n",
      "Epoch 21/300\n",
      "28/28 - 6s - loss: 13.3913 - acc: 0.6430 - f1_score: 0.6401 - val_loss: 13.8120 - val_acc: 0.4957 - val_f1_score: 0.4956\n",
      "Epoch 22/300\n",
      "28/28 - 6s - loss: 13.3750 - acc: 0.6466 - f1_score: 0.6440 - val_loss: 13.7955 - val_acc: 0.4957 - val_f1_score: 0.4957\n",
      "Epoch 23/300\n",
      "28/28 - 6s - loss: 13.3471 - acc: 0.6581 - f1_score: 0.6548 - val_loss: 13.8081 - val_acc: 0.5171 - val_f1_score: 0.5164\n",
      "Epoch 24/300\n",
      "28/28 - 6s - loss: 13.3346 - acc: 0.6651 - f1_score: 0.6624 - val_loss: 13.8080 - val_acc: 0.5342 - val_f1_score: 0.5332\n",
      "Epoch 25/300\n",
      "28/28 - 6s - loss: 13.3252 - acc: 0.6747 - f1_score: 0.6722 - val_loss: 13.8047 - val_acc: 0.5470 - val_f1_score: 0.5470\n",
      "Epoch 26/300\n",
      "28/28 - 6s - loss: 13.3081 - acc: 0.6727 - f1_score: 0.6704 - val_loss: 13.7600 - val_acc: 0.5342 - val_f1_score: 0.5335\n",
      "Epoch 27/300\n",
      "28/28 - 6s - loss: 13.3044 - acc: 0.6831 - f1_score: 0.6801 - val_loss: 13.7463 - val_acc: 0.5214 - val_f1_score: 0.5205\n",
      "Epoch 28/300\n",
      "28/28 - 6s - loss: 13.2887 - acc: 0.6824 - f1_score: 0.6797 - val_loss: 13.7795 - val_acc: 0.5256 - val_f1_score: 0.5254\n",
      "Epoch 29/300\n",
      "28/28 - 6s - loss: 13.2918 - acc: 0.6833 - f1_score: 0.6807 - val_loss: 13.7393 - val_acc: 0.5299 - val_f1_score: 0.5291\n",
      "Epoch 30/300\n",
      "28/28 - 7s - loss: 13.2837 - acc: 0.6870 - f1_score: 0.6846 - val_loss: 13.7312 - val_acc: 0.5470 - val_f1_score: 0.5469\n",
      "Epoch 31/300\n",
      "28/28 - 6s - loss: 13.2763 - acc: 0.6923 - f1_score: 0.6896 - val_loss: 13.7334 - val_acc: 0.5385 - val_f1_score: 0.5384\n",
      "Epoch 32/300\n",
      "28/28 - 7s - loss: 13.2637 - acc: 0.6962 - f1_score: 0.6939 - val_loss: 13.7156 - val_acc: 0.5427 - val_f1_score: 0.5413\n",
      "Epoch 33/300\n",
      "28/28 - 6s - loss: 13.2535 - acc: 0.7023 - f1_score: 0.7001 - val_loss: 13.7230 - val_acc: 0.5342 - val_f1_score: 0.5323\n",
      "Epoch 34/300\n",
      "28/28 - 6s - loss: 13.2274 - acc: 0.7219 - f1_score: 0.7197 - val_loss: 13.7053 - val_acc: 0.5299 - val_f1_score: 0.5287\n",
      "Epoch 35/300\n",
      "28/28 - 6s - loss: 13.2202 - acc: 0.7196 - f1_score: 0.7169 - val_loss: 13.7466 - val_acc: 0.5299 - val_f1_score: 0.5231\n",
      "Epoch 36/300\n",
      "28/28 - 6s - loss: 13.2187 - acc: 0.7147 - f1_score: 0.7128 - val_loss: 13.6783 - val_acc: 0.5513 - val_f1_score: 0.5506\n",
      "Epoch 37/300\n",
      "28/28 - 6s - loss: 13.2270 - acc: 0.7151 - f1_score: 0.7124 - val_loss: 13.7182 - val_acc: 0.5342 - val_f1_score: 0.5296\n",
      "Epoch 38/300\n",
      "28/28 - 6s - loss: 13.2195 - acc: 0.7238 - f1_score: 0.7217 - val_loss: 13.7063 - val_acc: 0.5299 - val_f1_score: 0.5294\n",
      "Epoch 39/300\n",
      "28/28 - 6s - loss: 13.2040 - acc: 0.7321 - f1_score: 0.7299 - val_loss: 13.7382 - val_acc: 0.5299 - val_f1_score: 0.5271\n",
      "Epoch 40/300\n",
      "28/28 - 6s - loss: 13.1873 - acc: 0.7415 - f1_score: 0.7393 - val_loss: 13.7453 - val_acc: 0.5342 - val_f1_score: 0.5323\n",
      "Epoch 41/300\n",
      "28/28 - 6s - loss: 13.1784 - acc: 0.7447 - f1_score: 0.7427 - val_loss: 13.7356 - val_acc: 0.5171 - val_f1_score: 0.5124\n",
      "Epoch 42/300\n",
      "28/28 - 6s - loss: 13.1761 - acc: 0.7491 - f1_score: 0.7469 - val_loss: 13.7483 - val_acc: 0.5085 - val_f1_score: 0.5046\n",
      "Epoch 43/300\n",
      "28/28 - 6s - loss: 13.1766 - acc: 0.7468 - f1_score: 0.7451 - val_loss: 13.7179 - val_acc: 0.5342 - val_f1_score: 0.5335\n",
      "Epoch 44/300\n",
      "28/28 - 6s - loss: 13.1626 - acc: 0.7582 - f1_score: 0.7557 - val_loss: 13.7544 - val_acc: 0.5385 - val_f1_score: 0.5297\n",
      "Epoch 45/300\n",
      "28/28 - 6s - loss: 13.1835 - acc: 0.7488 - f1_score: 0.7469 - val_loss: 13.7234 - val_acc: 0.5684 - val_f1_score: 0.5680\n",
      "Epoch 46/300\n",
      "28/28 - 6s - loss: 13.1586 - acc: 0.7551 - f1_score: 0.7532 - val_loss: 13.7135 - val_acc: 0.5684 - val_f1_score: 0.5655\n",
      "Epoch 47/300\n",
      "28/28 - 6s - loss: 13.1361 - acc: 0.7743 - f1_score: 0.7726 - val_loss: 13.6979 - val_acc: 0.5641 - val_f1_score: 0.5602\n",
      "Epoch 48/300\n",
      "28/28 - 6s - loss: 13.1321 - acc: 0.7778 - f1_score: 0.7759 - val_loss: 13.6910 - val_acc: 0.5598 - val_f1_score: 0.5575\n",
      "Epoch 49/300\n",
      "28/28 - 6s - loss: 13.1388 - acc: 0.7736 - f1_score: 0.7719 - val_loss: 13.6856 - val_acc: 0.5556 - val_f1_score: 0.5529\n",
      "Epoch 50/300\n",
      "28/28 - 6s - loss: 13.1222 - acc: 0.7793 - f1_score: 0.7777 - val_loss: 13.6804 - val_acc: 0.5513 - val_f1_score: 0.5483\n",
      "Epoch 51/300\n",
      "28/28 - 6s - loss: 13.1293 - acc: 0.7783 - f1_score: 0.7764 - val_loss: 13.7123 - val_acc: 0.5427 - val_f1_score: 0.5375\n",
      "Epoch 52/300\n",
      "28/28 - 6s - loss: 13.1254 - acc: 0.7802 - f1_score: 0.7785 - val_loss: 13.7005 - val_acc: 0.5427 - val_f1_score: 0.5390\n",
      "Epoch 53/300\n",
      "28/28 - 6s - loss: 13.1056 - acc: 0.7895 - f1_score: 0.7879 - val_loss: 13.7160 - val_acc: 0.5427 - val_f1_score: 0.5375\n",
      "Epoch 54/300\n",
      "28/28 - 6s - loss: 13.1095 - acc: 0.7845 - f1_score: 0.7829 - val_loss: 13.7266 - val_acc: 0.5299 - val_f1_score: 0.5265\n",
      "Epoch 55/300\n",
      "28/28 - 6s - loss: 13.1114 - acc: 0.7898 - f1_score: 0.7883 - val_loss: 13.7304 - val_acc: 0.5342 - val_f1_score: 0.5296\n",
      "Epoch 56/300\n",
      "28/28 - 6s - loss: 13.0998 - acc: 0.7946 - f1_score: 0.7931 - val_loss: 13.7112 - val_acc: 0.5598 - val_f1_score: 0.5580\n",
      "Epoch 57/300\n",
      "28/28 - 7s - loss: 13.0804 - acc: 0.8031 - f1_score: 0.8017 - val_loss: 13.7054 - val_acc: 0.5470 - val_f1_score: 0.5470\n",
      "Epoch 58/300\n",
      "28/28 - 6s - loss: 13.0818 - acc: 0.8017 - f1_score: 0.8002 - val_loss: 13.6745 - val_acc: 0.5556 - val_f1_score: 0.5550\n",
      "Epoch 59/300\n",
      "28/28 - 6s - loss: 13.0726 - acc: 0.8096 - f1_score: 0.8081 - val_loss: 13.7265 - val_acc: 0.5342 - val_f1_score: 0.5327\n",
      "Epoch 60/300\n",
      "28/28 - 6s - loss: 13.0598 - acc: 0.8205 - f1_score: 0.8194 - val_loss: 13.7265 - val_acc: 0.5342 - val_f1_score: 0.5342\n",
      "Epoch 61/300\n",
      "28/28 - 6s - loss: 13.0665 - acc: 0.8103 - f1_score: 0.8088 - val_loss: 13.7049 - val_acc: 0.5427 - val_f1_score: 0.5397\n",
      "Epoch 62/300\n",
      "28/28 - 6s - loss: 13.0515 - acc: 0.8265 - f1_score: 0.8252 - val_loss: 13.7217 - val_acc: 0.5342 - val_f1_score: 0.5311\n",
      "Epoch 63/300\n",
      "28/28 - 6s - loss: 13.0535 - acc: 0.8254 - f1_score: 0.8242 - val_loss: 13.7516 - val_acc: 0.5214 - val_f1_score: 0.5163\n",
      "Epoch 64/300\n",
      "28/28 - 6s - loss: 13.0354 - acc: 0.8319 - f1_score: 0.8306 - val_loss: 13.7389 - val_acc: 0.5171 - val_f1_score: 0.5145\n",
      "Epoch 65/300\n",
      "28/28 - 6s - loss: 13.0288 - acc: 0.8354 - f1_score: 0.8342 - val_loss: 13.8002 - val_acc: 0.5256 - val_f1_score: 0.5252\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00065: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.51      0.58       137\n",
      "           1       0.48      0.65      0.56        97\n",
      "\n",
      "    accuracy                           0.57       234\n",
      "   macro avg       0.58      0.58      0.57       234\n",
      "weighted avg       0.59      0.57      0.57       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1818\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 71s - loss: 14.3536 - acc: 0.4805 - f1_score: 0.4750 - val_loss: 13.3827 - val_acc: 0.3974 - val_f1_score: 0.3363\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 14.0033 - acc: 0.5175 - f1_score: 0.5156 - val_loss: 13.4029 - val_acc: 0.3761 - val_f1_score: 0.3147\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.9053 - acc: 0.5240 - f1_score: 0.5220 - val_loss: 13.3968 - val_acc: 0.4487 - val_f1_score: 0.4330\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.8210 - acc: 0.5366 - f1_score: 0.5346 - val_loss: 13.3911 - val_acc: 0.4786 - val_f1_score: 0.4759\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7594 - acc: 0.5457 - f1_score: 0.5439 - val_loss: 13.3767 - val_acc: 0.5150 - val_f1_score: 0.5100\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.7147 - acc: 0.5590 - f1_score: 0.5565 - val_loss: 13.3780 - val_acc: 0.5085 - val_f1_score: 0.4938\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6830 - acc: 0.5625 - f1_score: 0.5602 - val_loss: 13.3721 - val_acc: 0.5534 - val_f1_score: 0.5258\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6125 - acc: 0.5764 - f1_score: 0.5736 - val_loss: 13.3713 - val_acc: 0.5940 - val_f1_score: 0.5489\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.6248 - acc: 0.5805 - f1_score: 0.5782 - val_loss: 13.3932 - val_acc: 0.5876 - val_f1_score: 0.5383\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5870 - acc: 0.5832 - f1_score: 0.5810 - val_loss: 13.4026 - val_acc: 0.5855 - val_f1_score: 0.5531\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5680 - acc: 0.5861 - f1_score: 0.5841 - val_loss: 13.4487 - val_acc: 0.5534 - val_f1_score: 0.5116\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5264 - acc: 0.6006 - f1_score: 0.5984 - val_loss: 13.5006 - val_acc: 0.5705 - val_f1_score: 0.5206\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5293 - acc: 0.6006 - f1_score: 0.5981 - val_loss: 13.5213 - val_acc: 0.5449 - val_f1_score: 0.5244\n",
      "Epoch 14/300\n",
      "27/27 - 6s - loss: 13.4836 - acc: 0.6105 - f1_score: 0.6083 - val_loss: 13.5218 - val_acc: 0.5748 - val_f1_score: 0.5513\n",
      "Epoch 15/300\n",
      "27/27 - 6s - loss: 13.4718 - acc: 0.6119 - f1_score: 0.6098 - val_loss: 13.5635 - val_acc: 0.5705 - val_f1_score: 0.5352\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4501 - acc: 0.6242 - f1_score: 0.6222 - val_loss: 13.6308 - val_acc: 0.5385 - val_f1_score: 0.5104\n",
      "Epoch 17/300\n",
      "27/27 - 6s - loss: 13.4237 - acc: 0.6255 - f1_score: 0.6235 - val_loss: 13.6524 - val_acc: 0.5748 - val_f1_score: 0.5475\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4124 - acc: 0.6332 - f1_score: 0.6309 - val_loss: 13.6623 - val_acc: 0.5449 - val_f1_score: 0.5207\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.4274 - acc: 0.6287 - f1_score: 0.6269 - val_loss: 13.6942 - val_acc: 0.5577 - val_f1_score: 0.5333\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.3927 - acc: 0.6396 - f1_score: 0.6373 - val_loss: 13.7590 - val_acc: 0.5235 - val_f1_score: 0.5064\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.3526 - acc: 0.6523 - f1_score: 0.6507 - val_loss: 13.7682 - val_acc: 0.5278 - val_f1_score: 0.5084\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3526 - acc: 0.6525 - f1_score: 0.6504 - val_loss: 13.8033 - val_acc: 0.5192 - val_f1_score: 0.5120\n",
      "Epoch 23/300\n",
      "27/27 - 6s - loss: 13.3444 - acc: 0.6588 - f1_score: 0.6568 - val_loss: 13.7924 - val_acc: 0.5192 - val_f1_score: 0.5085\n",
      "Epoch 24/300\n",
      "27/27 - 6s - loss: 13.3310 - acc: 0.6694 - f1_score: 0.6680 - val_loss: 13.7476 - val_acc: 0.5363 - val_f1_score: 0.5108\n",
      "Epoch 25/300\n",
      "27/27 - 6s - loss: 13.3438 - acc: 0.6642 - f1_score: 0.6618 - val_loss: 13.7836 - val_acc: 0.5278 - val_f1_score: 0.5167\n",
      "Epoch 26/300\n",
      "27/27 - 6s - loss: 13.2972 - acc: 0.6880 - f1_score: 0.6865 - val_loss: 13.7502 - val_acc: 0.5363 - val_f1_score: 0.5205\n",
      "Epoch 27/300\n",
      "27/27 - 6s - loss: 13.3191 - acc: 0.6770 - f1_score: 0.6749 - val_loss: 13.7836 - val_acc: 0.5278 - val_f1_score: 0.5173\n",
      "Epoch 28/300\n",
      "27/27 - 6s - loss: 13.2856 - acc: 0.6878 - f1_score: 0.6859 - val_loss: 13.7533 - val_acc: 0.5278 - val_f1_score: 0.5153\n",
      "Epoch 29/300\n",
      "27/27 - 6s - loss: 13.2845 - acc: 0.6890 - f1_score: 0.6873 - val_loss: 13.7598 - val_acc: 0.5064 - val_f1_score: 0.4961\n",
      "Epoch 30/300\n",
      "27/27 - 6s - loss: 13.2670 - acc: 0.6982 - f1_score: 0.6965 - val_loss: 13.7855 - val_acc: 0.4979 - val_f1_score: 0.4908\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000151B97620D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.43      0.43       173\n",
      "           1       0.67      0.68      0.67       295\n",
      "\n",
      "    accuracy                           0.59       468\n",
      "   macro avg       0.55      0.55      0.55       468\n",
      "weighted avg       0.58      0.59      0.58       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1892\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 68s - loss: 14.5123 - acc: 0.4741 - f1_score: 0.4732 - val_loss: 13.2978 - val_acc: 0.9167 - val_f1_score: 0.4783\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 14.0303 - acc: 0.5240 - f1_score: 0.5135 - val_loss: 13.2646 - val_acc: 0.9124 - val_f1_score: 0.5408\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.9221 - acc: 0.5296 - f1_score: 0.5187 - val_loss: 13.2671 - val_acc: 0.8205 - val_f1_score: 0.4937\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.8332 - acc: 0.5434 - f1_score: 0.5312 - val_loss: 13.2464 - val_acc: 0.8205 - val_f1_score: 0.5125\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.8062 - acc: 0.5474 - f1_score: 0.5372 - val_loss: 13.2676 - val_acc: 0.7265 - val_f1_score: 0.4625\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.7403 - acc: 0.5653 - f1_score: 0.5548 - val_loss: 13.3135 - val_acc: 0.6432 - val_f1_score: 0.4420\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6963 - acc: 0.5667 - f1_score: 0.5553 - val_loss: 13.3383 - val_acc: 0.6154 - val_f1_score: 0.4318\n",
      "Epoch 8/300\n",
      "27/27 - 6s - loss: 13.6479 - acc: 0.5702 - f1_score: 0.5602 - val_loss: 13.4482 - val_acc: 0.4957 - val_f1_score: 0.3686\n",
      "Epoch 9/300\n",
      "27/27 - 6s - loss: 13.6265 - acc: 0.5855 - f1_score: 0.5755 - val_loss: 13.5551 - val_acc: 0.4060 - val_f1_score: 0.3157\n",
      "Epoch 10/300\n",
      "27/27 - 6s - loss: 13.5967 - acc: 0.5880 - f1_score: 0.5764 - val_loss: 13.5798 - val_acc: 0.4509 - val_f1_score: 0.3438\n",
      "Epoch 11/300\n",
      "27/27 - 6s - loss: 13.5575 - acc: 0.5964 - f1_score: 0.5861 - val_loss: 13.7072 - val_acc: 0.3846 - val_f1_score: 0.3008\n",
      "Epoch 12/300\n",
      "27/27 - 6s - loss: 13.5365 - acc: 0.6032 - f1_score: 0.5931 - val_loss: 13.8238 - val_acc: 0.3632 - val_f1_score: 0.2882\n",
      "Epoch 13/300\n",
      "27/27 - 6s - loss: 13.5211 - acc: 0.6054 - f1_score: 0.5946 - val_loss: 13.7552 - val_acc: 0.4252 - val_f1_score: 0.3215\n",
      "Epoch 14/300\n",
      "27/27 - 6s - loss: 13.4848 - acc: 0.6186 - f1_score: 0.6074 - val_loss: 13.7938 - val_acc: 0.3974 - val_f1_score: 0.3033\n",
      "Epoch 15/300\n",
      "27/27 - 6s - loss: 13.4893 - acc: 0.6148 - f1_score: 0.6052 - val_loss: 14.0092 - val_acc: 0.3226 - val_f1_score: 0.2631\n",
      "Epoch 16/300\n",
      "27/27 - 6s - loss: 13.4593 - acc: 0.6207 - f1_score: 0.6101 - val_loss: 13.9462 - val_acc: 0.3803 - val_f1_score: 0.3006\n",
      "Epoch 17/300\n",
      "27/27 - 6s - loss: 13.4223 - acc: 0.6313 - f1_score: 0.6215 - val_loss: 13.9533 - val_acc: 0.3739 - val_f1_score: 0.2968\n",
      "Epoch 18/300\n",
      "27/27 - 6s - loss: 13.4113 - acc: 0.6394 - f1_score: 0.6285 - val_loss: 13.8932 - val_acc: 0.4124 - val_f1_score: 0.3143\n",
      "Epoch 19/300\n",
      "27/27 - 6s - loss: 13.4032 - acc: 0.6412 - f1_score: 0.6308 - val_loss: 13.9887 - val_acc: 0.3782 - val_f1_score: 0.2994\n",
      "Epoch 20/300\n",
      "27/27 - 6s - loss: 13.3944 - acc: 0.6452 - f1_score: 0.6343 - val_loss: 13.8681 - val_acc: 0.4444 - val_f1_score: 0.3348\n",
      "Epoch 21/300\n",
      "27/27 - 6s - loss: 13.3446 - acc: 0.6584 - f1_score: 0.6488 - val_loss: 14.0016 - val_acc: 0.3996 - val_f1_score: 0.3120\n",
      "Epoch 22/300\n",
      "27/27 - 6s - loss: 13.3411 - acc: 0.6687 - f1_score: 0.6587 - val_loss: 13.7900 - val_acc: 0.4573 - val_f1_score: 0.3418\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       434\n",
      "           1       0.23      0.09      0.13        34\n",
      "\n",
      "    accuracy                           0.91       468\n",
      "   macro avg       0.58      0.53      0.54       468\n",
      "weighted avg       0.88      0.91      0.89       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-152134\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1929\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Attention_layer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/300\n",
      "27/27 - 90s - loss: 14.3246 - acc: 0.4879 - f1_score: 0.4804 - val_loss: 13.4315 - val_acc: 0.2158 - val_f1_score: 0.1791\n",
      "Epoch 2/300\n",
      "27/27 - 6s - loss: 14.0084 - acc: 0.5172 - f1_score: 0.5163 - val_loss: 13.4886 - val_acc: 0.2137 - val_f1_score: 0.1761\n",
      "Epoch 3/300\n",
      "27/27 - 6s - loss: 13.9018 - acc: 0.5280 - f1_score: 0.5273 - val_loss: 13.4812 - val_acc: 0.2286 - val_f1_score: 0.2023\n",
      "Epoch 4/300\n",
      "27/27 - 6s - loss: 13.8212 - acc: 0.5372 - f1_score: 0.5362 - val_loss: 13.4917 - val_acc: 0.2692 - val_f1_score: 0.2553\n",
      "Epoch 5/300\n",
      "27/27 - 6s - loss: 13.7820 - acc: 0.5322 - f1_score: 0.5309 - val_loss: 13.4809 - val_acc: 0.3355 - val_f1_score: 0.3353\n",
      "Epoch 6/300\n",
      "27/27 - 6s - loss: 13.7152 - acc: 0.5437 - f1_score: 0.5424 - val_loss: 13.4642 - val_acc: 0.4038 - val_f1_score: 0.3782\n",
      "Epoch 7/300\n",
      "27/27 - 6s - loss: 13.6722 - acc: 0.5535 - f1_score: 0.5526 - val_loss: 13.4541 - val_acc: 0.4252 - val_f1_score: 0.3811\n",
      "Epoch 8/300\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_resnet(input_shape=[(2560, 1), (2560, 3)], attx_type='II', attx_st='two_three', classes = num_classes)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['II']\n",
    "attx_st = ['two_three']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in sub_dict_ecg.keys():\n",
    "\n",
    "            if i in ['1765']:\n",
    "                continue\n",
    "\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=20, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_resnet(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=300, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))    \n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2560, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2560, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_a (Conv1D)  (None, 366, 32)      64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_a (Conv1D)  (None, 366, 32)      128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_a (BatchNorm (None, 366, 32)      128         conv_res_stage1_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_a (BatchNorm (None, 366, 32)      128         conv_res_stage1_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 366, 32)      0           conv_bn_stage1_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 366, 32)      0           conv_bn_stage1_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_b (Conv1D)  (None, 366, 32)      65568       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_b (Conv1D)  (None, 366, 32)      65568       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_b (BatchNorm (None, 366, 32)      128         conv_res_stage1_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_b (BatchNorm (None, 366, 32)      128         conv_res_stage1_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 366, 32)      0           conv_bn_stage1_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 366, 32)      0           conv_bn_stage1_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_c (Conv1D)  (None, 366, 64)      2112        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_ecg_short (Conv (None, 366, 64)      128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_c (Conv1D)  (None, 366, 64)      2112        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage1_eda_short (Conv (None, 366, 64)      256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_c (BatchNorm (None, 366, 64)      256         conv_res_stage1_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_ecg_short (Batch (None, 366, 64)      256         conv_res_stage1_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_c (BatchNorm (None, 366, 64)      256         conv_res_stage1_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage1_eda_short (Batch (None, 366, 64)      256         conv_res_stage1_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 366, 64)      0           conv_bn_stage1_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage1_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 366, 64)      0           conv_bn_stage1_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage1_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 366, 64)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 366, 64)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_a (Conv1D)  (None, 366, 32)      2080        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_a (Conv1D)  (None, 366, 32)      2080        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_a (BatchNorm (None, 366, 32)      128         iden_res_stage1_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_a (BatchNorm (None, 366, 32)      128         iden_res_stage1_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 366, 32)      0           iden_bn_stage1_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 366, 32)      0           iden_bn_stage1_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_b (Conv1D)  (None, 366, 32)      65568       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_b (Conv1D)  (None, 366, 32)      65568       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_b (BatchNorm (None, 366, 32)      128         iden_res_stage1_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_b (BatchNorm (None, 366, 32)      128         iden_res_stage1_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 366, 32)      0           iden_bn_stage1_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 366, 32)      0           iden_bn_stage1_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_ecg_c (Conv1D)  (None, 366, 64)      2112        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage1_eda_c (Conv1D)  (None, 366, 64)      2112        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_ecg_c (BatchNorm (None, 366, 64)      256         iden_res_stage1_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage1_eda_c (BatchNorm (None, 366, 64)      256         iden_res_stage1_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 366, 64)      0           iden_bn_stage1_ecg_c[0][0]       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 366, 64)      0           iden_bn_stage1_eda_c[0][0]       \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 366, 64)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 366, 64)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_a (Conv1D)  (None, 122, 64)      4160        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_a (Conv1D)  (None, 122, 64)      4160        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_a (BatchNorm (None, 122, 64)      256         conv_res_stage2_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_a (BatchNorm (None, 122, 64)      256         conv_res_stage2_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 122, 64)      0           conv_bn_stage2_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 122, 64)      0           conv_bn_stage2_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_b (Conv1D)  (None, 122, 64)      131136      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_b (Conv1D)  (None, 122, 64)      131136      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_b (BatchNorm (None, 122, 64)      256         conv_res_stage2_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_b (BatchNorm (None, 122, 64)      256         conv_res_stage2_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 122, 64)      0           conv_bn_stage2_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 122, 64)      0           conv_bn_stage2_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_c (Conv1D)  (None, 122, 128)     8320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_ecg_short (Conv (None, 122, 128)     8320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_c (Conv1D)  (None, 122, 128)     8320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage2_eda_short (Conv (None, 122, 128)     8320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_c (BatchNorm (None, 122, 128)     512         conv_res_stage2_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_ecg_short (Batch (None, 122, 128)     512         conv_res_stage2_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_c (BatchNorm (None, 122, 128)     512         conv_res_stage2_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage2_eda_short (Batch (None, 122, 128)     512         conv_res_stage2_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 122, 128)     0           conv_bn_stage2_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage2_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 122, 128)     0           conv_bn_stage2_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage2_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 122, 128)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 122, 128)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_a (Conv1D)  (None, 122, 64)      8256        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_a (Conv1D)  (None, 122, 64)      8256        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_a (BatchNorm (None, 122, 64)      256         iden_res_stage2_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_a (BatchNorm (None, 122, 64)      256         iden_res_stage2_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 122, 64)      0           iden_bn_stage2_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 122, 64)      0           iden_bn_stage2_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_b (Conv1D)  (None, 122, 64)      131136      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_b (Conv1D)  (None, 122, 64)      131136      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_b (BatchNorm (None, 122, 64)      256         iden_res_stage2_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_b (BatchNorm (None, 122, 64)      256         iden_res_stage2_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 122, 64)      0           iden_bn_stage2_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 122, 64)      0           iden_bn_stage2_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_ecg_c (Conv1D)  (None, 122, 128)     8320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage2_eda_c (Conv1D)  (None, 122, 128)     8320        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_ecg_c (BatchNorm (None, 122, 128)     512         iden_res_stage2_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage2_eda_c (BatchNorm (None, 122, 128)     512         iden_res_stage2_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 122, 128)     0           iden_bn_stage2_ecg_c[0][0]       \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 122, 128)     0           iden_bn_stage2_eda_c[0][0]       \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 122, 128)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 122, 128)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_a (Conv1D)  (None, 41, 128)      16512       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_a (Conv1D)  (None, 41, 128)      16512       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_a (BatchNorm (None, 41, 128)      512         conv_res_stage3_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_a (BatchNorm (None, 41, 128)      512         conv_res_stage3_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 41, 128)      0           conv_bn_stage3_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 41, 128)      0           conv_bn_stage3_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_b (Conv1D)  (None, 41, 128)      278656      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_b (Conv1D)  (None, 41, 128)      278656      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_b (BatchNorm (None, 41, 128)      512         conv_res_stage3_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_b (BatchNorm (None, 41, 128)      512         conv_res_stage3_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 41, 128)      0           conv_bn_stage3_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 41, 128)      0           conv_bn_stage3_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_c (Conv1D)  (None, 41, 256)      33024       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_ecg_short (Conv (None, 41, 256)      33024       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_c (Conv1D)  (None, 41, 256)      33024       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage3_eda_short (Conv (None, 41, 256)      33024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_c (BatchNorm (None, 41, 256)      1024        conv_res_stage3_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_ecg_short (Batch (None, 41, 256)      1024        conv_res_stage3_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_c (BatchNorm (None, 41, 256)      1024        conv_res_stage3_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage3_eda_short (Batch (None, 41, 256)      1024        conv_res_stage3_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 41, 256)      0           conv_bn_stage3_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage3_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 41, 256)      0           conv_bn_stage3_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage3_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 41, 256)      0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 41, 256)      0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_a (Conv1D)  (None, 41, 128)      32896       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_a (Conv1D)  (None, 41, 128)      32896       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_a (BatchNorm (None, 41, 128)      512         iden_res_stage3_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_a (BatchNorm (None, 41, 128)      512         iden_res_stage3_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 41, 128)      0           iden_bn_stage3_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 41, 128)      0           iden_bn_stage3_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_b (Conv1D)  (None, 41, 128)      278656      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_b (Conv1D)  (None, 41, 128)      278656      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_b (BatchNorm (None, 41, 128)      512         iden_res_stage3_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_b (BatchNorm (None, 41, 128)      512         iden_res_stage3_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 41, 128)      0           iden_bn_stage3_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 41, 128)      0           iden_bn_stage3_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_ecg_c (Conv1D)  (None, 41, 256)      33024       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage3_eda_c (Conv1D)  (None, 41, 256)      33024       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_ecg_c (BatchNorm (None, 41, 256)      1024        iden_res_stage3_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage3_eda_c (BatchNorm (None, 41, 256)      1024        iden_res_stage3_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 41, 256)      0           iden_bn_stage3_ecg_c[0][0]       \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 41, 256)      0           iden_bn_stage3_eda_c[0][0]       \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 41, 256)      0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 41, 256)      0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_a (Conv1D)  (None, 14, 256)      65792       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_a (Conv1D)  (None, 14, 256)      65792       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_a (BatchNorm (None, 14, 256)      1024        conv_res_stage4_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_a (BatchNorm (None, 14, 256)      1024        conv_res_stage4_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 256)      0           conv_bn_stage4_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 256)      0           conv_bn_stage4_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_b (Conv1D)  (None, 14, 256)      459008      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_b (Conv1D)  (None, 14, 256)      459008      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_b (BatchNorm (None, 14, 256)      1024        conv_res_stage4_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_b (BatchNorm (None, 14, 256)      1024        conv_res_stage4_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 256)      0           conv_bn_stage4_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 256)      0           conv_bn_stage4_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_c (Conv1D)  (None, 14, 512)      131584      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_ecg_short (Conv (None, 14, 512)      131584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_c (Conv1D)  (None, 14, 512)      131584      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_res_stage4_eda_short (Conv (None, 14, 512)      131584      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_c (BatchNorm (None, 14, 512)      2048        conv_res_stage4_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_ecg_short (Batch (None, 14, 512)      2048        conv_res_stage4_ecg_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_c (BatchNorm (None, 14, 512)      2048        conv_res_stage4_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_stage4_eda_short (Batch (None, 14, 512)      2048        conv_res_stage4_eda_short[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 512)      0           conv_bn_stage4_ecg_c[0][0]       \n",
      "                                                                 conv_bn_stage4_ecg_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 14, 512)      0           conv_bn_stage4_eda_c[0][0]       \n",
      "                                                                 conv_bn_stage4_eda_short[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 512)      0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 512)      0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_a (Conv1D)  (None, 14, 256)      131328      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_a (Conv1D)  (None, 14, 256)      131328      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_a (BatchNorm (None, 14, 256)      1024        iden_res_stage4_ecg_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_a (BatchNorm (None, 14, 256)      1024        iden_res_stage4_eda_a[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 256)      0           iden_bn_stage4_ecg_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 256)      0           iden_bn_stage4_eda_a[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_b (Conv1D)  (None, 14, 256)      459008      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_b (Conv1D)  (None, 14, 256)      459008      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_b (BatchNorm (None, 14, 256)      1024        iden_res_stage4_ecg_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_b (BatchNorm (None, 14, 256)      1024        iden_res_stage4_eda_b[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 256)      0           iden_bn_stage4_ecg_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 256)      0           iden_bn_stage4_eda_b[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_ecg_c (Conv1D)  (None, 14, 512)      131584      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_res_stage4_eda_c (Conv1D)  (None, 14, 512)      131584      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_ecg_c (BatchNorm (None, 14, 512)      2048        iden_res_stage4_ecg_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "iden_bn_stage4_eda_c (BatchNorm (None, 14, 512)      2048        iden_res_stage4_eda_c[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 512)      0           iden_bn_stage4_ecg_c[0][0]       \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 14, 512)      0           iden_bn_stage4_eda_c[0][0]       \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 512)      0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 512)      0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 7168)         0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7168)         0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         7341056     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         7341056     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecg_bf_merge (Dense)            (None, 256)          262400      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "eda_bf_merge (Dense)            (None, 256)          262400      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           ecg_bf_merge[0][0]               \n",
      "                                                                 eda_bf_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            1026        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 20,552,450\n",
      "Trainable params: 20,533,250\n",
      "Non-trainable params: 19,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------\n",
      "Training for Type FF, Stage FF\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-204902\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_weights\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1105\n",
      "Epoch 1/100\n",
      "27/27 - 96s - loss: 14.2743 - acc: 0.4878 - f1_score: 0.4828 - val_loss: 13.0201 - val_acc: 0.4081 - val_f1_score: 0.2898\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.5985 - acc: 0.5198 - f1_score: 0.5181 - val_loss: 13.0347 - val_acc: 0.4145 - val_f1_score: 0.3047\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.5131 - acc: 0.5388 - f1_score: 0.5353 - val_loss: 13.0559 - val_acc: 0.4252 - val_f1_score: 0.3215\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4498 - acc: 0.5438 - f1_score: 0.5415 - val_loss: 13.0490 - val_acc: 0.3996 - val_f1_score: 0.3396\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.3801 - acc: 0.5482 - f1_score: 0.5462 - val_loss: 13.0330 - val_acc: 0.4380 - val_f1_score: 0.4290\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3370 - acc: 0.5582 - f1_score: 0.5552 - val_loss: 13.0401 - val_acc: 0.4722 - val_f1_score: 0.4712\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.2983 - acc: 0.5571 - f1_score: 0.5548 - val_loss: 13.0235 - val_acc: 0.5214 - val_f1_score: 0.5077\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2391 - acc: 0.5755 - f1_score: 0.5733 - val_loss: 13.0305 - val_acc: 0.5620 - val_f1_score: 0.5431\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.2002 - acc: 0.5884 - f1_score: 0.5856 - val_loss: 13.0319 - val_acc: 0.5577 - val_f1_score: 0.5395\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.1729 - acc: 0.5945 - f1_score: 0.5922 - val_loss: 13.0596 - val_acc: 0.5513 - val_f1_score: 0.5260\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1558 - acc: 0.6006 - f1_score: 0.5980 - val_loss: 13.0829 - val_acc: 0.5427 - val_f1_score: 0.5199\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1314 - acc: 0.6065 - f1_score: 0.6040 - val_loss: 13.0898 - val_acc: 0.5449 - val_f1_score: 0.5207\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.1050 - acc: 0.6125 - f1_score: 0.6097 - val_loss: 13.1043 - val_acc: 0.5342 - val_f1_score: 0.5080\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.47      0.45       178\n",
      "           1       0.66      0.62      0.64       290\n",
      "\n",
      "    accuracy                           0.56       468\n",
      "   macro avg       0.54      0.54      0.54       468\n",
      "weighted avg       0.57      0.56      0.57       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1105\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1106\n",
      "Epoch 1/100\n",
      "27/27 - 52s - loss: 14.2692 - acc: 0.4954 - f1_score: 0.4862 - val_loss: 13.1388 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.5539 - acc: 0.5250 - f1_score: 0.5250 - val_loss: 13.2171 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.5031 - acc: 0.5335 - f1_score: 0.5331 - val_loss: 13.3508 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4224 - acc: 0.5445 - f1_score: 0.5444 - val_loss: 13.3916 - val_acc: 0.0876 - val_f1_score: 0.0819\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.3610 - acc: 0.5582 - f1_score: 0.5580 - val_loss: 13.3748 - val_acc: 0.0962 - val_f1_score: 0.0931\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3359 - acc: 0.5550 - f1_score: 0.5546 - val_loss: 13.4263 - val_acc: 0.1197 - val_f1_score: 0.1186\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.2793 - acc: 0.5708 - f1_score: 0.5705 - val_loss: 13.3598 - val_acc: 0.1966 - val_f1_score: 0.1928\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2132 - acc: 0.5854 - f1_score: 0.5851 - val_loss: 13.4154 - val_acc: 0.2158 - val_f1_score: 0.2085\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.1761 - acc: 0.5935 - f1_score: 0.5931 - val_loss: 13.4565 - val_acc: 0.2051 - val_f1_score: 0.1980\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.1573 - acc: 0.6032 - f1_score: 0.6028 - val_loss: 13.4584 - val_acc: 0.2436 - val_f1_score: 0.2273\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1344 - acc: 0.6084 - f1_score: 0.6081 - val_loss: 13.5163 - val_acc: 0.2671 - val_f1_score: 0.2467\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1153 - acc: 0.6181 - f1_score: 0.6179 - val_loss: 13.5498 - val_acc: 0.3141 - val_f1_score: 0.2859\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.0708 - acc: 0.6228 - f1_score: 0.6222 - val_loss: 13.6891 - val_acc: 0.2927 - val_f1_score: 0.2720\n",
      "Epoch 14/100\n",
      "27/27 - 6s - loss: 13.0672 - acc: 0.6265 - f1_score: 0.6263 - val_loss: 13.7777 - val_acc: 0.2991 - val_f1_score: 0.2791\n",
      "Epoch 15/100\n",
      "27/27 - 6s - loss: 13.0355 - acc: 0.6399 - f1_score: 0.6397 - val_loss: 13.8256 - val_acc: 0.2885 - val_f1_score: 0.2717\n",
      "Epoch 16/100\n",
      "27/27 - 6s - loss: 13.0170 - acc: 0.6380 - f1_score: 0.6376 - val_loss: 13.8630 - val_acc: 0.2885 - val_f1_score: 0.2707\n",
      "Epoch 17/100\n",
      "27/27 - 6s - loss: 12.9936 - acc: 0.6506 - f1_score: 0.6503 - val_loss: 13.8152 - val_acc: 0.3141 - val_f1_score: 0.2907\n",
      "Epoch 18/100\n",
      "27/27 - 6s - loss: 12.9753 - acc: 0.6544 - f1_score: 0.6542 - val_loss: 13.8283 - val_acc: 0.3355 - val_f1_score: 0.3081\n",
      "Epoch 19/100\n",
      "27/27 - 6s - loss: 12.9726 - acc: 0.6633 - f1_score: 0.6631 - val_loss: 13.8488 - val_acc: 0.3697 - val_f1_score: 0.3292\n",
      "Epoch 20/100\n",
      "27/27 - 6s - loss: 12.9380 - acc: 0.6793 - f1_score: 0.6789 - val_loss: 14.0026 - val_acc: 0.3355 - val_f1_score: 0.3069\n",
      "Epoch 21/100\n",
      "27/27 - 6s - loss: 12.9377 - acc: 0.6774 - f1_score: 0.6772 - val_loss: 14.0218 - val_acc: 0.3333 - val_f1_score: 0.3040\n",
      "Epoch 22/100\n",
      "27/27 - 6s - loss: 12.8981 - acc: 0.6845 - f1_score: 0.6843 - val_loss: 14.0035 - val_acc: 0.3419 - val_f1_score: 0.3091\n",
      "Epoch 23/100\n",
      "27/27 - 6s - loss: 12.8964 - acc: 0.6943 - f1_score: 0.6942 - val_loss: 13.8680 - val_acc: 0.3825 - val_f1_score: 0.3382\n",
      "Epoch 24/100\n",
      "27/27 - 6s - loss: 12.8739 - acc: 0.7114 - f1_score: 0.7112 - val_loss: 13.9036 - val_acc: 0.3718 - val_f1_score: 0.3291\n",
      "Epoch 25/100\n",
      "27/27 - 6s - loss: 12.8731 - acc: 0.7042 - f1_score: 0.7040 - val_loss: 13.8958 - val_acc: 0.3825 - val_f1_score: 0.3367\n",
      "Epoch 26/100\n",
      "27/27 - 6s - loss: 12.8416 - acc: 0.7239 - f1_score: 0.7237 - val_loss: 13.9536 - val_acc: 0.3654 - val_f1_score: 0.3261\n",
      "Epoch 27/100\n",
      "27/27 - 6s - loss: 12.8342 - acc: 0.7284 - f1_score: 0.7282 - val_loss: 14.0121 - val_acc: 0.3697 - val_f1_score: 0.3292\n",
      "Epoch 28/100\n",
      "27/27 - 6s - loss: 12.8182 - acc: 0.7374 - f1_score: 0.7373 - val_loss: 13.9643 - val_acc: 0.3803 - val_f1_score: 0.3352\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.09      0.17       308\n",
      "           1       0.35      0.94      0.51       160\n",
      "\n",
      "    accuracy                           0.38       468\n",
      "   macro avg       0.55      0.52      0.34       468\n",
      "weighted avg       0.61      0.38      0.28       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1106\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1175\n",
      "Epoch 1/100\n",
      "28/28 - 51s - loss: 14.4218 - acc: 0.4715 - f1_score: 0.4679 - val_loss: 12.9283 - val_acc: 0.7215 - val_f1_score: 0.4191\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6050 - acc: 0.5307 - f1_score: 0.5274 - val_loss: 12.9283 - val_acc: 0.6976 - val_f1_score: 0.4351\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.5006 - acc: 0.5435 - f1_score: 0.5389 - val_loss: 12.9497 - val_acc: 0.6021 - val_f1_score: 0.4484\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4699 - acc: 0.5440 - f1_score: 0.5383 - val_loss: 12.9598 - val_acc: 0.5782 - val_f1_score: 0.4464\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3836 - acc: 0.5529 - f1_score: 0.5473 - val_loss: 12.9935 - val_acc: 0.5438 - val_f1_score: 0.4789\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3426 - acc: 0.5609 - f1_score: 0.5545 - val_loss: 13.0277 - val_acc: 0.4960 - val_f1_score: 0.4554\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2932 - acc: 0.5786 - f1_score: 0.5727 - val_loss: 13.0843 - val_acc: 0.4615 - val_f1_score: 0.4208\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2487 - acc: 0.5772 - f1_score: 0.5723 - val_loss: 13.2676 - val_acc: 0.3899 - val_f1_score: 0.3896\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2150 - acc: 0.5914 - f1_score: 0.5849 - val_loss: 13.2589 - val_acc: 0.3740 - val_f1_score: 0.3717\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.2171 - acc: 0.5955 - f1_score: 0.5903 - val_loss: 13.3520 - val_acc: 0.3714 - val_f1_score: 0.3709\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66       237\n",
      "           1       0.35      0.26      0.30       140\n",
      "\n",
      "    accuracy                           0.54       377\n",
      "   macro avg       0.48      0.49      0.48       377\n",
      "weighted avg       0.52      0.54      0.53       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1175\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1194\n",
      "Epoch 1/100\n",
      "28/28 - 67s - loss: 14.3323 - acc: 0.4824 - f1_score: 0.4740 - val_loss: 13.0784 - val_acc: 0.2066 - val_f1_score: 0.1758\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6084 - acc: 0.5217 - f1_score: 0.5215 - val_loss: 13.0994 - val_acc: 0.2396 - val_f1_score: 0.2195\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.5237 - acc: 0.5256 - f1_score: 0.5248 - val_loss: 13.1320 - val_acc: 0.2462 - val_f1_score: 0.2289\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4775 - acc: 0.5346 - f1_score: 0.5340 - val_loss: 13.0769 - val_acc: 0.3604 - val_f1_score: 0.3596\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.4602 - acc: 0.5408 - f1_score: 0.5392 - val_loss: 13.1437 - val_acc: 0.3341 - val_f1_score: 0.3341\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.4182 - acc: 0.5361 - f1_score: 0.5353 - val_loss: 13.1149 - val_acc: 0.4198 - val_f1_score: 0.4105\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.4284 - acc: 0.5441 - f1_score: 0.5434 - val_loss: 13.0781 - val_acc: 0.4747 - val_f1_score: 0.4492\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.3639 - acc: 0.5434 - f1_score: 0.5423 - val_loss: 13.0689 - val_acc: 0.5187 - val_f1_score: 0.4527\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.3585 - acc: 0.5453 - f1_score: 0.5439 - val_loss: 13.1379 - val_acc: 0.4703 - val_f1_score: 0.4132\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.3162 - acc: 0.5678 - f1_score: 0.5670 - val_loss: 13.1647 - val_acc: 0.4835 - val_f1_score: 0.4260\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.3150 - acc: 0.5570 - f1_score: 0.5561 - val_loss: 13.1390 - val_acc: 0.5231 - val_f1_score: 0.4144\n",
      "Epoch 12/100\n",
      "28/28 - 6s - loss: 13.2705 - acc: 0.5704 - f1_score: 0.5693 - val_loss: 13.1474 - val_acc: 0.5670 - val_f1_score: 0.4470\n",
      "Epoch 13/100\n",
      "28/28 - 6s - loss: 13.2494 - acc: 0.5771 - f1_score: 0.5757 - val_loss: 13.2431 - val_acc: 0.5407 - val_f1_score: 0.4532\n",
      "Epoch 14/100\n",
      "28/28 - 6s - loss: 13.2458 - acc: 0.5632 - f1_score: 0.5626 - val_loss: 13.2215 - val_acc: 0.5626 - val_f1_score: 0.4499\n",
      "Epoch 15/100\n",
      "28/28 - 6s - loss: 13.1898 - acc: 0.5844 - f1_score: 0.5838 - val_loss: 13.2474 - val_acc: 0.5473 - val_f1_score: 0.4454\n",
      "Epoch 16/100\n",
      "28/28 - 6s - loss: 13.2035 - acc: 0.5808 - f1_score: 0.5798 - val_loss: 13.3251 - val_acc: 0.5363 - val_f1_score: 0.4568\n",
      "Epoch 17/100\n",
      "28/28 - 6s - loss: 13.1843 - acc: 0.5875 - f1_score: 0.5870 - val_loss: 13.3460 - val_acc: 0.5451 - val_f1_score: 0.4606\n",
      "Epoch 18/100\n",
      "28/28 - 6s - loss: 13.1688 - acc: 0.5943 - f1_score: 0.5934 - val_loss: 13.3667 - val_acc: 0.5165 - val_f1_score: 0.4473\n",
      "Epoch 19/100\n",
      "28/28 - 6s - loss: 13.1509 - acc: 0.5930 - f1_score: 0.5922 - val_loss: 13.3872 - val_acc: 0.5077 - val_f1_score: 0.4486\n",
      "Epoch 20/100\n",
      "28/28 - 6s - loss: 13.1459 - acc: 0.5947 - f1_score: 0.5940 - val_loss: 13.2933 - val_acc: 0.5297 - val_f1_score: 0.4458\n",
      "Epoch 21/100\n",
      "28/28 - 6s - loss: 13.1512 - acc: 0.5949 - f1_score: 0.5940 - val_loss: 13.2664 - val_acc: 0.5231 - val_f1_score: 0.4322\n",
      "Epoch 22/100\n",
      "28/28 - 6s - loss: 13.0997 - acc: 0.6148 - f1_score: 0.6132 - val_loss: 13.3506 - val_acc: 0.4901 - val_f1_score: 0.4270\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.18      0.25       184\n",
      "           1       0.59      0.79      0.67       271\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.48      0.49      0.46       455\n",
      "weighted avg       0.50      0.55      0.50       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1194\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1337\n",
      "Epoch 1/100\n",
      "27/27 - 56s - loss: 14.3215 - acc: 0.4912 - f1_score: 0.4890 - val_loss: 12.9279 - val_acc: 0.7137 - val_f1_score: 0.4165\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6198 - acc: 0.5192 - f1_score: 0.5147 - val_loss: 12.9294 - val_acc: 0.7030 - val_f1_score: 0.4512\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.4948 - acc: 0.5331 - f1_score: 0.5263 - val_loss: 12.9286 - val_acc: 0.6944 - val_f1_score: 0.5061\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4567 - acc: 0.5443 - f1_score: 0.5388 - val_loss: 12.9764 - val_acc: 0.5427 - val_f1_score: 0.5019\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.3484 - acc: 0.5684 - f1_score: 0.5631 - val_loss: 13.0889 - val_acc: 0.3675 - val_f1_score: 0.3555\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.2996 - acc: 0.5674 - f1_score: 0.5600 - val_loss: 13.1609 - val_acc: 0.3526 - val_f1_score: 0.3355\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.2945 - acc: 0.5815 - f1_score: 0.5753 - val_loss: 13.3738 - val_acc: 0.2885 - val_f1_score: 0.2410\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2321 - acc: 0.5877 - f1_score: 0.5811 - val_loss: 13.5646 - val_acc: 0.2949 - val_f1_score: 0.2487\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.73      0.81       419\n",
      "           1       0.14      0.37      0.20        49\n",
      "\n",
      "    accuracy                           0.69       468\n",
      "   macro avg       0.52      0.55      0.51       468\n",
      "weighted avg       0.83      0.69      0.75       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1337\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1390\n",
      "Epoch 1/100\n",
      "27/27 - 61s - loss: 14.2768 - acc: 0.4863 - f1_score: 0.4822 - val_loss: 13.0063 - val_acc: 0.4444 - val_f1_score: 0.3109\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6192 - acc: 0.5175 - f1_score: 0.5155 - val_loss: 13.0169 - val_acc: 0.4359 - val_f1_score: 0.3099\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.4976 - acc: 0.5318 - f1_score: 0.5285 - val_loss: 13.0262 - val_acc: 0.4423 - val_f1_score: 0.3280\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4549 - acc: 0.5367 - f1_score: 0.5343 - val_loss: 13.0096 - val_acc: 0.4615 - val_f1_score: 0.4069\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.3737 - acc: 0.5515 - f1_score: 0.5490 - val_loss: 12.9760 - val_acc: 0.5321 - val_f1_score: 0.5284\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3280 - acc: 0.5540 - f1_score: 0.5506 - val_loss: 12.9804 - val_acc: 0.5491 - val_f1_score: 0.5457\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.3056 - acc: 0.5644 - f1_score: 0.5618 - val_loss: 12.9799 - val_acc: 0.5513 - val_f1_score: 0.5508\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2429 - acc: 0.5745 - f1_score: 0.5718 - val_loss: 13.0017 - val_acc: 0.5363 - val_f1_score: 0.5331\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.2388 - acc: 0.5812 - f1_score: 0.5784 - val_loss: 13.0400 - val_acc: 0.5641 - val_f1_score: 0.5535\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.1809 - acc: 0.5910 - f1_score: 0.5880 - val_loss: 13.0688 - val_acc: 0.5556 - val_f1_score: 0.5465\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1469 - acc: 0.6067 - f1_score: 0.6041 - val_loss: 13.0881 - val_acc: 0.5897 - val_f1_score: 0.5747\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1405 - acc: 0.6038 - f1_score: 0.6016 - val_loss: 13.1283 - val_acc: 0.5705 - val_f1_score: 0.5386\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.1238 - acc: 0.6065 - f1_score: 0.6036 - val_loss: 13.1785 - val_acc: 0.5598 - val_f1_score: 0.5370\n",
      "Epoch 14/100\n",
      "27/27 - 6s - loss: 13.0940 - acc: 0.6126 - f1_score: 0.6100 - val_loss: 13.2126 - val_acc: 0.5577 - val_f1_score: 0.5352\n",
      "Epoch 15/100\n",
      "27/27 - 6s - loss: 13.0575 - acc: 0.6270 - f1_score: 0.6244 - val_loss: 13.2377 - val_acc: 0.5748 - val_f1_score: 0.5522\n",
      "Epoch 16/100\n",
      "27/27 - 6s - loss: 13.0503 - acc: 0.6216 - f1_score: 0.6192 - val_loss: 13.2486 - val_acc: 0.5812 - val_f1_score: 0.5612\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.55      0.49       172\n",
      "           1       0.70      0.61      0.65       296\n",
      "\n",
      "    accuracy                           0.59       468\n",
      "   macro avg       0.58      0.58      0.57       468\n",
      "weighted avg       0.61      0.59      0.60       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1390\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1400\n",
      "Epoch 1/100\n",
      "27/27 - 70s - loss: 14.2547 - acc: 0.4905 - f1_score: 0.4822 - val_loss: 13.1034 - val_acc: 0.1453 - val_f1_score: 0.1300\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6300 - acc: 0.5166 - f1_score: 0.5164 - val_loss: 13.1420 - val_acc: 0.1603 - val_f1_score: 0.1493\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.4927 - acc: 0.5302 - f1_score: 0.5295 - val_loss: 13.1900 - val_acc: 0.1667 - val_f1_score: 0.1585\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4556 - acc: 0.5416 - f1_score: 0.5410 - val_loss: 13.1438 - val_acc: 0.2372 - val_f1_score: 0.2368\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.4004 - acc: 0.5493 - f1_score: 0.5489 - val_loss: 13.0688 - val_acc: 0.3932 - val_f1_score: 0.3577\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3358 - acc: 0.5582 - f1_score: 0.5571 - val_loss: 13.1553 - val_acc: 0.3483 - val_f1_score: 0.3292\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.3153 - acc: 0.5640 - f1_score: 0.5636 - val_loss: 13.0883 - val_acc: 0.4786 - val_f1_score: 0.4096\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2315 - acc: 0.5793 - f1_score: 0.5788 - val_loss: 13.1111 - val_acc: 0.4893 - val_f1_score: 0.4106\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.2287 - acc: 0.5797 - f1_score: 0.5793 - val_loss: 13.1031 - val_acc: 0.5726 - val_f1_score: 0.4675\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.1723 - acc: 0.5989 - f1_score: 0.5983 - val_loss: 13.2222 - val_acc: 0.5235 - val_f1_score: 0.4397\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1425 - acc: 0.6016 - f1_score: 0.6010 - val_loss: 13.2750 - val_acc: 0.5214 - val_f1_score: 0.4404\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1407 - acc: 0.6107 - f1_score: 0.6104 - val_loss: 13.2525 - val_acc: 0.5897 - val_f1_score: 0.4650\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.1129 - acc: 0.6183 - f1_score: 0.6177 - val_loss: 13.3851 - val_acc: 0.5620 - val_f1_score: 0.4554\n",
      "Epoch 14/100\n",
      "27/27 - 6s - loss: 13.0729 - acc: 0.6192 - f1_score: 0.6186 - val_loss: 13.4011 - val_acc: 0.6026 - val_f1_score: 0.4845\n",
      "Epoch 15/100\n",
      "27/27 - 6s - loss: 13.0609 - acc: 0.6302 - f1_score: 0.6296 - val_loss: 13.4473 - val_acc: 0.6068 - val_f1_score: 0.4927\n",
      "Epoch 16/100\n",
      "27/27 - 6s - loss: 13.0396 - acc: 0.6305 - f1_score: 0.6300 - val_loss: 13.4287 - val_acc: 0.5962 - val_f1_score: 0.4829\n",
      "Epoch 17/100\n",
      "27/27 - 6s - loss: 13.0136 - acc: 0.6496 - f1_score: 0.6492 - val_loss: 13.4669 - val_acc: 0.6325 - val_f1_score: 0.5127\n",
      "Epoch 18/100\n",
      "27/27 - 6s - loss: 12.9796 - acc: 0.6564 - f1_score: 0.6560 - val_loss: 13.3942 - val_acc: 0.6624 - val_f1_score: 0.5306\n",
      "Epoch 19/100\n",
      "27/27 - 6s - loss: 12.9695 - acc: 0.6638 - f1_score: 0.6632 - val_loss: 13.5125 - val_acc: 0.6154 - val_f1_score: 0.5011\n",
      "Epoch 20/100\n",
      "27/27 - 6s - loss: 12.9627 - acc: 0.6688 - f1_score: 0.6685 - val_loss: 13.6225 - val_acc: 0.5983 - val_f1_score: 0.4921\n",
      "Epoch 21/100\n",
      "27/27 - 6s - loss: 12.9310 - acc: 0.6812 - f1_score: 0.6808 - val_loss: 13.6325 - val_acc: 0.6154 - val_f1_score: 0.4984\n",
      "Epoch 22/100\n",
      "27/27 - 6s - loss: 12.9089 - acc: 0.6874 - f1_score: 0.6869 - val_loss: 13.7548 - val_acc: 0.5577 - val_f1_score: 0.4693\n",
      "Epoch 23/100\n",
      "27/27 - 6s - loss: 12.9080 - acc: 0.6891 - f1_score: 0.6886 - val_loss: 13.7420 - val_acc: 0.5491 - val_f1_score: 0.4635\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.20      0.28       155\n",
      "           1       0.69      0.89      0.78       313\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.58      0.55      0.53       468\n",
      "weighted avg       0.62      0.66      0.61       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1400\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1419\n",
      "Epoch 1/100\n",
      "28/28 - 67s - loss: 14.3373 - acc: 0.4825 - f1_score: 0.4767 - val_loss: 13.0393 - val_acc: 0.3121 - val_f1_score: 0.2401\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6376 - acc: 0.5171 - f1_score: 0.5164 - val_loss: 13.0477 - val_acc: 0.3055 - val_f1_score: 0.2427\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.5360 - acc: 0.5260 - f1_score: 0.5241 - val_loss: 13.0503 - val_acc: 0.3407 - val_f1_score: 0.2989\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4799 - acc: 0.5333 - f1_score: 0.5310 - val_loss: 13.0513 - val_acc: 0.3868 - val_f1_score: 0.3728\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.4295 - acc: 0.5382 - f1_score: 0.5358 - val_loss: 13.0679 - val_acc: 0.4000 - val_f1_score: 0.3958\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.4122 - acc: 0.5393 - f1_score: 0.5378 - val_loss: 13.0470 - val_acc: 0.4703 - val_f1_score: 0.4666\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.3799 - acc: 0.5473 - f1_score: 0.5456 - val_loss: 13.0357 - val_acc: 0.4989 - val_f1_score: 0.4833\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.3606 - acc: 0.5411 - f1_score: 0.5392 - val_loss: 13.0331 - val_acc: 0.5451 - val_f1_score: 0.5123\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.3554 - acc: 0.5483 - f1_score: 0.5462 - val_loss: 13.0669 - val_acc: 0.5363 - val_f1_score: 0.5147\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.2961 - acc: 0.5609 - f1_score: 0.5587 - val_loss: 13.0613 - val_acc: 0.5758 - val_f1_score: 0.5463\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.3056 - acc: 0.5555 - f1_score: 0.5541 - val_loss: 13.0472 - val_acc: 0.6000 - val_f1_score: 0.5427\n",
      "Epoch 12/100\n",
      "28/28 - 6s - loss: 13.2785 - acc: 0.5648 - f1_score: 0.5632 - val_loss: 13.0561 - val_acc: 0.6066 - val_f1_score: 0.5235\n",
      "Epoch 13/100\n",
      "28/28 - 6s - loss: 13.2517 - acc: 0.5658 - f1_score: 0.5635 - val_loss: 13.1107 - val_acc: 0.5912 - val_f1_score: 0.5060\n",
      "Epoch 14/100\n",
      "28/28 - 6s - loss: 13.2351 - acc: 0.5716 - f1_score: 0.5697 - val_loss: 13.1506 - val_acc: 0.5648 - val_f1_score: 0.5126\n",
      "Epoch 15/100\n",
      "28/28 - 6s - loss: 13.1981 - acc: 0.5776 - f1_score: 0.5767 - val_loss: 13.1650 - val_acc: 0.5978 - val_f1_score: 0.5377\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.37      0.43       196\n",
      "           1       0.61      0.73      0.66       259\n",
      "\n",
      "    accuracy                           0.58       455\n",
      "   macro avg       0.56      0.55      0.55       455\n",
      "weighted avg       0.56      0.58      0.56       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1419\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1517\n",
      "Epoch 1/100\n",
      "28/28 - 59s - loss: 14.3023 - acc: 0.4802 - f1_score: 0.4783 - val_loss: 12.8952 - val_acc: 0.9060 - val_f1_score: 0.4753\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6202 - acc: 0.5309 - f1_score: 0.5250 - val_loss: 12.8773 - val_acc: 0.8632 - val_f1_score: 0.4832\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.4867 - acc: 0.5443 - f1_score: 0.5358 - val_loss: 12.8640 - val_acc: 0.8291 - val_f1_score: 0.4693\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4306 - acc: 0.5450 - f1_score: 0.5366 - val_loss: 12.8672 - val_acc: 0.7949 - val_f1_score: 0.5029\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.4000 - acc: 0.5416 - f1_score: 0.5350 - val_loss: 12.9591 - val_acc: 0.5954 - val_f1_score: 0.4398\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3284 - acc: 0.5702 - f1_score: 0.5619 - val_loss: 12.9579 - val_acc: 0.6125 - val_f1_score: 0.4396\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2873 - acc: 0.5712 - f1_score: 0.5638 - val_loss: 13.0127 - val_acc: 0.5185 - val_f1_score: 0.3882\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2307 - acc: 0.5886 - f1_score: 0.5812 - val_loss: 13.0565 - val_acc: 0.5043 - val_f1_score: 0.3953\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2138 - acc: 0.5886 - f1_score: 0.5807 - val_loss: 13.0792 - val_acc: 0.5356 - val_f1_score: 0.4138\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       295\n",
      "           1       0.19      0.09      0.12        56\n",
      "\n",
      "    accuracy                           0.79       351\n",
      "   macro avg       0.52      0.51      0.50       351\n",
      "weighted avg       0.74      0.79      0.76       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1517\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1544\n",
      "Epoch 1/100\n",
      "29/29 - 63s - loss: 14.3021 - acc: 0.4877 - f1_score: 0.4859 - val_loss: 13.0297 - val_acc: 0.3750 - val_f1_score: 0.2727\n",
      "Epoch 2/100\n",
      "29/29 - 6s - loss: 13.5860 - acc: 0.5197 - f1_score: 0.5184 - val_loss: 13.0274 - val_acc: 0.3942 - val_f1_score: 0.3169\n",
      "Epoch 3/100\n",
      "29/29 - 6s - loss: 13.4820 - acc: 0.5331 - f1_score: 0.5307 - val_loss: 13.0279 - val_acc: 0.3846 - val_f1_score: 0.3436\n",
      "Epoch 4/100\n",
      "29/29 - 6s - loss: 13.4170 - acc: 0.5489 - f1_score: 0.5463 - val_loss: 13.0158 - val_acc: 0.4519 - val_f1_score: 0.4507\n",
      "Epoch 5/100\n",
      "29/29 - 6s - loss: 13.3608 - acc: 0.5489 - f1_score: 0.5463 - val_loss: 13.0307 - val_acc: 0.4519 - val_f1_score: 0.4519\n",
      "Epoch 6/100\n",
      "29/29 - 6s - loss: 13.3186 - acc: 0.5546 - f1_score: 0.5524 - val_loss: 13.0073 - val_acc: 0.5288 - val_f1_score: 0.5267\n",
      "Epoch 7/100\n",
      "29/29 - 6s - loss: 13.2903 - acc: 0.5646 - f1_score: 0.5611 - val_loss: 13.0080 - val_acc: 0.5192 - val_f1_score: 0.5164\n",
      "Epoch 8/100\n",
      "29/29 - 6s - loss: 13.2470 - acc: 0.5749 - f1_score: 0.5726 - val_loss: 12.9960 - val_acc: 0.6250 - val_f1_score: 0.5771\n",
      "Epoch 9/100\n",
      "29/29 - 6s - loss: 13.2152 - acc: 0.5901 - f1_score: 0.5872 - val_loss: 13.0316 - val_acc: 0.6250 - val_f1_score: 0.5560\n",
      "Epoch 10/100\n",
      "29/29 - 6s - loss: 13.1687 - acc: 0.5881 - f1_score: 0.5852 - val_loss: 13.0364 - val_acc: 0.6154 - val_f1_score: 0.5404\n",
      "Epoch 11/100\n",
      "29/29 - 6s - loss: 13.1434 - acc: 0.6038 - f1_score: 0.6014 - val_loss: 13.0730 - val_acc: 0.6250 - val_f1_score: 0.5288\n",
      "Epoch 12/100\n",
      "29/29 - 6s - loss: 13.1205 - acc: 0.6092 - f1_score: 0.6054 - val_loss: 13.1427 - val_acc: 0.6442 - val_f1_score: 0.6044\n",
      "Epoch 13/100\n",
      "29/29 - 6s - loss: 13.0829 - acc: 0.6225 - f1_score: 0.6202 - val_loss: 13.2280 - val_acc: 0.6346 - val_f1_score: 0.5849\n",
      "Epoch 14/100\n",
      "29/29 - 6s - loss: 13.0741 - acc: 0.6239 - f1_score: 0.6208 - val_loss: 13.2969 - val_acc: 0.5577 - val_f1_score: 0.5048\n",
      "Epoch 15/100\n",
      "29/29 - 6s - loss: 13.0644 - acc: 0.6225 - f1_score: 0.6202 - val_loss: 13.2308 - val_acc: 0.5288 - val_f1_score: 0.5046\n",
      "Epoch 16/100\n",
      "29/29 - 6s - loss: 13.0310 - acc: 0.6419 - f1_score: 0.6394 - val_loss: 13.2402 - val_acc: 0.6346 - val_f1_score: 0.5909\n",
      "Epoch 17/100\n",
      "29/29 - 6s - loss: 13.0122 - acc: 0.6371 - f1_score: 0.6345 - val_loss: 13.2192 - val_acc: 0.6154 - val_f1_score: 0.5853\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.53      0.48        32\n",
      "           1       0.77      0.69      0.73        72\n",
      "\n",
      "    accuracy                           0.64       104\n",
      "   macro avg       0.60      0.61      0.60       104\n",
      "weighted avg       0.67      0.64      0.65       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1544\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1624\n",
      "Epoch 1/100\n",
      "28/28 - 69s - loss: 14.3280 - acc: 0.4863 - f1_score: 0.4831 - val_loss: 12.9504 - val_acc: 0.6638 - val_f1_score: 0.3990\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6408 - acc: 0.5235 - f1_score: 0.5201 - val_loss: 12.9423 - val_acc: 0.6638 - val_f1_score: 0.4558\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.4937 - acc: 0.5395 - f1_score: 0.5347 - val_loss: 12.9372 - val_acc: 0.6610 - val_f1_score: 0.4980\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4450 - acc: 0.5449 - f1_score: 0.5404 - val_loss: 12.9552 - val_acc: 0.6239 - val_f1_score: 0.5413\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3771 - acc: 0.5481 - f1_score: 0.5441 - val_loss: 12.9957 - val_acc: 0.5726 - val_f1_score: 0.5533\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3525 - acc: 0.5677 - f1_score: 0.5620 - val_loss: 13.0032 - val_acc: 0.5698 - val_f1_score: 0.5509\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2868 - acc: 0.5741 - f1_score: 0.5695 - val_loss: 13.0429 - val_acc: 0.5442 - val_f1_score: 0.5300\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2346 - acc: 0.5842 - f1_score: 0.5795 - val_loss: 13.0637 - val_acc: 0.4957 - val_f1_score: 0.4853\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2067 - acc: 0.5855 - f1_score: 0.5802 - val_loss: 13.0578 - val_acc: 0.5442 - val_f1_score: 0.5236\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.1768 - acc: 0.5959 - f1_score: 0.5911 - val_loss: 13.1051 - val_acc: 0.5242 - val_f1_score: 0.5099\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000228C2C3DE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.72      0.65       190\n",
      "           1       0.55      0.40      0.46       161\n",
      "\n",
      "    accuracy                           0.57       351\n",
      "   macro avg       0.57      0.56      0.55       351\n",
      "weighted avg       0.57      0.57      0.56       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1624\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1674\n",
      "Epoch 1/100\n",
      "28/28 - 73s - loss: 14.3799 - acc: 0.4837 - f1_score: 0.4776 - val_loss: 13.1327 - val_acc: 0.0431 - val_f1_score: 0.0418\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6262 - acc: 0.5250 - f1_score: 0.5248 - val_loss: 13.1368 - val_acc: 0.0800 - val_f1_score: 0.0800\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.5357 - acc: 0.5348 - f1_score: 0.5343 - val_loss: 13.1006 - val_acc: 0.2154 - val_f1_score: 0.1993\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4184 - acc: 0.5456 - f1_score: 0.5448 - val_loss: 13.0659 - val_acc: 0.3415 - val_f1_score: 0.2882\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3857 - acc: 0.5548 - f1_score: 0.5541 - val_loss: 13.0103 - val_acc: 0.5108 - val_f1_score: 0.3871\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3205 - acc: 0.5678 - f1_score: 0.5669 - val_loss: 13.0490 - val_acc: 0.4892 - val_f1_score: 0.3708\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2731 - acc: 0.5708 - f1_score: 0.5703 - val_loss: 12.9872 - val_acc: 0.5815 - val_f1_score: 0.4107\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2485 - acc: 0.5789 - f1_score: 0.5782 - val_loss: 13.0218 - val_acc: 0.5723 - val_f1_score: 0.4112\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2082 - acc: 0.5843 - f1_score: 0.5835 - val_loss: 12.9843 - val_acc: 0.6031 - val_f1_score: 0.4274\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.1929 - acc: 0.5852 - f1_score: 0.5846 - val_loss: 12.9397 - val_acc: 0.6246 - val_f1_score: 0.4328\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.1524 - acc: 0.6083 - f1_score: 0.6074 - val_loss: 12.9540 - val_acc: 0.6369 - val_f1_score: 0.4327\n",
      "Epoch 12/100\n",
      "28/28 - 6s - loss: 13.1311 - acc: 0.6066 - f1_score: 0.6058 - val_loss: 12.9479 - val_acc: 0.6492 - val_f1_score: 0.4388\n",
      "Epoch 13/100\n",
      "28/28 - 6s - loss: 13.1124 - acc: 0.6077 - f1_score: 0.6071 - val_loss: 13.0487 - val_acc: 0.6246 - val_f1_score: 0.4328\n",
      "Epoch 14/100\n",
      "28/28 - 6s - loss: 13.0807 - acc: 0.6249 - f1_score: 0.6242 - val_loss: 13.0763 - val_acc: 0.5846 - val_f1_score: 0.4122\n",
      "Epoch 15/100\n",
      "28/28 - 6s - loss: 13.0529 - acc: 0.6336 - f1_score: 0.6328 - val_loss: 13.1059 - val_acc: 0.6215 - val_f1_score: 0.4250\n",
      "Epoch 16/100\n",
      "28/28 - 6s - loss: 13.0350 - acc: 0.6289 - f1_score: 0.6282 - val_loss: 13.1546 - val_acc: 0.5631 - val_f1_score: 0.3958\n",
      "Epoch 17/100\n",
      "28/28 - 6s - loss: 13.0140 - acc: 0.6429 - f1_score: 0.6425 - val_loss: 13.0673 - val_acc: 0.6000 - val_f1_score: 0.4083\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000224E320ADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.05      0.10       113\n",
      "           1       0.66      0.97      0.78       212\n",
      "\n",
      "    accuracy                           0.65       325\n",
      "   macro avg       0.56      0.51      0.44       325\n",
      "weighted avg       0.59      0.65      0.54       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1674\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1688\n",
      "Epoch 1/100\n",
      "28/28 - 85s - loss: 14.3053 - acc: 0.4774 - f1_score: 0.4754 - val_loss: 12.9713 - val_acc: 0.5282 - val_f1_score: 0.3552\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.6279 - acc: 0.5168 - f1_score: 0.5141 - val_loss: 12.9639 - val_acc: 0.5462 - val_f1_score: 0.4381\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.4923 - acc: 0.5377 - f1_score: 0.5335 - val_loss: 12.9622 - val_acc: 0.5949 - val_f1_score: 0.5245\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4467 - acc: 0.5442 - f1_score: 0.5406 - val_loss: 12.9641 - val_acc: 0.5974 - val_f1_score: 0.5497\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3870 - acc: 0.5375 - f1_score: 0.5338 - val_loss: 12.9624 - val_acc: 0.6103 - val_f1_score: 0.6099\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3333 - acc: 0.5579 - f1_score: 0.5541 - val_loss: 12.9604 - val_acc: 0.6179 - val_f1_score: 0.6179\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2863 - acc: 0.5703 - f1_score: 0.5669 - val_loss: 12.9800 - val_acc: 0.5692 - val_f1_score: 0.5626\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2479 - acc: 0.5767 - f1_score: 0.5732 - val_loss: 13.0315 - val_acc: 0.5436 - val_f1_score: 0.5293\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.1860 - acc: 0.5862 - f1_score: 0.5825 - val_loss: 13.1001 - val_acc: 0.5231 - val_f1_score: 0.4909\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.1781 - acc: 0.5938 - f1_score: 0.5898 - val_loss: 13.1057 - val_acc: 0.5282 - val_f1_score: 0.5116\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.1618 - acc: 0.5876 - f1_score: 0.5832 - val_loss: 13.0900 - val_acc: 0.5308 - val_f1_score: 0.5291\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002283C354790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.66      0.62       187\n",
      "           1       0.65      0.58      0.61       203\n",
      "\n",
      "    accuracy                           0.62       390\n",
      "   macro avg       0.62      0.62      0.62       390\n",
      "weighted avg       0.62      0.62      0.62       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1688\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1717\n",
      "Epoch 1/100\n",
      "28/28 - 73s - loss: 14.2625 - acc: 0.4799 - f1_score: 0.4774 - val_loss: 12.9945 - val_acc: 0.4487 - val_f1_score: 0.3162\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.5697 - acc: 0.5271 - f1_score: 0.5255 - val_loss: 12.9987 - val_acc: 0.4744 - val_f1_score: 0.3855\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.4597 - acc: 0.5366 - f1_score: 0.5339 - val_loss: 13.0069 - val_acc: 0.4658 - val_f1_score: 0.3931\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4011 - acc: 0.5493 - f1_score: 0.5471 - val_loss: 12.9947 - val_acc: 0.5171 - val_f1_score: 0.5096\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3789 - acc: 0.5446 - f1_score: 0.5417 - val_loss: 12.9968 - val_acc: 0.5598 - val_f1_score: 0.5592\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3261 - acc: 0.5557 - f1_score: 0.5529 - val_loss: 12.9935 - val_acc: 0.5427 - val_f1_score: 0.5423\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2545 - acc: 0.5621 - f1_score: 0.5592 - val_loss: 13.0080 - val_acc: 0.5342 - val_f1_score: 0.5279\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2269 - acc: 0.5743 - f1_score: 0.5716 - val_loss: 13.0251 - val_acc: 0.5726 - val_f1_score: 0.5598\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2128 - acc: 0.5890 - f1_score: 0.5865 - val_loss: 13.0827 - val_acc: 0.5513 - val_f1_score: 0.5356\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.1986 - acc: 0.5934 - f1_score: 0.5908 - val_loss: 13.1652 - val_acc: 0.5556 - val_f1_score: 0.5243\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.1504 - acc: 0.6099 - f1_score: 0.6070 - val_loss: 13.2176 - val_acc: 0.5299 - val_f1_score: 0.4837\n",
      "Epoch 12/100\n",
      "28/28 - 6s - loss: 13.1368 - acc: 0.5976 - f1_score: 0.5942 - val_loss: 13.1188 - val_acc: 0.5684 - val_f1_score: 0.5411\n",
      "Epoch 13/100\n",
      "28/28 - 6s - loss: 13.0842 - acc: 0.6217 - f1_score: 0.6188 - val_loss: 13.0813 - val_acc: 0.6068 - val_f1_score: 0.5924\n",
      "Epoch 14/100\n",
      "28/28 - 6s - loss: 13.0601 - acc: 0.6304 - f1_score: 0.6278 - val_loss: 13.1392 - val_acc: 0.5641 - val_f1_score: 0.5510\n",
      "Epoch 15/100\n",
      "28/28 - 6s - loss: 13.0480 - acc: 0.6357 - f1_score: 0.6327 - val_loss: 13.2079 - val_acc: 0.5427 - val_f1_score: 0.5375\n",
      "Epoch 16/100\n",
      "28/28 - 6s - loss: 13.0379 - acc: 0.6403 - f1_score: 0.6379 - val_loss: 13.2610 - val_acc: 0.5556 - val_f1_score: 0.5471\n",
      "Epoch 17/100\n",
      "28/28 - 6s - loss: 13.0136 - acc: 0.6419 - f1_score: 0.6391 - val_loss: 13.2188 - val_acc: 0.5726 - val_f1_score: 0.5655\n",
      "Epoch 18/100\n",
      "28/28 - 6s - loss: 12.9896 - acc: 0.6493 - f1_score: 0.6464 - val_loss: 13.2527 - val_acc: 0.5214 - val_f1_score: 0.5185\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.52        86\n",
      "           1       0.72      0.63      0.67       148\n",
      "\n",
      "    accuracy                           0.61       234\n",
      "   macro avg       0.59      0.60      0.59       234\n",
      "weighted avg       0.63      0.61      0.61       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1717\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1818\n",
      "Epoch 1/100\n",
      "27/27 - 69s - loss: 14.4113 - acc: 0.4727 - f1_score: 0.4661 - val_loss: 13.0139 - val_acc: 0.3697 - val_f1_score: 0.2897\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6492 - acc: 0.5150 - f1_score: 0.5139 - val_loss: 13.0395 - val_acc: 0.3632 - val_f1_score: 0.2837\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.5291 - acc: 0.5335 - f1_score: 0.5320 - val_loss: 13.0288 - val_acc: 0.4359 - val_f1_score: 0.4078\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4600 - acc: 0.5363 - f1_score: 0.5335 - val_loss: 13.0349 - val_acc: 0.4423 - val_f1_score: 0.4299\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.4053 - acc: 0.5405 - f1_score: 0.5388 - val_loss: 13.0133 - val_acc: 0.4594 - val_f1_score: 0.4594\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3356 - acc: 0.5570 - f1_score: 0.5543 - val_loss: 13.0139 - val_acc: 0.4936 - val_f1_score: 0.4924\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.2976 - acc: 0.5656 - f1_score: 0.5633 - val_loss: 12.9960 - val_acc: 0.5363 - val_f1_score: 0.5266\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2367 - acc: 0.5748 - f1_score: 0.5731 - val_loss: 12.9900 - val_acc: 0.5919 - val_f1_score: 0.5637\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.2271 - acc: 0.5848 - f1_score: 0.5826 - val_loss: 13.0106 - val_acc: 0.5833 - val_f1_score: 0.5305\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.2022 - acc: 0.5831 - f1_score: 0.5810 - val_loss: 13.0408 - val_acc: 0.5662 - val_f1_score: 0.5269\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1870 - acc: 0.5947 - f1_score: 0.5932 - val_loss: 13.0504 - val_acc: 0.5684 - val_f1_score: 0.5190\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1409 - acc: 0.6010 - f1_score: 0.5991 - val_loss: 13.0914 - val_acc: 0.5684 - val_f1_score: 0.5129\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.1168 - acc: 0.6071 - f1_score: 0.6048 - val_loss: 13.1453 - val_acc: 0.5491 - val_f1_score: 0.5121\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000227CFFDFC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.44      0.45       180\n",
      "           1       0.66      0.69      0.67       288\n",
      "\n",
      "    accuracy                           0.59       468\n",
      "   macro avg       0.56      0.56      0.56       468\n",
      "weighted avg       0.59      0.59      0.59       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1818\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1892\n",
      "Epoch 1/100\n",
      "27/27 - 67s - loss: 14.4709 - acc: 0.4628 - f1_score: 0.4614 - val_loss: 12.8800 - val_acc: 0.9573 - val_f1_score: 0.4891\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6485 - acc: 0.5208 - f1_score: 0.5116 - val_loss: 12.8278 - val_acc: 0.9615 - val_f1_score: 0.4902\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.5271 - acc: 0.5399 - f1_score: 0.5298 - val_loss: 12.8366 - val_acc: 0.8974 - val_f1_score: 0.5113\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4328 - acc: 0.5508 - f1_score: 0.5396 - val_loss: 12.8194 - val_acc: 0.8761 - val_f1_score: 0.4836\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.4119 - acc: 0.5448 - f1_score: 0.5341 - val_loss: 12.8523 - val_acc: 0.7628 - val_f1_score: 0.4657\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3503 - acc: 0.5558 - f1_score: 0.5439 - val_loss: 12.8938 - val_acc: 0.6731 - val_f1_score: 0.4424\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.3022 - acc: 0.5695 - f1_score: 0.5581 - val_loss: 12.9476 - val_acc: 0.5940 - val_f1_score: 0.4087\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2415 - acc: 0.5789 - f1_score: 0.5689 - val_loss: 13.0781 - val_acc: 0.4915 - val_f1_score: 0.3632\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       429\n",
      "           1       0.15      0.05      0.08        39\n",
      "\n",
      "    accuracy                           0.90       468\n",
      "   macro avg       0.54      0.51      0.51       468\n",
      "weighted avg       0.85      0.90      0.87       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1892\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1929\n",
      "Epoch 1/100\n",
      "27/27 - 77s - loss: 14.3727 - acc: 0.4810 - f1_score: 0.4727 - val_loss: 13.0748 - val_acc: 0.2201 - val_f1_score: 0.1804\n",
      "Epoch 2/100\n",
      "27/27 - 6s - loss: 13.6700 - acc: 0.5176 - f1_score: 0.5172 - val_loss: 13.1278 - val_acc: 0.2222 - val_f1_score: 0.1834\n",
      "Epoch 3/100\n",
      "27/27 - 6s - loss: 13.5325 - acc: 0.5328 - f1_score: 0.5320 - val_loss: 13.1210 - val_acc: 0.2479 - val_f1_score: 0.2228\n",
      "Epoch 4/100\n",
      "27/27 - 6s - loss: 13.4505 - acc: 0.5405 - f1_score: 0.5392 - val_loss: 13.1502 - val_acc: 0.2778 - val_f1_score: 0.2613\n",
      "Epoch 5/100\n",
      "27/27 - 6s - loss: 13.4120 - acc: 0.5450 - f1_score: 0.5443 - val_loss: 13.1144 - val_acc: 0.3654 - val_f1_score: 0.3650\n",
      "Epoch 6/100\n",
      "27/27 - 6s - loss: 13.3417 - acc: 0.5463 - f1_score: 0.5450 - val_loss: 13.1075 - val_acc: 0.4081 - val_f1_score: 0.4044\n",
      "Epoch 7/100\n",
      "27/27 - 6s - loss: 13.2974 - acc: 0.5647 - f1_score: 0.5635 - val_loss: 13.0892 - val_acc: 0.4530 - val_f1_score: 0.4374\n",
      "Epoch 8/100\n",
      "27/27 - 6s - loss: 13.2641 - acc: 0.5644 - f1_score: 0.5637 - val_loss: 13.0308 - val_acc: 0.5192 - val_f1_score: 0.4600\n",
      "Epoch 9/100\n",
      "27/27 - 6s - loss: 13.2225 - acc: 0.5764 - f1_score: 0.5755 - val_loss: 13.0289 - val_acc: 0.5321 - val_f1_score: 0.4385\n",
      "Epoch 10/100\n",
      "27/27 - 6s - loss: 13.1983 - acc: 0.5806 - f1_score: 0.5793 - val_loss: 13.0472 - val_acc: 0.5577 - val_f1_score: 0.4693\n",
      "Epoch 11/100\n",
      "27/27 - 6s - loss: 13.1766 - acc: 0.5929 - f1_score: 0.5922 - val_loss: 13.0194 - val_acc: 0.5726 - val_f1_score: 0.4623\n",
      "Epoch 12/100\n",
      "27/27 - 6s - loss: 13.1419 - acc: 0.6050 - f1_score: 0.6040 - val_loss: 13.0273 - val_acc: 0.6026 - val_f1_score: 0.4950\n",
      "Epoch 13/100\n",
      "27/27 - 6s - loss: 13.1115 - acc: 0.6074 - f1_score: 0.6063 - val_loss: 13.0841 - val_acc: 0.5897 - val_f1_score: 0.5088\n",
      "Epoch 14/100\n",
      "27/27 - 6s - loss: 13.0765 - acc: 0.6152 - f1_score: 0.6143 - val_loss: 13.0782 - val_acc: 0.6368 - val_f1_score: 0.5452\n",
      "Epoch 15/100\n",
      "27/27 - 6s - loss: 13.0611 - acc: 0.6148 - f1_score: 0.6141 - val_loss: 13.0666 - val_acc: 0.6368 - val_f1_score: 0.5157\n",
      "Epoch 16/100\n",
      "27/27 - 6s - loss: 13.0509 - acc: 0.6294 - f1_score: 0.6283 - val_loss: 13.0759 - val_acc: 0.6603 - val_f1_score: 0.5457\n",
      "Epoch 17/100\n",
      "27/27 - 6s - loss: 13.0241 - acc: 0.6284 - f1_score: 0.6273 - val_loss: 13.1085 - val_acc: 0.6538 - val_f1_score: 0.5358\n",
      "Epoch 18/100\n",
      "27/27 - 6s - loss: 12.9926 - acc: 0.6491 - f1_score: 0.6480 - val_loss: 13.1327 - val_acc: 0.6218 - val_f1_score: 0.5133\n",
      "Epoch 19/100\n",
      "27/27 - 6s - loss: 12.9895 - acc: 0.6515 - f1_score: 0.6504 - val_loss: 13.1360 - val_acc: 0.6303 - val_f1_score: 0.5267\n",
      "Epoch 20/100\n",
      "27/27 - 6s - loss: 12.9834 - acc: 0.6568 - f1_score: 0.6557 - val_loss: 13.1690 - val_acc: 0.6090 - val_f1_score: 0.5184\n",
      "Epoch 21/100\n",
      "27/27 - 6s - loss: 12.9436 - acc: 0.6671 - f1_score: 0.6663 - val_loss: 13.1625 - val_acc: 0.6303 - val_f1_score: 0.5291\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.29      0.32       129\n",
      "           1       0.75      0.80      0.77       339\n",
      "\n",
      "    accuracy                           0.66       468\n",
      "   macro avg       0.55      0.54      0.55       468\n",
      "weighted avg       0.64      0.66      0.65       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1929\\assets\n",
      "(None, 2560, 1)\n",
      "(None, 2560, 3)\n",
      "(None, 366, 64)\n",
      "(None, 366, 64)\n",
      "Testing on 1933\n",
      "Epoch 1/100\n",
      "28/28 - 108s - loss: 14.2587 - acc: 0.4826 - f1_score: 0.4792 - val_loss: 13.0522 - val_acc: 0.3376 - val_f1_score: 0.2572\n",
      "Epoch 2/100\n",
      "28/28 - 6s - loss: 13.5686 - acc: 0.5209 - f1_score: 0.5195 - val_loss: 13.0731 - val_acc: 0.3248 - val_f1_score: 0.2542\n",
      "Epoch 3/100\n",
      "28/28 - 6s - loss: 13.4675 - acc: 0.5299 - f1_score: 0.5274 - val_loss: 13.0881 - val_acc: 0.3419 - val_f1_score: 0.2812\n",
      "Epoch 4/100\n",
      "28/28 - 6s - loss: 13.4232 - acc: 0.5408 - f1_score: 0.5387 - val_loss: 13.0685 - val_acc: 0.3504 - val_f1_score: 0.3364\n",
      "Epoch 5/100\n",
      "28/28 - 6s - loss: 13.3724 - acc: 0.5445 - f1_score: 0.5420 - val_loss: 13.0756 - val_acc: 0.3675 - val_f1_score: 0.3659\n",
      "Epoch 6/100\n",
      "28/28 - 6s - loss: 13.3071 - acc: 0.5577 - f1_score: 0.5553 - val_loss: 13.1014 - val_acc: 0.3803 - val_f1_score: 0.3794\n",
      "Epoch 7/100\n",
      "28/28 - 6s - loss: 13.2596 - acc: 0.5707 - f1_score: 0.5684 - val_loss: 13.1143 - val_acc: 0.4060 - val_f1_score: 0.4035\n",
      "Epoch 8/100\n",
      "28/28 - 6s - loss: 13.2299 - acc: 0.5715 - f1_score: 0.5694 - val_loss: 13.0923 - val_acc: 0.4744 - val_f1_score: 0.4560\n",
      "Epoch 9/100\n",
      "28/28 - 6s - loss: 13.2091 - acc: 0.5900 - f1_score: 0.5876 - val_loss: 13.0899 - val_acc: 0.4957 - val_f1_score: 0.4755\n",
      "Epoch 10/100\n",
      "28/28 - 6s - loss: 13.1963 - acc: 0.5916 - f1_score: 0.5890 - val_loss: 13.0743 - val_acc: 0.5214 - val_f1_score: 0.4827\n",
      "Epoch 11/100\n",
      "28/28 - 6s - loss: 13.1430 - acc: 0.6084 - f1_score: 0.6063 - val_loss: 13.1096 - val_acc: 0.5897 - val_f1_score: 0.5543\n",
      "Epoch 12/100\n",
      "28/28 - 6s - loss: 13.1182 - acc: 0.6078 - f1_score: 0.6052 - val_loss: 13.1247 - val_acc: 0.5556 - val_f1_score: 0.5091\n",
      "Epoch 13/100\n",
      "28/28 - 6s - loss: 13.0897 - acc: 0.6140 - f1_score: 0.6115 - val_loss: 13.0959 - val_acc: 0.6154 - val_f1_score: 0.5673\n",
      "Epoch 14/100\n",
      "28/28 - 6s - loss: 13.0668 - acc: 0.6252 - f1_score: 0.6231 - val_loss: 13.0591 - val_acc: 0.6197 - val_f1_score: 0.5707\n",
      "Epoch 15/100\n",
      "28/28 - 6s - loss: 13.0391 - acc: 0.6286 - f1_score: 0.6257 - val_loss: 13.0723 - val_acc: 0.5983 - val_f1_score: 0.5563\n",
      "Epoch 16/100\n",
      "28/28 - 6s - loss: 13.0361 - acc: 0.6363 - f1_score: 0.6344 - val_loss: 13.0945 - val_acc: 0.6197 - val_f1_score: 0.5735\n",
      "Epoch 17/100\n",
      "28/28 - 6s - loss: 13.0028 - acc: 0.6469 - f1_score: 0.6448 - val_loss: 13.1004 - val_acc: 0.6111 - val_f1_score: 0.5692\n",
      "Epoch 18/100\n",
      "28/28 - 6s - loss: 13.0080 - acc: 0.6513 - f1_score: 0.6490 - val_loss: 13.1471 - val_acc: 0.5940 - val_f1_score: 0.5623\n",
      "Epoch 19/100\n",
      "28/28 - 6s - loss: 12.9530 - acc: 0.6669 - f1_score: 0.6651 - val_loss: 13.1606 - val_acc: 0.5940 - val_f1_score: 0.5623\n",
      "Epoch 20/100\n",
      "28/28 - 6s - loss: 12.9516 - acc: 0.6706 - f1_score: 0.6687 - val_loss: 13.1876 - val_acc: 0.6197 - val_f1_score: 0.5879\n",
      "Epoch 21/100\n",
      "28/28 - 6s - loss: 12.9380 - acc: 0.6733 - f1_score: 0.6712 - val_loss: 13.2325 - val_acc: 0.5726 - val_f1_score: 0.5486\n",
      "Epoch 22/100\n",
      "28/28 - 6s - loss: 12.9327 - acc: 0.6798 - f1_score: 0.6782 - val_loss: 13.2432 - val_acc: 0.5470 - val_f1_score: 0.5151\n",
      "Epoch 23/100\n",
      "28/28 - 6s - loss: 12.9101 - acc: 0.6887 - f1_score: 0.6868 - val_loss: 13.2879 - val_acc: 0.5171 - val_f1_score: 0.4985\n",
      "Epoch 24/100\n",
      "28/28 - 6s - loss: 12.9000 - acc: 0.6912 - f1_score: 0.6893 - val_loss: 13.3055 - val_acc: 0.5214 - val_f1_score: 0.5038\n",
      "Epoch 25/100\n",
      "28/28 - 6s - loss: 12.8803 - acc: 0.7023 - f1_score: 0.7008 - val_loss: 13.2385 - val_acc: 0.5598 - val_f1_score: 0.5277\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.44      0.47        91\n",
      "           1       0.67      0.73      0.70       143\n",
      "\n",
      "    accuracy                           0.62       234\n",
      "   macro avg       0.59      0.59      0.59       234\n",
      "weighted avg       0.61      0.62      0.61       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-204902\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Type FF, Stage FF\n",
      "Average Accuracy:  0.6228150744530054\n",
      "F1 score for Baseline:  0.4987818131583718\n",
      "F1 score for Stress:  0.5564414575638561\n",
      "Macro F1:  0.527611635361114\n",
      "Weighted F1:  0.6039457098865072\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "model = mega_resnet(input_shape=[(2560, 1), (2560, 3)], attx_type='FF', attx_st='FF', classes = num_classes)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['FF']\n",
    "attx_st = ['FF']\n",
    "\n",
    "# attx_type = ['III']\n",
    "# attx_st = ['all']\n",
    "\n",
    "for conn_type in attx_type:\n",
    "    \n",
    "    for conn_stage in attx_st:\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Training for Type {}, Stage {}\".format(conn_type, conn_stage))\n",
    "        print(\"--------------------------------------------------------------------------\\n\")        \n",
    "        \n",
    "        hs, preds, clr = {}, {}, {}\n",
    "\n",
    "        path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "        tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "        for i in sub_dict_ecg.keys():\n",
    "\n",
    "            if i in ['1765']:\n",
    "                continue\n",
    "\n",
    "            opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "            tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                               datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "            X_test_ecg = sub_dict_ecg[i]\n",
    "            y_test = sub_label_ecg[i]\n",
    "            X_test_eda = sub_dict_eda[i]\n",
    "\n",
    "            X_test_ecg = vstack(X_test_ecg)\n",
    "            X_test_eda = vstack(X_test_eda)\n",
    "            y_test = [x for z in y_test for x in z]\n",
    "\n",
    "            X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n",
    "            X_eda = [vstack(v) for k, v in sub_dict_eda.items() if k != i]\n",
    "            y_train = [hstack(np.asarray(v)) for k, v in sub_label_ecg.items() if k != i]\n",
    "\n",
    "            X_ecg = vstack(X_ecg)\n",
    "            X_eda = vstack(X_eda)\n",
    "            y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "            y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "            y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "            \n",
    "            y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "            y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "            callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                              patience=5, verbose=1, mode='max', \n",
    "                                                              restore_best_weights=True)\n",
    "\n",
    "            class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "            model = mega_resnet(input_shape=[(2560, 1), (2560, 3)], \n",
    "                               attx_type=conn_type,\n",
    "                               attx_st=conn_stage,\n",
    "                               classes = num_classes)\n",
    "            mod_1 = inspect.getsource(mega_model)\n",
    "            model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "            print('Testing on {}'.format(i))\n",
    "\n",
    "            hist = model.fit([X_ecg, X_eda], y, epochs=100, verbose=2, shuffle=True,\n",
    "                            batch_size = 256, validation_data = ([X_test_ecg, X_test_eda], y_test),\n",
    "                            callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "            y_pred_i = model.predict([X_test_ecg, X_test_eda], batch_size = 128)\n",
    "\n",
    "            pred_list = list()\n",
    "            test_y = list()\n",
    "\n",
    "            for n in range(len(y_pred_i)):\n",
    "                pred_list.append(np.argmax(y_pred_i[n]))\n",
    "                test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            print(classification_report(pred_list, test_y))\n",
    "            a = classification_report(pred_list, test_y,\n",
    "                                      target_names = ['Baseline', 'Stress'],\n",
    "                                      output_dict=True)\n",
    "\n",
    "            clr[i] = a\n",
    "            hs[i] = hist\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "            scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                        'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "            model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "            model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "            model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "            with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "                pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "            K.clear_session()\n",
    "            \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print('Classfication report for Type {}, Stage {}'.format(conn_type, conn_stage))    \n",
    "        score_class(clr)\n",
    "        print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Training for Modality ecg\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-185713\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_weights\n",
      "Testing on 1105\n",
      "Epoch 1/150\n",
      "27/27 - 30s - loss: 7.5103 - acc: 0.4743 - f1_score: 0.3562 - val_loss: 5.0146 - val_acc: 0.4060 - val_f1_score: 0.2888\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0174 - acc: 0.4908 - f1_score: 0.4581 - val_loss: 5.0131 - val_acc: 0.4231 - val_f1_score: 0.3149\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6462 - acc: 0.5104 - f1_score: 0.5083 - val_loss: 4.9955 - val_acc: 0.4444 - val_f1_score: 0.4190\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5148 - acc: 0.5243 - f1_score: 0.5242 - val_loss: 4.9809 - val_acc: 0.5214 - val_f1_score: 0.5144\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4538 - acc: 0.5231 - f1_score: 0.5218 - val_loss: 4.9807 - val_acc: 0.5406 - val_f1_score: 0.5002\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4445 - acc: 0.5353 - f1_score: 0.5329 - val_loss: 4.9931 - val_acc: 0.5363 - val_f1_score: 0.4871\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3933 - acc: 0.5332 - f1_score: 0.5308 - val_loss: 5.0041 - val_acc: 0.5449 - val_f1_score: 0.5049\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3484 - acc: 0.5460 - f1_score: 0.5432 - val_loss: 5.0263 - val_acc: 0.5192 - val_f1_score: 0.4823\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.2987 - acc: 0.5443 - f1_score: 0.5415 - val_loss: 5.0411 - val_acc: 0.5214 - val_f1_score: 0.4912\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2906 - acc: 0.5469 - f1_score: 0.5437 - val_loss: 5.0701 - val_acc: 0.4936 - val_f1_score: 0.4678\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2429 - acc: 0.5599 - f1_score: 0.5565 - val_loss: 5.0845 - val_acc: 0.4872 - val_f1_score: 0.4693\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.43      0.46       217\n",
      "           1       0.55      0.60      0.57       251\n",
      "\n",
      "    accuracy                           0.52       468\n",
      "   macro avg       0.52      0.52      0.51       468\n",
      "weighted avg       0.52      0.52      0.52       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1105\\assets\n",
      "Testing on 1106\n",
      "Epoch 1/150\n",
      "27/27 - 22s - loss: 7.4693 - acc: 0.4949 - f1_score: 0.3660 - val_loss: 5.1454 - val_acc: 0.0833 - val_f1_score: 0.0769\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0217 - acc: 0.5004 - f1_score: 0.4597 - val_loss: 5.1709 - val_acc: 0.0855 - val_f1_score: 0.0806\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6494 - acc: 0.5096 - f1_score: 0.5038 - val_loss: 5.1447 - val_acc: 0.0897 - val_f1_score: 0.0855\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5207 - acc: 0.5212 - f1_score: 0.5210 - val_loss: 5.0886 - val_acc: 0.1410 - val_f1_score: 0.1410\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4570 - acc: 0.5196 - f1_score: 0.5196 - val_loss: 5.0658 - val_acc: 0.2821 - val_f1_score: 0.2646\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4450 - acc: 0.5425 - f1_score: 0.5423 - val_loss: 5.0797 - val_acc: 0.3675 - val_f1_score: 0.3261\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3960 - acc: 0.5380 - f1_score: 0.5378 - val_loss: 5.0974 - val_acc: 0.4060 - val_f1_score: 0.3494\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3349 - acc: 0.5482 - f1_score: 0.5477 - val_loss: 5.1547 - val_acc: 0.3996 - val_f1_score: 0.3485\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.2898 - acc: 0.5432 - f1_score: 0.5429 - val_loss: 5.2109 - val_acc: 0.3953 - val_f1_score: 0.3472\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2699 - acc: 0.5532 - f1_score: 0.5528 - val_loss: 5.2826 - val_acc: 0.3803 - val_f1_score: 0.3367\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2332 - acc: 0.5650 - f1_score: 0.5645 - val_loss: 5.3572 - val_acc: 0.3697 - val_f1_score: 0.3306\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2288 - acc: 0.5554 - f1_score: 0.5551 - val_loss: 5.4288 - val_acc: 0.3739 - val_f1_score: 0.3337\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2018 - acc: 0.5702 - f1_score: 0.5698 - val_loss: 5.4972 - val_acc: 0.3825 - val_f1_score: 0.3382\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.1786 - acc: 0.5719 - f1_score: 0.5717 - val_loss: 5.6106 - val_acc: 0.3568 - val_f1_score: 0.3199\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.09      0.16       291\n",
      "           1       0.38      0.93      0.54       177\n",
      "\n",
      "    accuracy                           0.41       468\n",
      "   macro avg       0.52      0.51      0.35       468\n",
      "weighted avg       0.56      0.41      0.30       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1106\\assets\n",
      "Testing on 1175\n",
      "Epoch 1/150\n",
      "28/28 - 27s - loss: 7.8552 - acc: 0.4545 - f1_score: 0.3408 - val_loss: 4.9231 - val_acc: 0.7162 - val_f1_score: 0.4173\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.1517 - acc: 0.4745 - f1_score: 0.4367 - val_loss: 4.9299 - val_acc: 0.6658 - val_f1_score: 0.4144\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6656 - acc: 0.5119 - f1_score: 0.5090 - val_loss: 4.9688 - val_acc: 0.5517 - val_f1_score: 0.4869\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5829 - acc: 0.5203 - f1_score: 0.5198 - val_loss: 5.0229 - val_acc: 0.4191 - val_f1_score: 0.4125\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4987 - acc: 0.5267 - f1_score: 0.5232 - val_loss: 5.0730 - val_acc: 0.3979 - val_f1_score: 0.3976\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4543 - acc: 0.5429 - f1_score: 0.5375 - val_loss: 5.1215 - val_acc: 0.3899 - val_f1_score: 0.3899\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3920 - acc: 0.5423 - f1_score: 0.5370 - val_loss: 5.1587 - val_acc: 0.4085 - val_f1_score: 0.4077\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3662 - acc: 0.5375 - f1_score: 0.5307 - val_loss: 5.1596 - val_acc: 0.4589 - val_f1_score: 0.4565\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3579 - acc: 0.5438 - f1_score: 0.5382 - val_loss: 5.1392 - val_acc: 0.4642 - val_f1_score: 0.4601\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3430 - acc: 0.5420 - f1_score: 0.5369 - val_loss: 5.1906 - val_acc: 0.4430 - val_f1_score: 0.4412\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67       238\n",
      "           1       0.36      0.27      0.30       139\n",
      "\n",
      "    accuracy                           0.55       377\n",
      "   macro avg       0.49      0.49      0.49       377\n",
      "weighted avg       0.53      0.55      0.53       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1175\\assets\n",
      "Testing on 1194\n",
      "Epoch 1/150\n",
      "28/28 - 24s - loss: 7.5326 - acc: 0.4835 - f1_score: 0.3545 - val_loss: 5.0590 - val_acc: 0.2000 - val_f1_score: 0.1725\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.1136 - acc: 0.4941 - f1_score: 0.4393 - val_loss: 5.0738 - val_acc: 0.2264 - val_f1_score: 0.2103\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6592 - acc: 0.5189 - f1_score: 0.5107 - val_loss: 5.0356 - val_acc: 0.3275 - val_f1_score: 0.3251\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5609 - acc: 0.5162 - f1_score: 0.5155 - val_loss: 4.9810 - val_acc: 0.5143 - val_f1_score: 0.4568\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5556 - acc: 0.5224 - f1_score: 0.5222 - val_loss: 4.9580 - val_acc: 0.5802 - val_f1_score: 0.4822\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4838 - acc: 0.5382 - f1_score: 0.5378 - val_loss: 4.9553 - val_acc: 0.5978 - val_f1_score: 0.4629\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.5023 - acc: 0.5318 - f1_score: 0.5312 - val_loss: 4.9682 - val_acc: 0.6110 - val_f1_score: 0.4865\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.4213 - acc: 0.5338 - f1_score: 0.5327 - val_loss: 4.9804 - val_acc: 0.6066 - val_f1_score: 0.4837\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.4007 - acc: 0.5333 - f1_score: 0.5313 - val_loss: 5.0159 - val_acc: 0.5780 - val_f1_score: 0.4477\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3824 - acc: 0.5363 - f1_score: 0.5350 - val_loss: 5.0529 - val_acc: 0.5538 - val_f1_score: 0.4271\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.3777 - acc: 0.5374 - f1_score: 0.5367 - val_loss: 5.0489 - val_acc: 0.5648 - val_f1_score: 0.4485\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.3632 - acc: 0.5396 - f1_score: 0.5386 - val_loss: 5.0683 - val_acc: 0.5956 - val_f1_score: 0.4848\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.3396 - acc: 0.5460 - f1_score: 0.5440 - val_loss: 5.1213 - val_acc: 0.5451 - val_f1_score: 0.4650\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.3287 - acc: 0.5403 - f1_score: 0.5391 - val_loss: 5.1517 - val_acc: 0.5209 - val_f1_score: 0.4483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.19      0.23       140\n",
      "           1       0.69      0.80      0.74       315\n",
      "\n",
      "    accuracy                           0.61       455\n",
      "   macro avg       0.49      0.49      0.49       455\n",
      "weighted avg       0.57      0.61      0.58       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1194\\assets\n",
      "Testing on 1337\n",
      "Epoch 1/150\n",
      "27/27 - 25s - loss: 7.6498 - acc: 0.4526 - f1_score: 0.3436 - val_loss: 4.9123 - val_acc: 0.7137 - val_f1_score: 0.4165\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.1016 - acc: 0.4799 - f1_score: 0.4508 - val_loss: 4.9102 - val_acc: 0.7137 - val_f1_score: 0.4165\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6474 - acc: 0.5054 - f1_score: 0.5041 - val_loss: 4.9392 - val_acc: 0.7137 - val_f1_score: 0.4501\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5286 - acc: 0.5315 - f1_score: 0.5303 - val_loss: 4.9957 - val_acc: 0.4957 - val_f1_score: 0.4842\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4803 - acc: 0.5263 - f1_score: 0.5220 - val_loss: 5.0559 - val_acc: 0.3355 - val_f1_score: 0.3139\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4207 - acc: 0.5396 - f1_score: 0.5330 - val_loss: 5.1175 - val_acc: 0.3120 - val_f1_score: 0.2748\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3935 - acc: 0.5427 - f1_score: 0.5363 - val_loss: 5.1810 - val_acc: 0.3141 - val_f1_score: 0.2777\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3565 - acc: 0.5574 - f1_score: 0.5508 - val_loss: 5.2275 - val_acc: 0.3098 - val_f1_score: 0.2718\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3017 - acc: 0.5535 - f1_score: 0.5464 - val_loss: 5.2757 - val_acc: 0.3034 - val_f1_score: 0.2728\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2856 - acc: 0.5621 - f1_score: 0.5548 - val_loss: 5.3173 - val_acc: 0.3184 - val_f1_score: 0.2962\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2477 - acc: 0.5651 - f1_score: 0.5570 - val_loss: 5.3473 - val_acc: 0.2991 - val_f1_score: 0.2769\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.76      0.56       200\n",
      "           1       0.62      0.30      0.41       268\n",
      "\n",
      "    accuracy                           0.50       468\n",
      "   macro avg       0.53      0.53      0.48       468\n",
      "weighted avg       0.55      0.50      0.47       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1337\\assets\n",
      "Testing on 1390\n",
      "Epoch 1/150\n",
      "27/27 - 25s - loss: 7.5064 - acc: 0.4710 - f1_score: 0.3517 - val_loss: 4.9914 - val_acc: 0.4530 - val_f1_score: 0.3337\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0526 - acc: 0.4928 - f1_score: 0.4585 - val_loss: 4.9925 - val_acc: 0.4423 - val_f1_score: 0.3695\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6194 - acc: 0.5159 - f1_score: 0.5129 - val_loss: 4.9833 - val_acc: 0.4850 - val_f1_score: 0.4838\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5089 - acc: 0.5275 - f1_score: 0.5273 - val_loss: 4.9831 - val_acc: 0.5085 - val_f1_score: 0.4959\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4659 - acc: 0.5286 - f1_score: 0.5270 - val_loss: 4.9890 - val_acc: 0.5235 - val_f1_score: 0.4972\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4379 - acc: 0.5254 - f1_score: 0.5228 - val_loss: 5.0018 - val_acc: 0.5192 - val_f1_score: 0.4895\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3805 - acc: 0.5395 - f1_score: 0.5366 - val_loss: 5.0215 - val_acc: 0.5214 - val_f1_score: 0.4965\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3494 - acc: 0.5506 - f1_score: 0.5475 - val_loss: 5.0468 - val_acc: 0.5043 - val_f1_score: 0.4785\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3210 - acc: 0.5434 - f1_score: 0.5403 - val_loss: 5.0734 - val_acc: 0.5278 - val_f1_score: 0.5017\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2979 - acc: 0.5492 - f1_score: 0.5451 - val_loss: 5.0985 - val_acc: 0.5299 - val_f1_score: 0.5065\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2411 - acc: 0.5632 - f1_score: 0.5596 - val_loss: 5.1244 - val_acc: 0.4893 - val_f1_score: 0.4710\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2391 - acc: 0.5487 - f1_score: 0.5456 - val_loss: 5.1564 - val_acc: 0.4551 - val_f1_score: 0.4408\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2129 - acc: 0.5637 - f1_score: 0.5587 - val_loss: 5.1665 - val_acc: 0.4551 - val_f1_score: 0.4408\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.1875 - acc: 0.5651 - f1_score: 0.5616 - val_loss: 5.1531 - val_acc: 0.5000 - val_f1_score: 0.4865\n",
      "Epoch 15/150\n",
      "27/27 - 3s - loss: 5.1572 - acc: 0.5695 - f1_score: 0.5663 - val_loss: 5.1533 - val_acc: 0.5171 - val_f1_score: 0.5096\n",
      "Epoch 16/150\n",
      "27/27 - 3s - loss: 5.1844 - acc: 0.5648 - f1_score: 0.5615 - val_loss: 5.1443 - val_acc: 0.5256 - val_f1_score: 0.5154\n",
      "Epoch 17/150\n",
      "27/27 - 3s - loss: 5.1415 - acc: 0.5809 - f1_score: 0.5781 - val_loss: 5.1538 - val_acc: 0.5385 - val_f1_score: 0.5238\n",
      "Epoch 18/150\n",
      "27/27 - 3s - loss: 5.1085 - acc: 0.5818 - f1_score: 0.5778 - val_loss: 5.1449 - val_acc: 0.5385 - val_f1_score: 0.5318\n",
      "Epoch 19/150\n",
      "27/27 - 3s - loss: 5.0976 - acc: 0.5839 - f1_score: 0.5812 - val_loss: 5.1162 - val_acc: 0.5321 - val_f1_score: 0.5240\n",
      "Epoch 20/150\n",
      "27/27 - 3s - loss: 5.0920 - acc: 0.5813 - f1_score: 0.5780 - val_loss: 5.0964 - val_acc: 0.5620 - val_f1_score: 0.5528\n",
      "Epoch 21/150\n",
      "27/27 - 3s - loss: 5.0587 - acc: 0.5918 - f1_score: 0.5888 - val_loss: 5.1386 - val_acc: 0.5449 - val_f1_score: 0.5335\n",
      "Epoch 22/150\n",
      "27/27 - 3s - loss: 5.0431 - acc: 0.5997 - f1_score: 0.5970 - val_loss: 5.1649 - val_acc: 0.5363 - val_f1_score: 0.5220\n",
      "Epoch 23/150\n",
      "27/27 - 3s - loss: 5.0372 - acc: 0.5922 - f1_score: 0.5889 - val_loss: 5.1763 - val_acc: 0.5363 - val_f1_score: 0.5197\n",
      "Epoch 24/150\n",
      "27/27 - 3s - loss: 5.0303 - acc: 0.6038 - f1_score: 0.6003 - val_loss: 5.1694 - val_acc: 0.5427 - val_f1_score: 0.5310\n",
      "Epoch 25/150\n",
      "27/27 - 3s - loss: 5.0137 - acc: 0.6086 - f1_score: 0.6046 - val_loss: 5.1715 - val_acc: 0.5513 - val_f1_score: 0.5404\n",
      "Epoch 26/150\n",
      "27/27 - 3s - loss: 4.9820 - acc: 0.6160 - f1_score: 0.6125 - val_loss: 5.1757 - val_acc: 0.5449 - val_f1_score: 0.5308\n",
      "Epoch 27/150\n",
      "27/27 - 3s - loss: 4.9975 - acc: 0.6239 - f1_score: 0.6208 - val_loss: 5.1876 - val_acc: 0.5256 - val_f1_score: 0.5135\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.51      0.49       193\n",
      "           1       0.63      0.60      0.62       275\n",
      "\n",
      "    accuracy                           0.56       468\n",
      "   macro avg       0.55      0.55      0.55       468\n",
      "weighted avg       0.57      0.56      0.56       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1390\\assets\n",
      "Testing on 1400\n",
      "Epoch 1/150\n",
      "27/27 - 23s - loss: 7.4746 - acc: 0.4922 - f1_score: 0.3658 - val_loss: 5.0826 - val_acc: 0.1474 - val_f1_score: 0.1327\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 5.9912 - acc: 0.5062 - f1_score: 0.4700 - val_loss: 5.0738 - val_acc: 0.1816 - val_f1_score: 0.1759\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6000 - acc: 0.5221 - f1_score: 0.5178 - val_loss: 5.0179 - val_acc: 0.4295 - val_f1_score: 0.4081\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.4958 - acc: 0.5308 - f1_score: 0.5307 - val_loss: 4.9683 - val_acc: 0.5748 - val_f1_score: 0.4832\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4599 - acc: 0.5328 - f1_score: 0.5327 - val_loss: 4.9357 - val_acc: 0.6538 - val_f1_score: 0.5156\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4174 - acc: 0.5267 - f1_score: 0.5261 - val_loss: 4.9266 - val_acc: 0.6538 - val_f1_score: 0.4887\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3776 - acc: 0.5383 - f1_score: 0.5378 - val_loss: 4.9461 - val_acc: 0.6474 - val_f1_score: 0.5019\n",
      "Epoch 8/150\n",
      "27/27 - 6s - loss: 5.3248 - acc: 0.5527 - f1_score: 0.5520 - val_loss: 5.0116 - val_acc: 0.6154 - val_f1_score: 0.4929\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3090 - acc: 0.5472 - f1_score: 0.5463 - val_loss: 5.0905 - val_acc: 0.6239 - val_f1_score: 0.5042\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2974 - acc: 0.5511 - f1_score: 0.5502 - val_loss: 5.1993 - val_acc: 0.5876 - val_f1_score: 0.4920\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2344 - acc: 0.5629 - f1_score: 0.5621 - val_loss: 5.2942 - val_acc: 0.5556 - val_f1_score: 0.4587\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2235 - acc: 0.5529 - f1_score: 0.5523 - val_loss: 5.3536 - val_acc: 0.5470 - val_f1_score: 0.4576\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.18      0.26       153\n",
      "           1       0.69      0.88      0.77       315\n",
      "\n",
      "    accuracy                           0.65       468\n",
      "   macro avg       0.56      0.53      0.52       468\n",
      "weighted avg       0.61      0.65      0.61       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1400\\assets\n",
      "Testing on 1419\n",
      "Epoch 1/150\n",
      "28/28 - 25s - loss: 7.6143 - acc: 0.4795 - f1_score: 0.3538 - val_loss: 5.0349 - val_acc: 0.3121 - val_f1_score: 0.2401\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.1606 - acc: 0.4970 - f1_score: 0.4491 - val_loss: 5.0377 - val_acc: 0.3385 - val_f1_score: 0.2958\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6935 - acc: 0.5088 - f1_score: 0.5016 - val_loss: 5.0098 - val_acc: 0.4220 - val_f1_score: 0.4219\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5533 - acc: 0.5156 - f1_score: 0.5152 - val_loss: 4.9805 - val_acc: 0.5011 - val_f1_score: 0.4638\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5498 - acc: 0.5228 - f1_score: 0.5227 - val_loss: 4.9721 - val_acc: 0.5560 - val_f1_score: 0.4869\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4746 - acc: 0.5296 - f1_score: 0.5289 - val_loss: 4.9779 - val_acc: 0.5868 - val_f1_score: 0.4938\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4651 - acc: 0.5294 - f1_score: 0.5281 - val_loss: 4.9940 - val_acc: 0.5956 - val_f1_score: 0.5091\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.4560 - acc: 0.5267 - f1_score: 0.5252 - val_loss: 5.0211 - val_acc: 0.5560 - val_f1_score: 0.4907\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.4330 - acc: 0.5262 - f1_score: 0.5243 - val_loss: 5.0365 - val_acc: 0.5429 - val_f1_score: 0.4811\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.4077 - acc: 0.5304 - f1_score: 0.5285 - val_loss: 5.0616 - val_acc: 0.5495 - val_f1_score: 0.4945\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.3864 - acc: 0.5262 - f1_score: 0.5240 - val_loss: 5.0950 - val_acc: 0.5407 - val_f1_score: 0.4863\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.3782 - acc: 0.5305 - f1_score: 0.5281 - val_loss: 5.0988 - val_acc: 0.5319 - val_f1_score: 0.4829\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.3796 - acc: 0.5356 - f1_score: 0.5329 - val_loss: 5.1294 - val_acc: 0.5407 - val_f1_score: 0.4911\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.3642 - acc: 0.5369 - f1_score: 0.5342 - val_loss: 5.1518 - val_acc: 0.5275 - val_f1_score: 0.4909\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.33      0.30       121\n",
      "           1       0.74      0.69      0.72       334\n",
      "\n",
      "    accuracy                           0.60       455\n",
      "   macro avg       0.51      0.51      0.51       455\n",
      "weighted avg       0.62      0.60      0.61       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1419\\assets\n",
      "Testing on 1517\n",
      "Epoch 1/150\n",
      "28/28 - 26s - loss: 7.6448 - acc: 0.4514 - f1_score: 0.3454 - val_loss: 4.8955 - val_acc: 0.8917 - val_f1_score: 0.4963\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0716 - acc: 0.4813 - f1_score: 0.4610 - val_loss: 4.9158 - val_acc: 0.7322 - val_f1_score: 0.4933\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6084 - acc: 0.5175 - f1_score: 0.5175 - val_loss: 5.0030 - val_acc: 0.4501 - val_f1_score: 0.3746\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5073 - acc: 0.5272 - f1_score: 0.5248 - val_loss: 5.1082 - val_acc: 0.2849 - val_f1_score: 0.2634\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4787 - acc: 0.5410 - f1_score: 0.5354 - val_loss: 5.1852 - val_acc: 0.2536 - val_f1_score: 0.2387\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4352 - acc: 0.5353 - f1_score: 0.5282 - val_loss: 5.2391 - val_acc: 0.2621 - val_f1_score: 0.2492\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4288 - acc: 0.5375 - f1_score: 0.5299 - val_loss: 5.2723 - val_acc: 0.2821 - val_f1_score: 0.2653\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3303 - acc: 0.5527 - f1_score: 0.5435 - val_loss: 5.2741 - val_acc: 0.3048 - val_f1_score: 0.2832\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       337\n",
      "           1       0.04      0.07      0.05        14\n",
      "\n",
      "    accuracy                           0.89       351\n",
      "   macro avg       0.50      0.50      0.50       351\n",
      "weighted avg       0.92      0.89      0.91       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1517\\assets\n",
      "Testing on 1544\n",
      "Epoch 1/150\n",
      "29/29 - 23s - loss: 7.5707 - acc: 0.4684 - f1_score: 0.3567 - val_loss: 5.0141 - val_acc: 0.3654 - val_f1_score: 0.2676\n",
      "Epoch 2/150\n",
      "29/29 - 3s - loss: 5.9114 - acc: 0.5021 - f1_score: 0.4815 - val_loss: 5.0079 - val_acc: 0.3846 - val_f1_score: 0.3558\n",
      "Epoch 3/150\n",
      "29/29 - 3s - loss: 5.5779 - acc: 0.5170 - f1_score: 0.5167 - val_loss: 4.9832 - val_acc: 0.4712 - val_f1_score: 0.4628\n",
      "Epoch 4/150\n",
      "29/29 - 3s - loss: 5.5131 - acc: 0.5213 - f1_score: 0.5204 - val_loss: 4.9723 - val_acc: 0.5481 - val_f1_score: 0.4975\n",
      "Epoch 5/150\n",
      "29/29 - 3s - loss: 5.4674 - acc: 0.5338 - f1_score: 0.5314 - val_loss: 4.9588 - val_acc: 0.5769 - val_f1_score: 0.4740\n",
      "Epoch 6/150\n",
      "29/29 - 3s - loss: 5.4151 - acc: 0.5349 - f1_score: 0.5320 - val_loss: 4.9542 - val_acc: 0.5673 - val_f1_score: 0.4676\n",
      "Epoch 7/150\n",
      "29/29 - 3s - loss: 5.3976 - acc: 0.5368 - f1_score: 0.5334 - val_loss: 4.9422 - val_acc: 0.6250 - val_f1_score: 0.5386\n",
      "Epoch 8/150\n",
      "29/29 - 5s - loss: 5.3443 - acc: 0.5433 - f1_score: 0.5403 - val_loss: 4.9541 - val_acc: 0.5769 - val_f1_score: 0.5117\n",
      "Epoch 9/150\n",
      "29/29 - 3s - loss: 5.3237 - acc: 0.5442 - f1_score: 0.5409 - val_loss: 4.9814 - val_acc: 0.5769 - val_f1_score: 0.5487\n",
      "Epoch 10/150\n",
      "29/29 - 3s - loss: 5.2946 - acc: 0.5393 - f1_score: 0.5357 - val_loss: 4.9945 - val_acc: 0.5577 - val_f1_score: 0.5407\n",
      "Epoch 11/150\n",
      "29/29 - 3s - loss: 5.2595 - acc: 0.5558 - f1_score: 0.5524 - val_loss: 5.0000 - val_acc: 0.5865 - val_f1_score: 0.5653\n",
      "Epoch 12/150\n",
      "29/29 - 3s - loss: 5.2217 - acc: 0.5643 - f1_score: 0.5599 - val_loss: 5.0321 - val_acc: 0.5673 - val_f1_score: 0.5554\n",
      "Epoch 13/150\n",
      "29/29 - 3s - loss: 5.1933 - acc: 0.5592 - f1_score: 0.5561 - val_loss: 5.0140 - val_acc: 0.5096 - val_f1_score: 0.4992\n",
      "Epoch 14/150\n",
      "29/29 - 3s - loss: 5.1795 - acc: 0.5647 - f1_score: 0.5612 - val_loss: 4.9825 - val_acc: 0.6154 - val_f1_score: 0.5974\n",
      "Epoch 15/150\n",
      "29/29 - 3s - loss: 5.1703 - acc: 0.5674 - f1_score: 0.5642 - val_loss: 4.9649 - val_acc: 0.6442 - val_f1_score: 0.6259\n",
      "Epoch 16/150\n",
      "29/29 - 3s - loss: 5.1360 - acc: 0.5711 - f1_score: 0.5672 - val_loss: 5.0045 - val_acc: 0.6250 - val_f1_score: 0.6020\n",
      "Epoch 17/150\n",
      "29/29 - 3s - loss: 5.1258 - acc: 0.5753 - f1_score: 0.5719 - val_loss: 5.0521 - val_acc: 0.5865 - val_f1_score: 0.5723\n",
      "Epoch 18/150\n",
      "29/29 - 3s - loss: 5.1107 - acc: 0.5828 - f1_score: 0.5791 - val_loss: 5.1184 - val_acc: 0.4808 - val_f1_score: 0.4608\n",
      "Epoch 19/150\n",
      "29/29 - 3s - loss: 5.0837 - acc: 0.5835 - f1_score: 0.5796 - val_loss: 5.1683 - val_acc: 0.4712 - val_f1_score: 0.4599\n",
      "Epoch 20/150\n",
      "29/29 - 3s - loss: 5.0804 - acc: 0.5862 - f1_score: 0.5831 - val_loss: 5.1609 - val_acc: 0.5000 - val_f1_score: 0.4808\n",
      "Epoch 21/150\n",
      "29/29 - 3s - loss: 5.0720 - acc: 0.5870 - f1_score: 0.5844 - val_loss: 5.1239 - val_acc: 0.5481 - val_f1_score: 0.5154\n",
      "Epoch 22/150\n",
      "29/29 - 3s - loss: 5.0459 - acc: 0.5985 - f1_score: 0.5946 - val_loss: 5.0999 - val_acc: 0.5481 - val_f1_score: 0.5100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.52      0.54        42\n",
      "           1       0.69      0.73      0.71        62\n",
      "\n",
      "    accuracy                           0.64       104\n",
      "   macro avg       0.63      0.62      0.63       104\n",
      "weighted avg       0.64      0.64      0.64       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1544\\assets\n",
      "Testing on 1624\n",
      "Epoch 1/150\n",
      "28/28 - 28s - loss: 7.5802 - acc: 0.4632 - f1_score: 0.3532 - val_loss: 4.9411 - val_acc: 0.6581 - val_f1_score: 0.3969\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0297 - acc: 0.4887 - f1_score: 0.4660 - val_loss: 4.9471 - val_acc: 0.6296 - val_f1_score: 0.4489\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.5956 - acc: 0.5218 - f1_score: 0.5215 - val_loss: 4.9753 - val_acc: 0.5100 - val_f1_score: 0.4695\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5141 - acc: 0.5289 - f1_score: 0.5278 - val_loss: 5.0139 - val_acc: 0.4473 - val_f1_score: 0.4471\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4881 - acc: 0.5358 - f1_score: 0.5321 - val_loss: 5.0533 - val_acc: 0.4188 - val_f1_score: 0.4188\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4461 - acc: 0.5358 - f1_score: 0.5317 - val_loss: 5.0902 - val_acc: 0.4188 - val_f1_score: 0.4188\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4008 - acc: 0.5417 - f1_score: 0.5370 - val_loss: 5.1227 - val_acc: 0.4330 - val_f1_score: 0.4326\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3177 - acc: 0.5489 - f1_score: 0.5431 - val_loss: 5.1327 - val_acc: 0.4359 - val_f1_score: 0.4355\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3453 - acc: 0.5510 - f1_score: 0.5461 - val_loss: 5.1554 - val_acc: 0.4672 - val_f1_score: 0.4668\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.2895 - acc: 0.5588 - f1_score: 0.5523 - val_loss: 5.2187 - val_acc: 0.4729 - val_f1_score: 0.4709\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2494 - acc: 0.5679 - f1_score: 0.5629 - val_loss: 5.3012 - val_acc: 0.4131 - val_f1_score: 0.4131\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2417 - acc: 0.5540 - f1_score: 0.5476 - val_loss: 5.3191 - val_acc: 0.4245 - val_f1_score: 0.4239\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.1922 - acc: 0.5667 - f1_score: 0.5605 - val_loss: 5.3718 - val_acc: 0.4416 - val_f1_score: 0.4403\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.1717 - acc: 0.5709 - f1_score: 0.5661 - val_loss: 5.3782 - val_acc: 0.4530 - val_f1_score: 0.4514\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.1447 - acc: 0.5825 - f1_score: 0.5775 - val_loss: 5.4651 - val_acc: 0.4188 - val_f1_score: 0.4184\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.1445 - acc: 0.5826 - f1_score: 0.5758 - val_loss: 5.4686 - val_acc: 0.4387 - val_f1_score: 0.4369\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.1221 - acc: 0.5855 - f1_score: 0.5800 - val_loss: 5.4907 - val_acc: 0.4473 - val_f1_score: 0.4449\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000227B8895A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.68      0.50       139\n",
      "           1       0.62      0.34      0.44       212\n",
      "\n",
      "    accuracy                           0.47       351\n",
      "   macro avg       0.51      0.51      0.47       351\n",
      "weighted avg       0.53      0.47      0.46       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1624\\assets\n",
      "Testing on 1674\n",
      "Epoch 1/150\n",
      "28/28 - 27s - loss: 7.5146 - acc: 0.4896 - f1_score: 0.3654 - val_loss: 5.0762 - val_acc: 0.0646 - val_f1_score: 0.0644\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0156 - acc: 0.4997 - f1_score: 0.4594 - val_loss: 5.0587 - val_acc: 0.2154 - val_f1_score: 0.2007\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6139 - acc: 0.5255 - f1_score: 0.5210 - val_loss: 4.9881 - val_acc: 0.4923 - val_f1_score: 0.3766\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5077 - acc: 0.5336 - f1_score: 0.5336 - val_loss: 4.9272 - val_acc: 0.6523 - val_f1_score: 0.4263\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4932 - acc: 0.5305 - f1_score: 0.5303 - val_loss: 4.8930 - val_acc: 0.6831 - val_f1_score: 0.4483\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4001 - acc: 0.5314 - f1_score: 0.5307 - val_loss: 4.8847 - val_acc: 0.7077 - val_f1_score: 0.4605\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3741 - acc: 0.5395 - f1_score: 0.5389 - val_loss: 4.8744 - val_acc: 0.6954 - val_f1_score: 0.4462\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3516 - acc: 0.5431 - f1_score: 0.5421 - val_loss: 4.8865 - val_acc: 0.6646 - val_f1_score: 0.4319\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3321 - acc: 0.5494 - f1_score: 0.5483 - val_loss: 4.9121 - val_acc: 0.6554 - val_f1_score: 0.4277\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3043 - acc: 0.5443 - f1_score: 0.5428 - val_loss: 4.9265 - val_acc: 0.6708 - val_f1_score: 0.4348\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2659 - acc: 0.5558 - f1_score: 0.5546 - val_loss: 4.9482 - val_acc: 0.6462 - val_f1_score: 0.4007\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2444 - acc: 0.5527 - f1_score: 0.5516 - val_loss: 4.9723 - val_acc: 0.6369 - val_f1_score: 0.3970\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.2072 - acc: 0.5583 - f1_score: 0.5574 - val_loss: 4.9785 - val_acc: 0.5877 - val_f1_score: 0.3836\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022787A973A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.05      0.10        92\n",
      "           1       0.72      0.97      0.83       233\n",
      "\n",
      "    accuracy                           0.71       325\n",
      "   macro avg       0.55      0.51      0.46       325\n",
      "weighted avg       0.63      0.71      0.62       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1674\\assets\n",
      "Testing on 1688\n",
      "Epoch 1/150\n",
      "28/28 - 26s - loss: 7.5018 - acc: 0.4683 - f1_score: 0.3606 - val_loss: 4.9813 - val_acc: 0.5231 - val_f1_score: 0.3434\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 5.9420 - acc: 0.4905 - f1_score: 0.4696 - val_loss: 4.9782 - val_acc: 0.5256 - val_f1_score: 0.3494\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6290 - acc: 0.5122 - f1_score: 0.5118 - val_loss: 4.9795 - val_acc: 0.5205 - val_f1_score: 0.4889\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5438 - acc: 0.5280 - f1_score: 0.5268 - val_loss: 4.9977 - val_acc: 0.4872 - val_f1_score: 0.4747\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4764 - acc: 0.5390 - f1_score: 0.5362 - val_loss: 5.0184 - val_acc: 0.4846 - val_f1_score: 0.4409\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4258 - acc: 0.5343 - f1_score: 0.5314 - val_loss: 5.0314 - val_acc: 0.4718 - val_f1_score: 0.4166\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3785 - acc: 0.5382 - f1_score: 0.5341 - val_loss: 5.0433 - val_acc: 0.4795 - val_f1_score: 0.4354\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3517 - acc: 0.5440 - f1_score: 0.5397 - val_loss: 5.0390 - val_acc: 0.5282 - val_f1_score: 0.5017\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3153 - acc: 0.5482 - f1_score: 0.5441 - val_loss: 5.0445 - val_acc: 0.5205 - val_f1_score: 0.5021\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.2934 - acc: 0.5525 - f1_score: 0.5482 - val_loss: 5.0399 - val_acc: 0.5256 - val_f1_score: 0.5145\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2445 - acc: 0.5544 - f1_score: 0.5499 - val_loss: 5.0357 - val_acc: 0.5256 - val_f1_score: 0.5208\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2404 - acc: 0.5582 - f1_score: 0.5544 - val_loss: 5.0398 - val_acc: 0.5231 - val_f1_score: 0.5202\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.1955 - acc: 0.5631 - f1_score: 0.5584 - val_loss: 5.0465 - val_acc: 0.5154 - val_f1_score: 0.5147\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.1932 - acc: 0.5675 - f1_score: 0.5637 - val_loss: 5.0611 - val_acc: 0.4949 - val_f1_score: 0.4946\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.1661 - acc: 0.5698 - f1_score: 0.5651 - val_loss: 5.0821 - val_acc: 0.4949 - val_f1_score: 0.4941\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.1603 - acc: 0.5834 - f1_score: 0.5796 - val_loss: 5.0838 - val_acc: 0.4923 - val_f1_score: 0.4912\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.1220 - acc: 0.5842 - f1_score: 0.5799 - val_loss: 5.0961 - val_acc: 0.4949 - val_f1_score: 0.4945\n",
      "Epoch 18/150\n",
      "28/28 - 3s - loss: 5.0989 - acc: 0.5906 - f1_score: 0.5861 - val_loss: 5.0984 - val_acc: 0.4897 - val_f1_score: 0.4892\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022789557DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.58      0.47       143\n",
      "           1       0.67      0.49      0.57       247\n",
      "\n",
      "    accuracy                           0.53       390\n",
      "   macro avg       0.53      0.54      0.52       390\n",
      "weighted avg       0.57      0.53      0.53       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1688\\assets\n",
      "Testing on 1717\n",
      "Epoch 1/150\n",
      "28/28 - 35s - loss: 7.4817 - acc: 0.4718 - f1_score: 0.3614 - val_loss: 4.9914 - val_acc: 0.4573 - val_f1_score: 0.3389\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 5.8996 - acc: 0.4987 - f1_score: 0.4757 - val_loss: 4.9908 - val_acc: 0.4786 - val_f1_score: 0.4305\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.5370 - acc: 0.5218 - f1_score: 0.5209 - val_loss: 4.9793 - val_acc: 0.5085 - val_f1_score: 0.5081\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.4865 - acc: 0.5254 - f1_score: 0.5247 - val_loss: 4.9788 - val_acc: 0.5299 - val_f1_score: 0.5158\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4776 - acc: 0.5296 - f1_score: 0.5278 - val_loss: 4.9885 - val_acc: 0.5299 - val_f1_score: 0.5143\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4131 - acc: 0.5345 - f1_score: 0.5317 - val_loss: 5.0129 - val_acc: 0.4915 - val_f1_score: 0.4769\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3730 - acc: 0.5456 - f1_score: 0.5428 - val_loss: 5.0460 - val_acc: 0.4786 - val_f1_score: 0.4613\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3203 - acc: 0.5463 - f1_score: 0.5431 - val_loss: 5.0599 - val_acc: 0.4915 - val_f1_score: 0.4798\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3307 - acc: 0.5505 - f1_score: 0.5470 - val_loss: 5.0764 - val_acc: 0.5085 - val_f1_score: 0.5029\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.2910 - acc: 0.5494 - f1_score: 0.5461 - val_loss: 5.0811 - val_acc: 0.5342 - val_f1_score: 0.5179\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2501 - acc: 0.5501 - f1_score: 0.5472 - val_loss: 5.1044 - val_acc: 0.5256 - val_f1_score: 0.5106\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2197 - acc: 0.5609 - f1_score: 0.5577 - val_loss: 5.0800 - val_acc: 0.5598 - val_f1_score: 0.5429\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.1884 - acc: 0.5644 - f1_score: 0.5604 - val_loss: 5.0984 - val_acc: 0.5427 - val_f1_score: 0.5310\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.1906 - acc: 0.5579 - f1_score: 0.5540 - val_loss: 5.1042 - val_acc: 0.5769 - val_f1_score: 0.5720\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.1554 - acc: 0.5684 - f1_score: 0.5653 - val_loss: 5.0855 - val_acc: 0.5256 - val_f1_score: 0.5148\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.1280 - acc: 0.5809 - f1_score: 0.5781 - val_loss: 5.0720 - val_acc: 0.5812 - val_f1_score: 0.5673\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.1092 - acc: 0.5771 - f1_score: 0.5738 - val_loss: 5.0781 - val_acc: 0.5983 - val_f1_score: 0.5849\n",
      "Epoch 18/150\n",
      "28/28 - 3s - loss: 5.1022 - acc: 0.5787 - f1_score: 0.5758 - val_loss: 5.0930 - val_acc: 0.5897 - val_f1_score: 0.5774\n",
      "Epoch 19/150\n",
      "28/28 - 3s - loss: 5.0774 - acc: 0.5890 - f1_score: 0.5858 - val_loss: 5.1027 - val_acc: 0.5769 - val_f1_score: 0.5661\n",
      "Epoch 20/150\n",
      "28/28 - 3s - loss: 5.0574 - acc: 0.5972 - f1_score: 0.5937 - val_loss: 5.0973 - val_acc: 0.5812 - val_f1_score: 0.5699\n",
      "Epoch 21/150\n",
      "28/28 - 3s - loss: 5.0572 - acc: 0.6003 - f1_score: 0.5962 - val_loss: 5.1692 - val_acc: 0.5598 - val_f1_score: 0.5498\n",
      "Epoch 22/150\n",
      "28/28 - 3s - loss: 5.0418 - acc: 0.5980 - f1_score: 0.5949 - val_loss: 5.1943 - val_acc: 0.5385 - val_f1_score: 0.5273\n",
      "Epoch 23/150\n",
      "28/28 - 3s - loss: 5.0166 - acc: 0.6129 - f1_score: 0.6095 - val_loss: 5.1735 - val_acc: 0.5171 - val_f1_score: 0.5033\n",
      "Epoch 24/150\n",
      "28/28 - 3s - loss: 5.0295 - acc: 0.6070 - f1_score: 0.6038 - val_loss: 5.1514 - val_acc: 0.5385 - val_f1_score: 0.5260\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.56      0.51        88\n",
      "           1       0.70      0.62      0.66       146\n",
      "\n",
      "    accuracy                           0.60       234\n",
      "   macro avg       0.59      0.59      0.58       234\n",
      "weighted avg       0.61      0.60      0.60       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1717\\assets\n",
      "Testing on 1818\n",
      "Epoch 1/150\n",
      "27/27 - 26s - loss: 7.5637 - acc: 0.4720 - f1_score: 0.3466 - val_loss: 5.0090 - val_acc: 0.3718 - val_f1_score: 0.2977\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.1215 - acc: 0.4941 - f1_score: 0.4542 - val_loss: 5.0065 - val_acc: 0.4124 - val_f1_score: 0.3825\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6417 - acc: 0.5177 - f1_score: 0.5124 - val_loss: 4.9867 - val_acc: 0.4850 - val_f1_score: 0.4767\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5117 - acc: 0.5225 - f1_score: 0.5225 - val_loss: 4.9778 - val_acc: 0.5299 - val_f1_score: 0.4761\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4978 - acc: 0.5241 - f1_score: 0.5233 - val_loss: 4.9845 - val_acc: 0.5577 - val_f1_score: 0.4758\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4129 - acc: 0.5390 - f1_score: 0.5372 - val_loss: 4.9998 - val_acc: 0.5214 - val_f1_score: 0.4316\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3742 - acc: 0.5374 - f1_score: 0.5356 - val_loss: 5.0067 - val_acc: 0.5449 - val_f1_score: 0.4444\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3679 - acc: 0.5389 - f1_score: 0.5361 - val_loss: 5.0088 - val_acc: 0.5556 - val_f1_score: 0.4880\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3370 - acc: 0.5456 - f1_score: 0.5432 - val_loss: 5.0115 - val_acc: 0.5363 - val_f1_score: 0.4775\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2877 - acc: 0.5560 - f1_score: 0.5534 - val_loss: 5.0091 - val_acc: 0.5620 - val_f1_score: 0.5209\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2568 - acc: 0.5505 - f1_score: 0.5479 - val_loss: 5.0510 - val_acc: 0.5299 - val_f1_score: 0.4991\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2416 - acc: 0.5590 - f1_score: 0.5571 - val_loss: 5.0589 - val_acc: 0.5491 - val_f1_score: 0.5108\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2008 - acc: 0.5631 - f1_score: 0.5600 - val_loss: 5.0886 - val_acc: 0.5427 - val_f1_score: 0.5094\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.1785 - acc: 0.5663 - f1_score: 0.5636 - val_loss: 5.1208 - val_acc: 0.5363 - val_f1_score: 0.5097\n",
      "Epoch 15/150\n",
      "27/27 - 3s - loss: 5.1704 - acc: 0.5674 - f1_score: 0.5653 - val_loss: 5.1277 - val_acc: 0.5534 - val_f1_score: 0.5258\n",
      "Epoch 16/150\n",
      "27/27 - 3s - loss: 5.1675 - acc: 0.5686 - f1_score: 0.5660 - val_loss: 5.1649 - val_acc: 0.5299 - val_f1_score: 0.5055\n",
      "Epoch 17/150\n",
      "27/27 - 3s - loss: 5.1348 - acc: 0.5740 - f1_score: 0.5716 - val_loss: 5.1627 - val_acc: 0.5150 - val_f1_score: 0.4892\n",
      "Epoch 18/150\n",
      "27/27 - 3s - loss: 5.0956 - acc: 0.5855 - f1_score: 0.5825 - val_loss: 5.1613 - val_acc: 0.5064 - val_f1_score: 0.4919\n",
      "Epoch 19/150\n",
      "27/27 - 3s - loss: 5.1062 - acc: 0.5905 - f1_score: 0.5883 - val_loss: 5.1463 - val_acc: 0.5491 - val_f1_score: 0.5306\n",
      "Epoch 20/150\n",
      "27/27 - 3s - loss: 5.0691 - acc: 0.5915 - f1_score: 0.5887 - val_loss: 5.2097 - val_acc: 0.5427 - val_f1_score: 0.5235\n",
      "Epoch 21/150\n",
      "27/27 - 3s - loss: 5.0515 - acc: 0.6000 - f1_score: 0.5980 - val_loss: 5.2247 - val_acc: 0.4936 - val_f1_score: 0.4787\n",
      "Epoch 22/150\n",
      "27/27 - 3s - loss: 5.0576 - acc: 0.5945 - f1_score: 0.5924 - val_loss: 5.2157 - val_acc: 0.5299 - val_f1_score: 0.5158\n",
      "Epoch 23/150\n",
      "27/27 - 3s - loss: 5.0364 - acc: 0.6065 - f1_score: 0.6040 - val_loss: 5.2174 - val_acc: 0.5043 - val_f1_score: 0.4878\n",
      "Epoch 24/150\n",
      "27/27 - 3s - loss: 5.0121 - acc: 0.6109 - f1_score: 0.6087 - val_loss: 5.2234 - val_acc: 0.5021 - val_f1_score: 0.4817\n",
      "Epoch 25/150\n",
      "27/27 - 3s - loss: 5.0024 - acc: 0.6138 - f1_score: 0.6110 - val_loss: 5.2386 - val_acc: 0.4893 - val_f1_score: 0.4727\n",
      "Epoch 26/150\n",
      "27/27 - 3s - loss: 5.0013 - acc: 0.6190 - f1_score: 0.6168 - val_loss: 5.2402 - val_acc: 0.5107 - val_f1_score: 0.4906\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002280CA7F0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.40      0.44       206\n",
      "           1       0.59      0.67      0.62       262\n",
      "\n",
      "    accuracy                           0.55       468\n",
      "   macro avg       0.54      0.53      0.53       468\n",
      "weighted avg       0.54      0.55      0.54       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1818\\assets\n",
      "Testing on 1892\n",
      "Epoch 1/150\n",
      "27/27 - 31s - loss: 7.8995 - acc: 0.4346 - f1_score: 0.3289 - val_loss: 4.8790 - val_acc: 0.9573 - val_f1_score: 0.4891\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.2578 - acc: 0.4650 - f1_score: 0.4376 - val_loss: 4.9113 - val_acc: 0.7735 - val_f1_score: 0.4540\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6709 - acc: 0.5046 - f1_score: 0.5038 - val_loss: 4.9910 - val_acc: 0.4808 - val_f1_score: 0.3451\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5219 - acc: 0.5173 - f1_score: 0.5145 - val_loss: 5.0792 - val_acc: 0.2949 - val_f1_score: 0.2452\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4906 - acc: 0.5306 - f1_score: 0.5239 - val_loss: 5.1599 - val_acc: 0.2436 - val_f1_score: 0.2118\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4128 - acc: 0.5430 - f1_score: 0.5332 - val_loss: 5.2246 - val_acc: 0.2308 - val_f1_score: 0.2039\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3664 - acc: 0.5456 - f1_score: 0.5349 - val_loss: 5.2652 - val_acc: 0.2479 - val_f1_score: 0.2176\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3657 - acc: 0.5428 - f1_score: 0.5301 - val_loss: 5.2727 - val_acc: 0.3226 - val_f1_score: 0.2706\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       461\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.96       468\n",
      "   macro avg       0.49      0.49      0.49       468\n",
      "weighted avg       0.97      0.96      0.96       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1892\\assets\n",
      "Testing on 1929\n",
      "Epoch 1/150\n",
      "27/27 - 32s - loss: 7.5010 - acc: 0.4817 - f1_score: 0.3506 - val_loss: 5.0438 - val_acc: 0.2479 - val_f1_score: 0.2203\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0846 - acc: 0.4993 - f1_score: 0.4561 - val_loss: 5.0312 - val_acc: 0.3226 - val_f1_score: 0.3179\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6206 - acc: 0.5224 - f1_score: 0.5160 - val_loss: 4.9868 - val_acc: 0.5021 - val_f1_score: 0.4736\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5131 - acc: 0.5260 - f1_score: 0.5259 - val_loss: 4.9449 - val_acc: 0.6132 - val_f1_score: 0.5098\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4848 - acc: 0.5293 - f1_score: 0.5291 - val_loss: 4.9152 - val_acc: 0.6709 - val_f1_score: 0.5453\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4063 - acc: 0.5366 - f1_score: 0.5358 - val_loss: 4.9037 - val_acc: 0.6560 - val_f1_score: 0.5201\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.3784 - acc: 0.5367 - f1_score: 0.5358 - val_loss: 4.9048 - val_acc: 0.6709 - val_f1_score: 0.5425\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3750 - acc: 0.5382 - f1_score: 0.5368 - val_loss: 4.9189 - val_acc: 0.6496 - val_f1_score: 0.5245\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3345 - acc: 0.5518 - f1_score: 0.5505 - val_loss: 4.9406 - val_acc: 0.6303 - val_f1_score: 0.5315\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.2836 - acc: 0.5525 - f1_score: 0.5510 - val_loss: 4.9682 - val_acc: 0.5962 - val_f1_score: 0.5003\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2586 - acc: 0.5511 - f1_score: 0.5498 - val_loss: 4.9823 - val_acc: 0.5833 - val_f1_score: 0.4936\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2345 - acc: 0.5529 - f1_score: 0.5518 - val_loss: 5.0213 - val_acc: 0.5876 - val_f1_score: 0.5169\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.29      0.31       118\n",
      "           1       0.77      0.80      0.78       350\n",
      "\n",
      "    accuracy                           0.67       468\n",
      "   macro avg       0.55      0.54      0.55       468\n",
      "weighted avg       0.66      0.67      0.66       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1929\\assets\n",
      "Testing on 1933\n",
      "Epoch 1/150\n",
      "28/28 - 33s - loss: 7.4399 - acc: 0.4727 - f1_score: 0.3611 - val_loss: 5.0192 - val_acc: 0.3376 - val_f1_score: 0.2745\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 5.8773 - acc: 0.4922 - f1_score: 0.4678 - val_loss: 5.0073 - val_acc: 0.4103 - val_f1_score: 0.3886\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.5134 - acc: 0.5259 - f1_score: 0.5247 - val_loss: 4.9768 - val_acc: 0.4829 - val_f1_score: 0.4665\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.4691 - acc: 0.5253 - f1_score: 0.5248 - val_loss: 4.9617 - val_acc: 0.5855 - val_f1_score: 0.5258\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4720 - acc: 0.5285 - f1_score: 0.5271 - val_loss: 4.9549 - val_acc: 0.6068 - val_f1_score: 0.5232\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4089 - acc: 0.5324 - f1_score: 0.5296 - val_loss: 4.9571 - val_acc: 0.6026 - val_f1_score: 0.5201\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3461 - acc: 0.5467 - f1_score: 0.5445 - val_loss: 4.9702 - val_acc: 0.5641 - val_f1_score: 0.4844\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3171 - acc: 0.5418 - f1_score: 0.5390 - val_loss: 4.9812 - val_acc: 0.5598 - val_f1_score: 0.4929\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3065 - acc: 0.5557 - f1_score: 0.5531 - val_loss: 5.0071 - val_acc: 0.5427 - val_f1_score: 0.4992\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.2964 - acc: 0.5512 - f1_score: 0.5482 - val_loss: 5.0393 - val_acc: 0.5385 - val_f1_score: 0.4986\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2440 - acc: 0.5563 - f1_score: 0.5535 - val_loss: 5.0546 - val_acc: 0.5342 - val_f1_score: 0.5026\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.37      0.36        73\n",
      "           1       0.71      0.68      0.69       161\n",
      "\n",
      "    accuracy                           0.59       234\n",
      "   macro avg       0.53      0.53      0.53       234\n",
      "weighted avg       0.59      0.59      0.59       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-185713\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Ablation ecg\n",
      "Average Accuracy:  0.6111418895422727\n",
      "F1 score for Baseline:  0.459710900070182\n",
      "F1 score for Stress:  0.5568585503639256\n",
      "Macro F1:  0.5082847252170537\n",
      "Weighted F1:  0.5952511933531084\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Training for Modality eda\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Model files saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-192814\n",
      "Tensorboard files for model saved in:  X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\t_b\n",
      "Model report files saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\reports\n",
      "Model weights saved in :  X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_weights\n",
      "Testing on 1105\n",
      "Epoch 1/150\n",
      "27/27 - 56s - loss: 7.6302 - acc: 0.4665 - f1_score: 0.3435 - val_loss: 5.0004 - val_acc: 0.4679 - val_f1_score: 0.4043\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0547 - acc: 0.4753 - f1_score: 0.4408 - val_loss: 5.0138 - val_acc: 0.4188 - val_f1_score: 0.3874\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.7188 - acc: 0.4799 - f1_score: 0.4776 - val_loss: 5.0071 - val_acc: 0.4466 - val_f1_score: 0.4441\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5774 - acc: 0.4976 - f1_score: 0.4975 - val_loss: 4.9956 - val_acc: 0.5534 - val_f1_score: 0.5476\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5371 - acc: 0.5054 - f1_score: 0.5042 - val_loss: 4.9934 - val_acc: 0.5641 - val_f1_score: 0.5080\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4415 - acc: 0.5186 - f1_score: 0.5163 - val_loss: 4.9906 - val_acc: 0.5684 - val_f1_score: 0.4934\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4478 - acc: 0.5217 - f1_score: 0.5184 - val_loss: 4.9868 - val_acc: 0.5897 - val_f1_score: 0.5222\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3802 - acc: 0.5276 - f1_score: 0.5238 - val_loss: 4.9925 - val_acc: 0.5748 - val_f1_score: 0.5193\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3544 - acc: 0.5217 - f1_score: 0.5181 - val_loss: 4.9950 - val_acc: 0.6004 - val_f1_score: 0.5453\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3232 - acc: 0.5390 - f1_score: 0.5353 - val_loss: 5.0078 - val_acc: 0.6068 - val_f1_score: 0.5682\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2984 - acc: 0.5293 - f1_score: 0.5259 - val_loss: 5.0375 - val_acc: 0.5641 - val_f1_score: 0.5300\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2786 - acc: 0.5357 - f1_score: 0.5303 - val_loss: 5.0421 - val_acc: 0.5641 - val_f1_score: 0.5396\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2652 - acc: 0.5401 - f1_score: 0.5357 - val_loss: 5.0536 - val_acc: 0.5491 - val_f1_score: 0.5262\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.2426 - acc: 0.5515 - f1_score: 0.5462 - val_loss: 5.0539 - val_acc: 0.5662 - val_f1_score: 0.5514\n",
      "Epoch 15/150\n",
      "27/27 - 3s - loss: 5.2396 - acc: 0.5444 - f1_score: 0.5399 - val_loss: 5.0705 - val_acc: 0.5705 - val_f1_score: 0.5592\n",
      "Epoch 16/150\n",
      "27/27 - 3s - loss: 5.2178 - acc: 0.5541 - f1_score: 0.5501 - val_loss: 5.1007 - val_acc: 0.5598 - val_f1_score: 0.5444\n",
      "Epoch 17/150\n",
      "27/27 - 3s - loss: 5.2136 - acc: 0.5512 - f1_score: 0.5457 - val_loss: 5.1040 - val_acc: 0.5556 - val_f1_score: 0.5400\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.54      0.44       133\n",
      "           1       0.78      0.63      0.70       335\n",
      "\n",
      "    accuracy                           0.61       468\n",
      "   macro avg       0.57      0.59      0.57       468\n",
      "weighted avg       0.66      0.61      0.62       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1105\\assets\n",
      "Testing on 1106\n",
      "Epoch 1/150\n",
      "27/27 - 23s - loss: 7.4697 - acc: 0.4883 - f1_score: 0.3581 - val_loss: 5.0637 - val_acc: 0.1774 - val_f1_score: 0.1772\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 5.9957 - acc: 0.4814 - f1_score: 0.4418 - val_loss: 5.0619 - val_acc: 0.2671 - val_f1_score: 0.2544\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.7057 - acc: 0.4804 - f1_score: 0.4755 - val_loss: 5.0084 - val_acc: 0.4188 - val_f1_score: 0.3462\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5711 - acc: 0.4933 - f1_score: 0.4929 - val_loss: 4.9415 - val_acc: 0.6111 - val_f1_score: 0.4487\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5176 - acc: 0.5098 - f1_score: 0.5097 - val_loss: 4.8968 - val_acc: 0.7286 - val_f1_score: 0.5189\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4291 - acc: 0.5188 - f1_score: 0.5185 - val_loss: 4.8719 - val_acc: 0.7714 - val_f1_score: 0.5584\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4363 - acc: 0.5186 - f1_score: 0.5182 - val_loss: 4.8544 - val_acc: 0.7521 - val_f1_score: 0.5584\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3894 - acc: 0.5235 - f1_score: 0.5228 - val_loss: 4.8629 - val_acc: 0.7286 - val_f1_score: 0.5415\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3647 - acc: 0.5230 - f1_score: 0.5222 - val_loss: 4.8952 - val_acc: 0.6966 - val_f1_score: 0.5418\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3127 - acc: 0.5325 - f1_score: 0.5313 - val_loss: 4.9445 - val_acc: 0.6474 - val_f1_score: 0.4918\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.3003 - acc: 0.5344 - f1_score: 0.5338 - val_loss: 4.9597 - val_acc: 0.6645 - val_f1_score: 0.5131\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2792 - acc: 0.5350 - f1_score: 0.5337 - val_loss: 5.0111 - val_acc: 0.6175 - val_f1_score: 0.4795\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2770 - acc: 0.5347 - f1_score: 0.5342 - val_loss: 5.0257 - val_acc: 0.6218 - val_f1_score: 0.4822\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.17      0.25       104\n",
      "           1       0.80      0.94      0.87       364\n",
      "\n",
      "    accuracy                           0.77       468\n",
      "   macro avg       0.63      0.56      0.56       468\n",
      "weighted avg       0.72      0.77      0.73       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1106\\assets\n",
      "Testing on 1175\n",
      "Epoch 1/150\n",
      "28/28 - 22s - loss: 7.7074 - acc: 0.4538 - f1_score: 0.3436 - val_loss: 4.9422 - val_acc: 0.7056 - val_f1_score: 0.5261\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0871 - acc: 0.4693 - f1_score: 0.4389 - val_loss: 4.9589 - val_acc: 0.6711 - val_f1_score: 0.5575\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6549 - acc: 0.4953 - f1_score: 0.4937 - val_loss: 5.0100 - val_acc: 0.3873 - val_f1_score: 0.3780\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5401 - acc: 0.5156 - f1_score: 0.5146 - val_loss: 5.0827 - val_acc: 0.3342 - val_f1_score: 0.3319\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5025 - acc: 0.5216 - f1_score: 0.5175 - val_loss: 5.1672 - val_acc: 0.3369 - val_f1_score: 0.3297\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4684 - acc: 0.5240 - f1_score: 0.5177 - val_loss: 5.2355 - val_acc: 0.3714 - val_f1_score: 0.3649\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4230 - acc: 0.5296 - f1_score: 0.5227 - val_loss: 5.3161 - val_acc: 0.3554 - val_f1_score: 0.3508\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3747 - acc: 0.5396 - f1_score: 0.5322 - val_loss: 5.3748 - val_acc: 0.3581 - val_f1_score: 0.3553\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3470 - acc: 0.5409 - f1_score: 0.5325 - val_loss: 5.3649 - val_acc: 0.3687 - val_f1_score: 0.3671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       295\n",
      "           1       0.30      0.38      0.33        82\n",
      "\n",
      "    accuracy                           0.67       377\n",
      "   macro avg       0.56      0.57      0.56       377\n",
      "weighted avg       0.70      0.67      0.68       377\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1175\\assets\n",
      "Testing on 1194\n",
      "Epoch 1/150\n",
      "28/28 - 25s - loss: 7.6513 - acc: 0.4815 - f1_score: 0.3494 - val_loss: 5.0520 - val_acc: 0.2505 - val_f1_score: 0.2435\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.2688 - acc: 0.4818 - f1_score: 0.4261 - val_loss: 5.0511 - val_acc: 0.2923 - val_f1_score: 0.2923\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.7407 - acc: 0.4909 - f1_score: 0.4808 - val_loss: 5.0146 - val_acc: 0.4132 - val_f1_score: 0.3739\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.6307 - acc: 0.4913 - f1_score: 0.4905 - val_loss: 4.9712 - val_acc: 0.5824 - val_f1_score: 0.4039\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5906 - acc: 0.5013 - f1_score: 0.5013 - val_loss: 4.9505 - val_acc: 0.6418 - val_f1_score: 0.4078\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.5275 - acc: 0.5048 - f1_score: 0.5039 - val_loss: 4.9487 - val_acc: 0.6440 - val_f1_score: 0.4087\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.5268 - acc: 0.5061 - f1_score: 0.5053 - val_loss: 4.9366 - val_acc: 0.6615 - val_f1_score: 0.4480\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.4625 - acc: 0.5100 - f1_score: 0.5088 - val_loss: 4.9763 - val_acc: 0.6374 - val_f1_score: 0.4447\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.4697 - acc: 0.5165 - f1_score: 0.5144 - val_loss: 5.0010 - val_acc: 0.6110 - val_f1_score: 0.4428\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.4684 - acc: 0.5094 - f1_score: 0.5077 - val_loss: 5.0512 - val_acc: 0.6110 - val_f1_score: 0.4540\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.4392 - acc: 0.5133 - f1_score: 0.5119 - val_loss: 5.0731 - val_acc: 0.5956 - val_f1_score: 0.4584\n",
      "Epoch 12/150\n",
      "28/28 - 4s - loss: 5.4015 - acc: 0.5132 - f1_score: 0.5114 - val_loss: 5.1042 - val_acc: 0.5758 - val_f1_score: 0.4524\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.3914 - acc: 0.5246 - f1_score: 0.5227 - val_loss: 5.1570 - val_acc: 0.5758 - val_f1_score: 0.4691\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.3782 - acc: 0.5192 - f1_score: 0.5179 - val_loss: 5.2057 - val_acc: 0.5868 - val_f1_score: 0.4962\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.3558 - acc: 0.5292 - f1_score: 0.5271 - val_loss: 5.3043 - val_acc: 0.5780 - val_f1_score: 0.4966\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.3424 - acc: 0.5260 - f1_score: 0.5236 - val_loss: 5.3571 - val_acc: 0.5538 - val_f1_score: 0.4795\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.3159 - acc: 0.5317 - f1_score: 0.5307 - val_loss: 5.3821 - val_acc: 0.5429 - val_f1_score: 0.4717\n",
      "Epoch 18/150\n",
      "28/28 - 3s - loss: 5.3382 - acc: 0.5208 - f1_score: 0.5194 - val_loss: 5.4181 - val_acc: 0.5319 - val_f1_score: 0.4600\n",
      "Epoch 19/150\n",
      "28/28 - 3s - loss: 5.3397 - acc: 0.5191 - f1_score: 0.5173 - val_loss: 5.4349 - val_acc: 0.5319 - val_f1_score: 0.4639\n",
      "Epoch 20/150\n",
      "28/28 - 3s - loss: 5.2917 - acc: 0.5234 - f1_score: 0.5219 - val_loss: 5.4718 - val_acc: 0.5341 - val_f1_score: 0.4674\n",
      "Epoch 21/150\n",
      "28/28 - 3s - loss: 5.2672 - acc: 0.5295 - f1_score: 0.5280 - val_loss: 5.5030 - val_acc: 0.5253 - val_f1_score: 0.4554\n",
      "Epoch 22/150\n",
      "28/28 - 3s - loss: 5.2464 - acc: 0.5370 - f1_score: 0.5353 - val_loss: 5.4791 - val_acc: 0.5165 - val_f1_score: 0.4492\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.22      0.29       181\n",
      "           1       0.61      0.81      0.70       274\n",
      "\n",
      "    accuracy                           0.58       455\n",
      "   macro avg       0.53      0.52      0.50       455\n",
      "weighted avg       0.54      0.58      0.54       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1194\\assets\n",
      "Testing on 1337\n",
      "Epoch 1/150\n",
      "27/27 - 28s - loss: 7.6698 - acc: 0.4495 - f1_score: 0.3406 - val_loss: 4.9513 - val_acc: 0.6709 - val_f1_score: 0.4195\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0663 - acc: 0.4653 - f1_score: 0.4396 - val_loss: 4.9715 - val_acc: 0.5598 - val_f1_score: 0.4164\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6878 - acc: 0.4776 - f1_score: 0.4771 - val_loss: 5.0112 - val_acc: 0.4573 - val_f1_score: 0.4177\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5713 - acc: 0.4947 - f1_score: 0.4932 - val_loss: 5.0679 - val_acc: 0.3291 - val_f1_score: 0.3278\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4907 - acc: 0.5127 - f1_score: 0.5089 - val_loss: 5.1262 - val_acc: 0.3034 - val_f1_score: 0.3013\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4266 - acc: 0.5248 - f1_score: 0.5183 - val_loss: 5.1772 - val_acc: 0.3034 - val_f1_score: 0.2993\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4242 - acc: 0.5219 - f1_score: 0.5148 - val_loss: 5.2282 - val_acc: 0.3248 - val_f1_score: 0.3203\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.4014 - acc: 0.5257 - f1_score: 0.5156 - val_loss: 5.2339 - val_acc: 0.3547 - val_f1_score: 0.3544\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.71      0.80       438\n",
      "           1       0.02      0.10      0.04        30\n",
      "\n",
      "    accuracy                           0.67       468\n",
      "   macro avg       0.47      0.41      0.42       468\n",
      "weighted avg       0.86      0.67      0.75       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1337\\assets\n",
      "Testing on 1390\n",
      "Epoch 1/150\n",
      "27/27 - 24s - loss: 7.6124 - acc: 0.4649 - f1_score: 0.3456 - val_loss: 5.0147 - val_acc: 0.4637 - val_f1_score: 0.3896\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0740 - acc: 0.4796 - f1_score: 0.4486 - val_loss: 5.0292 - val_acc: 0.4145 - val_f1_score: 0.3790\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6762 - acc: 0.4844 - f1_score: 0.4828 - val_loss: 5.0258 - val_acc: 0.4124 - val_f1_score: 0.4107\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5774 - acc: 0.4998 - f1_score: 0.4993 - val_loss: 5.0265 - val_acc: 0.4252 - val_f1_score: 0.4139\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.4953 - acc: 0.5137 - f1_score: 0.5122 - val_loss: 5.0399 - val_acc: 0.5043 - val_f1_score: 0.4405\n",
      "Epoch 6/150\n",
      "27/27 - 4s - loss: 5.4539 - acc: 0.5238 - f1_score: 0.5205 - val_loss: 5.0621 - val_acc: 0.5256 - val_f1_score: 0.4496\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4599 - acc: 0.5202 - f1_score: 0.5167 - val_loss: 5.0783 - val_acc: 0.5470 - val_f1_score: 0.4724\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.4058 - acc: 0.5273 - f1_score: 0.5224 - val_loss: 5.1068 - val_acc: 0.5342 - val_f1_score: 0.4760\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3763 - acc: 0.5351 - f1_score: 0.5307 - val_loss: 5.1490 - val_acc: 0.5363 - val_f1_score: 0.4900\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3283 - acc: 0.5369 - f1_score: 0.5328 - val_loss: 5.1865 - val_acc: 0.5150 - val_f1_score: 0.4764\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.3180 - acc: 0.5344 - f1_score: 0.5300 - val_loss: 5.2157 - val_acc: 0.5150 - val_f1_score: 0.4826\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2858 - acc: 0.5441 - f1_score: 0.5387 - val_loss: 5.2344 - val_acc: 0.5278 - val_f1_score: 0.5057\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2887 - acc: 0.5434 - f1_score: 0.5395 - val_loss: 5.2568 - val_acc: 0.5449 - val_f1_score: 0.5262\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.2427 - acc: 0.5438 - f1_score: 0.5382 - val_loss: 5.2782 - val_acc: 0.5043 - val_f1_score: 0.4901\n",
      "Epoch 15/150\n",
      "27/27 - 3s - loss: 5.2162 - acc: 0.5519 - f1_score: 0.5473 - val_loss: 5.2736 - val_acc: 0.5342 - val_f1_score: 0.5222\n",
      "Epoch 16/150\n",
      "27/27 - 3s - loss: 5.2150 - acc: 0.5544 - f1_score: 0.5499 - val_loss: 5.2889 - val_acc: 0.5256 - val_f1_score: 0.5128\n",
      "Epoch 17/150\n",
      "27/27 - 3s - loss: 5.1858 - acc: 0.5567 - f1_score: 0.5511 - val_loss: 5.3115 - val_acc: 0.5192 - val_f1_score: 0.5139\n",
      "Epoch 18/150\n",
      "27/27 - 3s - loss: 5.1693 - acc: 0.5624 - f1_score: 0.5581 - val_loss: 5.3452 - val_acc: 0.5150 - val_f1_score: 0.5077\n",
      "Epoch 19/150\n",
      "27/27 - 3s - loss: 5.1588 - acc: 0.5689 - f1_score: 0.5641 - val_loss: 5.3555 - val_acc: 0.5064 - val_f1_score: 0.4973\n",
      "Epoch 20/150\n",
      "27/27 - 3s - loss: 5.1486 - acc: 0.5760 - f1_score: 0.5711 - val_loss: 5.3796 - val_acc: 0.4893 - val_f1_score: 0.4773\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.49      0.43       167\n",
      "           1       0.67      0.58      0.62       301\n",
      "\n",
      "    accuracy                           0.54       468\n",
      "   macro avg       0.53      0.53      0.53       468\n",
      "weighted avg       0.57      0.54      0.55       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1390\\assets\n",
      "Testing on 1400\n",
      "Epoch 1/150\n",
      "27/27 - 29s - loss: 7.4848 - acc: 0.4844 - f1_score: 0.3587 - val_loss: 5.0582 - val_acc: 0.1902 - val_f1_score: 0.1888\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0324 - acc: 0.4818 - f1_score: 0.4432 - val_loss: 5.0511 - val_acc: 0.2842 - val_f1_score: 0.2781\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6551 - acc: 0.4879 - f1_score: 0.4828 - val_loss: 4.9955 - val_acc: 0.4786 - val_f1_score: 0.4153\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5643 - acc: 0.4976 - f1_score: 0.4975 - val_loss: 4.9307 - val_acc: 0.6581 - val_f1_score: 0.5154\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5343 - acc: 0.5022 - f1_score: 0.5022 - val_loss: 4.8811 - val_acc: 0.7393 - val_f1_score: 0.5533\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4578 - acc: 0.5173 - f1_score: 0.5165 - val_loss: 4.8694 - val_acc: 0.7415 - val_f1_score: 0.5462\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4397 - acc: 0.5133 - f1_score: 0.5128 - val_loss: 4.8660 - val_acc: 0.7564 - val_f1_score: 0.5615\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3984 - acc: 0.5328 - f1_score: 0.5318 - val_loss: 4.9071 - val_acc: 0.7350 - val_f1_score: 0.5460\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3852 - acc: 0.5308 - f1_score: 0.5291 - val_loss: 4.9574 - val_acc: 0.6752 - val_f1_score: 0.4981\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3517 - acc: 0.5302 - f1_score: 0.5293 - val_loss: 5.0044 - val_acc: 0.6197 - val_f1_score: 0.4644\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.3164 - acc: 0.5357 - f1_score: 0.5345 - val_loss: 5.0397 - val_acc: 0.5919 - val_f1_score: 0.4543\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.3006 - acc: 0.5263 - f1_score: 0.5251 - val_loss: 5.0702 - val_acc: 0.5513 - val_f1_score: 0.4485\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2920 - acc: 0.5308 - f1_score: 0.5297 - val_loss: 5.0844 - val_acc: 0.5791 - val_f1_score: 0.4717\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.2521 - acc: 0.5538 - f1_score: 0.5518 - val_loss: 5.1310 - val_acc: 0.5641 - val_f1_score: 0.4619\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.23      0.27        91\n",
      "           1       0.83      0.88      0.85       377\n",
      "\n",
      "    accuracy                           0.76       468\n",
      "   macro avg       0.57      0.56      0.56       468\n",
      "weighted avg       0.73      0.76      0.74       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1400\\assets\n",
      "Testing on 1419\n",
      "Epoch 1/150\n",
      "28/28 - 31s - loss: 7.6613 - acc: 0.4718 - f1_score: 0.3454 - val_loss: 5.0408 - val_acc: 0.3319 - val_f1_score: 0.2735\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.1956 - acc: 0.4738 - f1_score: 0.4316 - val_loss: 5.0498 - val_acc: 0.3473 - val_f1_score: 0.3314\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.7081 - acc: 0.4887 - f1_score: 0.4827 - val_loss: 5.0216 - val_acc: 0.3890 - val_f1_score: 0.3887\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.6146 - acc: 0.4887 - f1_score: 0.4887 - val_loss: 4.9972 - val_acc: 0.4681 - val_f1_score: 0.4428\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5912 - acc: 0.5038 - f1_score: 0.5031 - val_loss: 4.9846 - val_acc: 0.5626 - val_f1_score: 0.4857\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.5568 - acc: 0.5082 - f1_score: 0.5069 - val_loss: 4.9763 - val_acc: 0.5890 - val_f1_score: 0.4930\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.5154 - acc: 0.4996 - f1_score: 0.4979 - val_loss: 4.9831 - val_acc: 0.5846 - val_f1_score: 0.5172\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.5010 - acc: 0.5077 - f1_score: 0.5059 - val_loss: 4.9853 - val_acc: 0.6044 - val_f1_score: 0.5240\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.4784 - acc: 0.5153 - f1_score: 0.5127 - val_loss: 5.0144 - val_acc: 0.5692 - val_f1_score: 0.5076\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.4486 - acc: 0.5136 - f1_score: 0.5113 - val_loss: 5.0391 - val_acc: 0.5824 - val_f1_score: 0.5227\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.4421 - acc: 0.5165 - f1_score: 0.5147 - val_loss: 5.0555 - val_acc: 0.5868 - val_f1_score: 0.5341\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.4160 - acc: 0.5149 - f1_score: 0.5118 - val_loss: 5.0478 - val_acc: 0.6000 - val_f1_score: 0.5562\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.3882 - acc: 0.5208 - f1_score: 0.5180 - val_loss: 5.1009 - val_acc: 0.5692 - val_f1_score: 0.5341\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.3749 - acc: 0.5236 - f1_score: 0.5206 - val_loss: 5.1082 - val_acc: 0.5604 - val_f1_score: 0.5347\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.3602 - acc: 0.5233 - f1_score: 0.5207 - val_loss: 5.1158 - val_acc: 0.5538 - val_f1_score: 0.5240\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.3503 - acc: 0.5224 - f1_score: 0.5198 - val_loss: 5.1610 - val_acc: 0.5429 - val_f1_score: 0.5139\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.3285 - acc: 0.5215 - f1_score: 0.5199 - val_loss: 5.1617 - val_acc: 0.5604 - val_f1_score: 0.5337\n",
      "Epoch 18/150\n",
      "28/28 - 3s - loss: 5.3343 - acc: 0.5236 - f1_score: 0.5211 - val_loss: 5.1714 - val_acc: 0.5758 - val_f1_score: 0.5505\n",
      "Epoch 19/150\n",
      "28/28 - 3s - loss: 5.3031 - acc: 0.5308 - f1_score: 0.5277 - val_loss: 5.1786 - val_acc: 0.5780 - val_f1_score: 0.5492\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.38      0.42       169\n",
      "           1       0.67      0.73      0.70       286\n",
      "\n",
      "    accuracy                           0.60       455\n",
      "   macro avg       0.56      0.56      0.56       455\n",
      "weighted avg       0.59      0.60      0.59       455\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1419\\assets\n",
      "Testing on 1517\n",
      "Epoch 1/150\n",
      "28/28 - 26s - loss: 7.6002 - acc: 0.4460 - f1_score: 0.3424 - val_loss: 4.8920 - val_acc: 0.8604 - val_f1_score: 0.5167\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0027 - acc: 0.4595 - f1_score: 0.4409 - val_loss: 4.9196 - val_acc: 0.6952 - val_f1_score: 0.5252\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6118 - acc: 0.4890 - f1_score: 0.4890 - val_loss: 5.0044 - val_acc: 0.4929 - val_f1_score: 0.4048\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5455 - acc: 0.4986 - f1_score: 0.4957 - val_loss: 5.1202 - val_acc: 0.3618 - val_f1_score: 0.3259\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4871 - acc: 0.5224 - f1_score: 0.5153 - val_loss: 5.2302 - val_acc: 0.2336 - val_f1_score: 0.2255\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4579 - acc: 0.5235 - f1_score: 0.5149 - val_loss: 5.2993 - val_acc: 0.2393 - val_f1_score: 0.2303\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4548 - acc: 0.5323 - f1_score: 0.5240 - val_loss: 5.3266 - val_acc: 0.2507 - val_f1_score: 0.2398\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3560 - acc: 0.5370 - f1_score: 0.5257 - val_loss: 5.3015 - val_acc: 0.2934 - val_f1_score: 0.2743\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3730 - acc: 0.5316 - f1_score: 0.5203 - val_loss: 5.2740 - val_acc: 0.3447 - val_f1_score: 0.3134\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.96      0.81       236\n",
      "           1       0.65      0.15      0.24       115\n",
      "\n",
      "    accuracy                           0.70       351\n",
      "   macro avg       0.68      0.55      0.53       351\n",
      "weighted avg       0.68      0.70      0.62       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1517\\assets\n",
      "Testing on 1544\n",
      "Epoch 1/150\n",
      "29/29 - 28s - loss: 7.6047 - acc: 0.4646 - f1_score: 0.3495 - val_loss: 5.0337 - val_acc: 0.3654 - val_f1_score: 0.2790\n",
      "Epoch 2/150\n",
      "29/29 - 5s - loss: 5.9957 - acc: 0.4767 - f1_score: 0.4531 - val_loss: 5.0432 - val_acc: 0.3846 - val_f1_score: 0.3610\n",
      "Epoch 3/150\n",
      "29/29 - 3s - loss: 5.6458 - acc: 0.4984 - f1_score: 0.4977 - val_loss: 5.0246 - val_acc: 0.3654 - val_f1_score: 0.3654\n",
      "Epoch 4/150\n",
      "29/29 - 3s - loss: 5.5567 - acc: 0.5068 - f1_score: 0.5059 - val_loss: 5.0065 - val_acc: 0.3942 - val_f1_score: 0.3685\n",
      "Epoch 5/150\n",
      "29/29 - 3s - loss: 5.5106 - acc: 0.5126 - f1_score: 0.5099 - val_loss: 4.9833 - val_acc: 0.5481 - val_f1_score: 0.4649\n",
      "Epoch 6/150\n",
      "29/29 - 3s - loss: 5.5071 - acc: 0.5173 - f1_score: 0.5140 - val_loss: 4.9610 - val_acc: 0.5769 - val_f1_score: 0.4945\n",
      "Epoch 7/150\n",
      "29/29 - 3s - loss: 5.4106 - acc: 0.5174 - f1_score: 0.5134 - val_loss: 4.9485 - val_acc: 0.6538 - val_f1_score: 0.6067\n",
      "Epoch 8/150\n",
      "29/29 - 3s - loss: 5.3900 - acc: 0.5229 - f1_score: 0.5196 - val_loss: 4.9589 - val_acc: 0.6250 - val_f1_score: 0.5636\n",
      "Epoch 9/150\n",
      "29/29 - 3s - loss: 5.3443 - acc: 0.5330 - f1_score: 0.5283 - val_loss: 4.9546 - val_acc: 0.6346 - val_f1_score: 0.5783\n",
      "Epoch 10/150\n",
      "29/29 - 3s - loss: 5.3221 - acc: 0.5331 - f1_score: 0.5286 - val_loss: 4.9676 - val_acc: 0.5962 - val_f1_score: 0.5478\n",
      "Epoch 11/150\n",
      "29/29 - 3s - loss: 5.3114 - acc: 0.5401 - f1_score: 0.5362 - val_loss: 5.0308 - val_acc: 0.5385 - val_f1_score: 0.4902\n",
      "Epoch 12/150\n",
      "29/29 - 3s - loss: 5.2721 - acc: 0.5409 - f1_score: 0.5356 - val_loss: 5.0624 - val_acc: 0.5288 - val_f1_score: 0.4829\n",
      "Epoch 13/150\n",
      "29/29 - 3s - loss: 5.2506 - acc: 0.5382 - f1_score: 0.5325 - val_loss: 5.0862 - val_acc: 0.5385 - val_f1_score: 0.5024\n",
      "Epoch 14/150\n",
      "29/29 - 3s - loss: 5.2511 - acc: 0.5427 - f1_score: 0.5375 - val_loss: 5.1006 - val_acc: 0.5769 - val_f1_score: 0.5385\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.55      0.47        29\n",
      "           1       0.80      0.69      0.74        75\n",
      "\n",
      "    accuracy                           0.65       104\n",
      "   macro avg       0.61      0.62      0.61       104\n",
      "weighted avg       0.69      0.65      0.67       104\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1544\\assets\n",
      "Testing on 1624\n",
      "Epoch 1/150\n",
      "28/28 - 28s - loss: 7.5313 - acc: 0.4605 - f1_score: 0.3513 - val_loss: 4.9589 - val_acc: 0.6068 - val_f1_score: 0.5286\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 5.9689 - acc: 0.4678 - f1_score: 0.4454 - val_loss: 4.9810 - val_acc: 0.5157 - val_f1_score: 0.4870\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6034 - acc: 0.4946 - f1_score: 0.4944 - val_loss: 5.0199 - val_acc: 0.4473 - val_f1_score: 0.4435\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5292 - acc: 0.5063 - f1_score: 0.5048 - val_loss: 5.0743 - val_acc: 0.3989 - val_f1_score: 0.3950\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4767 - acc: 0.5172 - f1_score: 0.5131 - val_loss: 5.1390 - val_acc: 0.3447 - val_f1_score: 0.3270\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4397 - acc: 0.5177 - f1_score: 0.5120 - val_loss: 5.1962 - val_acc: 0.3390 - val_f1_score: 0.3211\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4568 - acc: 0.5271 - f1_score: 0.5218 - val_loss: 5.2589 - val_acc: 0.3590 - val_f1_score: 0.3446\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3556 - acc: 0.5409 - f1_score: 0.5334 - val_loss: 5.3034 - val_acc: 0.3704 - val_f1_score: 0.3551\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000227D03709D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       260\n",
      "           1       0.30      0.38      0.34        91\n",
      "\n",
      "    accuracy                           0.61       351\n",
      "   macro avg       0.53      0.53      0.53       351\n",
      "weighted avg       0.64      0.61      0.62       351\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1624\\assets\n",
      "Testing on 1674\n",
      "Epoch 1/150\n",
      "28/28 - 26s - loss: 7.4471 - acc: 0.4844 - f1_score: 0.3587 - val_loss: 5.0762 - val_acc: 0.0954 - val_f1_score: 0.0952\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0034 - acc: 0.4837 - f1_score: 0.4462 - val_loss: 5.0675 - val_acc: 0.2462 - val_f1_score: 0.2095\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6516 - acc: 0.4969 - f1_score: 0.4937 - val_loss: 5.0010 - val_acc: 0.4646 - val_f1_score: 0.3316\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5219 - acc: 0.5050 - f1_score: 0.5048 - val_loss: 4.9318 - val_acc: 0.6246 - val_f1_score: 0.3995\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.4883 - acc: 0.5101 - f1_score: 0.5097 - val_loss: 4.8901 - val_acc: 0.7385 - val_f1_score: 0.4467\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4586 - acc: 0.5162 - f1_score: 0.5154 - val_loss: 4.8670 - val_acc: 0.7662 - val_f1_score: 0.4464\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.3968 - acc: 0.5325 - f1_score: 0.5315 - val_loss: 4.8787 - val_acc: 0.7323 - val_f1_score: 0.4637\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3639 - acc: 0.5336 - f1_score: 0.5321 - val_loss: 4.9162 - val_acc: 0.6708 - val_f1_score: 0.4188\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3459 - acc: 0.5307 - f1_score: 0.5295 - val_loss: 4.9165 - val_acc: 0.6492 - val_f1_score: 0.4320\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3242 - acc: 0.5397 - f1_score: 0.5380 - val_loss: 4.9648 - val_acc: 0.6123 - val_f1_score: 0.4204\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.2797 - acc: 0.5450 - f1_score: 0.5433 - val_loss: 5.0129 - val_acc: 0.6123 - val_f1_score: 0.4323\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2772 - acc: 0.5404 - f1_score: 0.5385 - val_loss: 5.0736 - val_acc: 0.5815 - val_f1_score: 0.4107\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.2863 - acc: 0.5448 - f1_score: 0.5432 - val_loss: 5.1168 - val_acc: 0.5385 - val_f1_score: 0.3884\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.2630 - acc: 0.5528 - f1_score: 0.5506 - val_loss: 5.1403 - val_acc: 0.5231 - val_f1_score: 0.3850\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022510AC5E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.05      0.08        82\n",
      "           1       0.75      0.96      0.84       243\n",
      "\n",
      "    accuracy                           0.73       325\n",
      "   macro avg       0.53      0.51      0.46       325\n",
      "weighted avg       0.64      0.73      0.65       325\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1674\\assets\n",
      "Testing on 1688\n",
      "Epoch 1/150\n",
      "28/28 - 25s - loss: 7.5916 - acc: 0.4653 - f1_score: 0.3507 - val_loss: 5.0242 - val_acc: 0.4795 - val_f1_score: 0.4062\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.1226 - acc: 0.4744 - f1_score: 0.4448 - val_loss: 5.0483 - val_acc: 0.3641 - val_f1_score: 0.3300\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.7043 - acc: 0.4859 - f1_score: 0.4850 - val_loss: 5.0754 - val_acc: 0.2590 - val_f1_score: 0.2564\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5923 - acc: 0.5041 - f1_score: 0.5038 - val_loss: 5.1066 - val_acc: 0.2974 - val_f1_score: 0.2943\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5434 - acc: 0.5127 - f1_score: 0.5109 - val_loss: 5.1426 - val_acc: 0.4103 - val_f1_score: 0.3786\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4842 - acc: 0.5186 - f1_score: 0.5151 - val_loss: 5.1705 - val_acc: 0.4333 - val_f1_score: 0.3992\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4413 - acc: 0.5227 - f1_score: 0.5181 - val_loss: 5.1860 - val_acc: 0.4513 - val_f1_score: 0.4204\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.4152 - acc: 0.5236 - f1_score: 0.5187 - val_loss: 5.2240 - val_acc: 0.4590 - val_f1_score: 0.4382\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3703 - acc: 0.5259 - f1_score: 0.5198 - val_loss: 5.2278 - val_acc: 0.4462 - val_f1_score: 0.4336\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3430 - acc: 0.5396 - f1_score: 0.5348 - val_loss: 5.2530 - val_acc: 0.4692 - val_f1_score: 0.4639\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.3311 - acc: 0.5409 - f1_score: 0.5357 - val_loss: 5.2813 - val_acc: 0.4154 - val_f1_score: 0.4132\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.3150 - acc: 0.5293 - f1_score: 0.5242 - val_loss: 5.3075 - val_acc: 0.4538 - val_f1_score: 0.4499\n",
      "Epoch 13/150\n",
      "28/28 - 3s - loss: 5.2603 - acc: 0.5492 - f1_score: 0.5419 - val_loss: 5.3149 - val_acc: 0.4718 - val_f1_score: 0.4687\n",
      "Epoch 14/150\n",
      "28/28 - 3s - loss: 5.2421 - acc: 0.5504 - f1_score: 0.5448 - val_loss: 5.3391 - val_acc: 0.4974 - val_f1_score: 0.4944\n",
      "Epoch 15/150\n",
      "28/28 - 3s - loss: 5.2391 - acc: 0.5522 - f1_score: 0.5461 - val_loss: 5.3931 - val_acc: 0.4846 - val_f1_score: 0.4836\n",
      "Epoch 16/150\n",
      "28/28 - 3s - loss: 5.2214 - acc: 0.5488 - f1_score: 0.5426 - val_loss: 5.4308 - val_acc: 0.4846 - val_f1_score: 0.4834\n",
      "Epoch 17/150\n",
      "28/28 - 3s - loss: 5.1976 - acc: 0.5598 - f1_score: 0.5529 - val_loss: 5.4089 - val_acc: 0.5026 - val_f1_score: 0.5007\n",
      "Epoch 18/150\n",
      "28/28 - 3s - loss: 5.1883 - acc: 0.5607 - f1_score: 0.5547 - val_loss: 5.4130 - val_acc: 0.5026 - val_f1_score: 0.5003\n",
      "Epoch 19/150\n",
      "28/28 - 3s - loss: 5.1880 - acc: 0.5595 - f1_score: 0.5530 - val_loss: 5.3866 - val_acc: 0.5231 - val_f1_score: 0.5198\n",
      "Epoch 20/150\n",
      "28/28 - 3s - loss: 5.1567 - acc: 0.5681 - f1_score: 0.5622 - val_loss: 5.3640 - val_acc: 0.5282 - val_f1_score: 0.5250\n",
      "Epoch 21/150\n",
      "28/28 - 3s - loss: 5.1565 - acc: 0.5690 - f1_score: 0.5629 - val_loss: 5.3406 - val_acc: 0.5410 - val_f1_score: 0.5381\n",
      "Epoch 22/150\n",
      "28/28 - 3s - loss: 5.1245 - acc: 0.5748 - f1_score: 0.5689 - val_loss: 5.3425 - val_acc: 0.5333 - val_f1_score: 0.5302\n",
      "Epoch 23/150\n",
      "28/28 - 3s - loss: 5.1338 - acc: 0.5673 - f1_score: 0.5609 - val_loss: 5.3116 - val_acc: 0.5179 - val_f1_score: 0.5117\n",
      "Epoch 24/150\n",
      "28/28 - 3s - loss: 5.1221 - acc: 0.5675 - f1_score: 0.5608 - val_loss: 5.3471 - val_acc: 0.5333 - val_f1_score: 0.5309\n",
      "Epoch 25/150\n",
      "28/28 - 3s - loss: 5.1079 - acc: 0.5677 - f1_score: 0.5615 - val_loss: 5.3442 - val_acc: 0.5410 - val_f1_score: 0.5381\n",
      "Epoch 26/150\n",
      "28/28 - 3s - loss: 5.1067 - acc: 0.5803 - f1_score: 0.5732 - val_loss: 5.3465 - val_acc: 0.5359 - val_f1_score: 0.5340\n",
      "Epoch 27/150\n",
      "28/28 - 3s - loss: 5.0932 - acc: 0.5857 - f1_score: 0.5789 - val_loss: 5.3239 - val_acc: 0.5308 - val_f1_score: 0.5278\n",
      "Epoch 28/150\n",
      "28/28 - 3s - loss: 5.0800 - acc: 0.5864 - f1_score: 0.5793 - val_loss: 5.3698 - val_acc: 0.5333 - val_f1_score: 0.5316\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002280D2C0940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50       151\n",
      "           1       0.66      0.51      0.57       239\n",
      "\n",
      "    accuracy                           0.54       390\n",
      "   macro avg       0.55      0.55      0.54       390\n",
      "weighted avg       0.57      0.54      0.55       390\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1688\\assets\n",
      "Testing on 1717\n",
      "Epoch 1/150\n",
      "28/28 - 32s - loss: 7.6153 - acc: 0.4704 - f1_score: 0.3540 - val_loss: 5.0152 - val_acc: 0.4487 - val_f1_score: 0.3818\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0549 - acc: 0.4778 - f1_score: 0.4488 - val_loss: 5.0145 - val_acc: 0.4872 - val_f1_score: 0.4817\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6396 - acc: 0.5055 - f1_score: 0.5046 - val_loss: 5.0182 - val_acc: 0.3932 - val_f1_score: 0.3688\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5640 - acc: 0.5025 - f1_score: 0.5018 - val_loss: 5.0303 - val_acc: 0.4017 - val_f1_score: 0.3654\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5069 - acc: 0.5053 - f1_score: 0.5033 - val_loss: 5.0639 - val_acc: 0.4359 - val_f1_score: 0.3903\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4466 - acc: 0.5193 - f1_score: 0.5159 - val_loss: 5.1125 - val_acc: 0.4359 - val_f1_score: 0.3839\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4422 - acc: 0.5230 - f1_score: 0.5195 - val_loss: 5.1769 - val_acc: 0.4316 - val_f1_score: 0.3809\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3515 - acc: 0.5264 - f1_score: 0.5220 - val_loss: 5.2672 - val_acc: 0.4060 - val_f1_score: 0.3657\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3354 - acc: 0.5463 - f1_score: 0.5426 - val_loss: 5.3555 - val_acc: 0.3932 - val_f1_score: 0.3534\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.45      0.53       154\n",
      "           1       0.35      0.56      0.43        80\n",
      "\n",
      "    accuracy                           0.49       234\n",
      "   macro avg       0.50      0.51      0.48       234\n",
      "weighted avg       0.55      0.49      0.50       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1717\\assets\n",
      "Testing on 1818\n",
      "Epoch 1/150\n",
      "27/27 - 25s - loss: 7.6104 - acc: 0.4699 - f1_score: 0.3465 - val_loss: 5.0218 - val_acc: 0.4402 - val_f1_score: 0.4035\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0884 - acc: 0.4756 - f1_score: 0.4385 - val_loss: 5.0272 - val_acc: 0.4274 - val_f1_score: 0.4235\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6869 - acc: 0.4959 - f1_score: 0.4921 - val_loss: 5.0132 - val_acc: 0.4466 - val_f1_score: 0.4431\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5780 - acc: 0.4949 - f1_score: 0.4948 - val_loss: 5.0021 - val_acc: 0.5043 - val_f1_score: 0.4753\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5218 - acc: 0.5098 - f1_score: 0.5084 - val_loss: 5.0044 - val_acc: 0.5385 - val_f1_score: 0.4249\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4796 - acc: 0.5206 - f1_score: 0.5186 - val_loss: 5.0102 - val_acc: 0.5684 - val_f1_score: 0.4459\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4187 - acc: 0.5175 - f1_score: 0.5154 - val_loss: 5.0166 - val_acc: 0.5769 - val_f1_score: 0.4422\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3899 - acc: 0.5233 - f1_score: 0.5205 - val_loss: 5.0234 - val_acc: 0.5855 - val_f1_score: 0.4759\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3752 - acc: 0.5327 - f1_score: 0.5293 - val_loss: 5.0474 - val_acc: 0.5641 - val_f1_score: 0.4884\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3348 - acc: 0.5470 - f1_score: 0.5441 - val_loss: 5.0775 - val_acc: 0.5598 - val_f1_score: 0.4965\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.3189 - acc: 0.5266 - f1_score: 0.5226 - val_loss: 5.0934 - val_acc: 0.5363 - val_f1_score: 0.4808\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.3008 - acc: 0.5388 - f1_score: 0.5343 - val_loss: 5.1137 - val_acc: 0.5363 - val_f1_score: 0.4942\n",
      "Epoch 13/150\n",
      "27/27 - 3s - loss: 5.2806 - acc: 0.5332 - f1_score: 0.5296 - val_loss: 5.1554 - val_acc: 0.5256 - val_f1_score: 0.4934\n",
      "Epoch 14/150\n",
      "27/27 - 3s - loss: 5.2704 - acc: 0.5547 - f1_score: 0.5507 - val_loss: 5.1669 - val_acc: 0.5256 - val_f1_score: 0.5020\n",
      "Epoch 15/150\n",
      "27/27 - 3s - loss: 5.2113 - acc: 0.5543 - f1_score: 0.5499 - val_loss: 5.1791 - val_acc: 0.5235 - val_f1_score: 0.5039\n",
      "Epoch 16/150\n",
      "27/27 - 3s - loss: 5.1971 - acc: 0.5524 - f1_score: 0.5483 - val_loss: 5.2215 - val_acc: 0.5299 - val_f1_score: 0.5127\n",
      "Epoch 17/150\n",
      "27/27 - 3s - loss: 5.1786 - acc: 0.5629 - f1_score: 0.5594 - val_loss: 5.2250 - val_acc: 0.5256 - val_f1_score: 0.5091\n",
      "Epoch 18/150\n",
      "27/27 - 3s - loss: 5.1749 - acc: 0.5651 - f1_score: 0.5613 - val_loss: 5.2426 - val_acc: 0.5406 - val_f1_score: 0.5257\n",
      "Epoch 19/150\n",
      "27/27 - 3s - loss: 5.1473 - acc: 0.5683 - f1_score: 0.5647 - val_loss: 5.2501 - val_acc: 0.5342 - val_f1_score: 0.5163\n",
      "Epoch 20/150\n",
      "27/27 - 3s - loss: 5.1438 - acc: 0.5674 - f1_score: 0.5630 - val_loss: 5.2547 - val_acc: 0.5342 - val_f1_score: 0.5202\n",
      "Epoch 21/150\n",
      "27/27 - 3s - loss: 5.1053 - acc: 0.5815 - f1_score: 0.5773 - val_loss: 5.2530 - val_acc: 0.5192 - val_f1_score: 0.5044\n",
      "Epoch 22/150\n",
      "27/27 - 3s - loss: 5.1257 - acc: 0.5683 - f1_score: 0.5645 - val_loss: 5.2511 - val_acc: 0.5128 - val_f1_score: 0.4966\n",
      "Epoch 23/150\n",
      "27/27 - 3s - loss: 5.1104 - acc: 0.5728 - f1_score: 0.5689 - val_loss: 5.2584 - val_acc: 0.5171 - val_f1_score: 0.5054\n",
      "Epoch 24/150\n",
      "27/27 - 3s - loss: 5.0888 - acc: 0.5882 - f1_score: 0.5849 - val_loss: 5.2470 - val_acc: 0.5192 - val_f1_score: 0.5066\n",
      "Epoch 25/150\n",
      "27/27 - 3s - loss: 5.0913 - acc: 0.5841 - f1_score: 0.5796 - val_loss: 5.2464 - val_acc: 0.5128 - val_f1_score: 0.5010\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002250B694A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.39      0.44       216\n",
      "           1       0.56      0.67      0.61       252\n",
      "\n",
      "    accuracy                           0.54       468\n",
      "   macro avg       0.53      0.53      0.53       468\n",
      "weighted avg       0.53      0.54      0.53       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1818\\assets\n",
      "Testing on 1892\n",
      "Epoch 1/150\n",
      "27/27 - 30s - loss: 7.7632 - acc: 0.4343 - f1_score: 0.3307 - val_loss: 4.8898 - val_acc: 0.8825 - val_f1_score: 0.4688\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.1219 - acc: 0.4572 - f1_score: 0.4351 - val_loss: 4.9136 - val_acc: 0.7585 - val_f1_score: 0.4313\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6818 - acc: 0.4825 - f1_score: 0.4823 - val_loss: 5.0019 - val_acc: 0.4893 - val_f1_score: 0.3393\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5864 - acc: 0.4998 - f1_score: 0.4965 - val_loss: 5.1238 - val_acc: 0.2350 - val_f1_score: 0.1969\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5030 - acc: 0.5102 - f1_score: 0.5023 - val_loss: 5.2309 - val_acc: 0.1517 - val_f1_score: 0.1380\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4648 - acc: 0.5288 - f1_score: 0.5183 - val_loss: 5.3165 - val_acc: 0.1410 - val_f1_score: 0.1303\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4081 - acc: 0.5350 - f1_score: 0.5230 - val_loss: 5.4045 - val_acc: 0.1474 - val_f1_score: 0.1355\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3889 - acc: 0.5398 - f1_score: 0.5260 - val_loss: 5.4593 - val_acc: 0.1624 - val_f1_score: 0.1493\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       426\n",
      "           1       0.00      0.00      0.00        42\n",
      "\n",
      "    accuracy                           0.88       468\n",
      "   macro avg       0.45      0.48      0.47       468\n",
      "weighted avg       0.83      0.88      0.85       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1892\\assets\n",
      "Testing on 1929\n",
      "Epoch 1/150\n",
      "27/27 - 27s - loss: 7.5346 - acc: 0.4782 - f1_score: 0.3505 - val_loss: 5.0929 - val_acc: 0.2179 - val_f1_score: 0.1805\n",
      "Epoch 2/150\n",
      "27/27 - 3s - loss: 6.0583 - acc: 0.4820 - f1_score: 0.4418 - val_loss: 5.1063 - val_acc: 0.2436 - val_f1_score: 0.2273\n",
      "Epoch 3/150\n",
      "27/27 - 3s - loss: 5.6641 - acc: 0.4873 - f1_score: 0.4828 - val_loss: 5.0623 - val_acc: 0.3056 - val_f1_score: 0.3046\n",
      "Epoch 4/150\n",
      "27/27 - 3s - loss: 5.5958 - acc: 0.4991 - f1_score: 0.4990 - val_loss: 4.9999 - val_acc: 0.4017 - val_f1_score: 0.3948\n",
      "Epoch 5/150\n",
      "27/27 - 3s - loss: 5.5243 - acc: 0.4953 - f1_score: 0.4949 - val_loss: 4.9617 - val_acc: 0.5684 - val_f1_score: 0.4810\n",
      "Epoch 6/150\n",
      "27/27 - 3s - loss: 5.4691 - acc: 0.5164 - f1_score: 0.5156 - val_loss: 4.9623 - val_acc: 0.6132 - val_f1_score: 0.4571\n",
      "Epoch 7/150\n",
      "27/27 - 3s - loss: 5.4220 - acc: 0.5157 - f1_score: 0.5149 - val_loss: 4.9708 - val_acc: 0.6346 - val_f1_score: 0.4697\n",
      "Epoch 8/150\n",
      "27/27 - 3s - loss: 5.3970 - acc: 0.5189 - f1_score: 0.5177 - val_loss: 5.0050 - val_acc: 0.6068 - val_f1_score: 0.4601\n",
      "Epoch 9/150\n",
      "27/27 - 3s - loss: 5.3530 - acc: 0.5379 - f1_score: 0.5360 - val_loss: 5.0621 - val_acc: 0.5534 - val_f1_score: 0.4253\n",
      "Epoch 10/150\n",
      "27/27 - 3s - loss: 5.3367 - acc: 0.5234 - f1_score: 0.5219 - val_loss: 5.1080 - val_acc: 0.5214 - val_f1_score: 0.4036\n",
      "Epoch 11/150\n",
      "27/27 - 3s - loss: 5.2847 - acc: 0.5317 - f1_score: 0.5302 - val_loss: 5.1614 - val_acc: 0.5513 - val_f1_score: 0.4435\n",
      "Epoch 12/150\n",
      "27/27 - 3s - loss: 5.2800 - acc: 0.5330 - f1_score: 0.5305 - val_loss: 5.2130 - val_acc: 0.5513 - val_f1_score: 0.4627\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.22      0.27       172\n",
      "           1       0.63      0.77      0.69       296\n",
      "\n",
      "    accuracy                           0.57       468\n",
      "   macro avg       0.49      0.49      0.48       468\n",
      "weighted avg       0.53      0.57      0.54       468\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1929\\assets\n",
      "Testing on 1933\n",
      "Epoch 1/150\n",
      "28/28 - 27s - loss: 7.6473 - acc: 0.4662 - f1_score: 0.3431 - val_loss: 5.0304 - val_acc: 0.3333 - val_f1_score: 0.2547\n",
      "Epoch 2/150\n",
      "28/28 - 3s - loss: 6.0901 - acc: 0.4813 - f1_score: 0.4470 - val_loss: 5.0269 - val_acc: 0.3248 - val_f1_score: 0.2868\n",
      "Epoch 3/150\n",
      "28/28 - 3s - loss: 5.6825 - acc: 0.4928 - f1_score: 0.4911 - val_loss: 4.9957 - val_acc: 0.4744 - val_f1_score: 0.4743\n",
      "Epoch 4/150\n",
      "28/28 - 3s - loss: 5.5761 - acc: 0.4980 - f1_score: 0.4975 - val_loss: 4.9687 - val_acc: 0.5983 - val_f1_score: 0.5612\n",
      "Epoch 5/150\n",
      "28/28 - 3s - loss: 5.5164 - acc: 0.5050 - f1_score: 0.5034 - val_loss: 4.9520 - val_acc: 0.6667 - val_f1_score: 0.5629\n",
      "Epoch 6/150\n",
      "28/28 - 3s - loss: 5.4760 - acc: 0.5149 - f1_score: 0.5122 - val_loss: 4.9458 - val_acc: 0.6368 - val_f1_score: 0.5157\n",
      "Epoch 7/150\n",
      "28/28 - 3s - loss: 5.4350 - acc: 0.5193 - f1_score: 0.5161 - val_loss: 4.9559 - val_acc: 0.6111 - val_f1_score: 0.4873\n",
      "Epoch 8/150\n",
      "28/28 - 3s - loss: 5.3927 - acc: 0.5260 - f1_score: 0.5223 - val_loss: 4.9736 - val_acc: 0.6282 - val_f1_score: 0.5548\n",
      "Epoch 9/150\n",
      "28/28 - 3s - loss: 5.3697 - acc: 0.5337 - f1_score: 0.5305 - val_loss: 5.0080 - val_acc: 0.6197 - val_f1_score: 0.5519\n",
      "Epoch 10/150\n",
      "28/28 - 3s - loss: 5.3547 - acc: 0.5338 - f1_score: 0.5300 - val_loss: 5.0096 - val_acc: 0.6068 - val_f1_score: 0.5548\n",
      "Epoch 11/150\n",
      "28/28 - 3s - loss: 5.3134 - acc: 0.5312 - f1_score: 0.5271 - val_loss: 4.9991 - val_acc: 0.6068 - val_f1_score: 0.5487\n",
      "Epoch 12/150\n",
      "28/28 - 3s - loss: 5.2687 - acc: 0.5446 - f1_score: 0.5400 - val_loss: 5.0343 - val_acc: 0.5983 - val_f1_score: 0.5389\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.50      0.35        42\n",
      "           1       0.87      0.70      0.78       192\n",
      "\n",
      "    accuracy                           0.67       234\n",
      "   macro avg       0.57      0.60      0.56       234\n",
      "weighted avg       0.76      0.67      0.70       234\n",
      "\n",
      "INFO:tensorflow:Assets written to: X:/Data Files/TAFFC/Cola/experiment_20220607-192814\\model_arch\\model_1933\\assets\n",
      "--------------------------------------------------------------------------\n",
      "Classfication report for Ablation eda\n",
      "Average Accuracy:  0.6430005442265979\n",
      "F1 score for Baseline:  0.4891256532741513\n",
      "F1 score for Stress:  0.5582732534643075\n",
      "Macro F1:  0.5236994533692293\n",
      "Weighted F1:  0.635721924753293\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "# opt = Adam(learning_rate = 0.001)\n",
    "# model = mega_model(input_shape=[(2560, 1), (2560, 3)], attx_type='III', attx_st='all', classes = num_classes)\n",
    "\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "method = 'LOSO'\n",
    "dataset_name = 'cola'\n",
    "\n",
    "attx_type = ['FF']\n",
    "attx_st = ['FF']\n",
    "modType = ['ecg', 'eda']\n",
    "\n",
    "\n",
    "for mod in modType:\n",
    "\n",
    "    main_path = r\"X:\\Thesis\\matb2\\Processed_Data\"\n",
    "    with open(os.path.join(main_path, 'cola_labels.pickle'), 'rb') as handle:\n",
    "        sub_label = pickle.load(handle)\n",
    "\n",
    "    if mod == 'ecg':\n",
    "        ipShape = [(2560, 1)]\n",
    "        with open(os.path.join(main_path, 'cola_ecg.pickle'), 'rb') as handle:\n",
    "            sub_dict = pickle.load(handle)\n",
    "    else:\n",
    "        ipShape = [(2560, 3)]\n",
    "        with open(os.path.join(main_path, 'cola_eda.pickle'), 'rb') as handle:\n",
    "            sub_dict = pickle.load(handle)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Training for Modality {}\".format(mod))\n",
    "    print(\"--------------------------------------------------------------------------\\n\")        \n",
    "    \n",
    "    hs, preds, clr = {}, {}, {}\n",
    "\n",
    "    path_logs = r'X:/Data Files/TAFFC/Cola/'\n",
    "    tensorbrd_dir, model_report, model_data, model_score, model_arch, model_fid, model_weights, model_files = create_dirs(path_logs)\n",
    "\n",
    "    for i in sub_dict.keys():\n",
    "\n",
    "        if i in ['1765']:\n",
    "            continue\n",
    "\n",
    "        opt = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho=0.95)\n",
    "        tb = tensorflow.keras.callbacks.TensorBoard(log_dir = os.path.join(tensorbrd_dir,\n",
    "                                                                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "        X_test = sub_dict[i]\n",
    "        y_test = sub_label[i]\n",
    "\n",
    "        X_test = vstack(X_test)\n",
    "        y_test = [x for z in y_test for x in z]\n",
    "\n",
    "\n",
    "        X = [vstack(v) for k, v in sub_dict.items() if k != i]\n",
    "        y_train = [hstack(np.asarray(v)) for k, v in sub_label.items() if k != i]\n",
    "\n",
    "        X = vstack(X)\n",
    "        y_train = hstack(np.asarray(y_train))\n",
    "\n",
    "        y_train = [1 if x > 5 else 0 for x in y_train]\n",
    "        y_test = [1 if x > 5 else 0 for x in y_test]\n",
    "        \n",
    "        y = tensorflow.keras.utils.to_categorical(y_train)\n",
    "        y_test = tensorflow.keras.utils.to_categorical(y_test)\n",
    "\n",
    "        callbacks_list = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score',\n",
    "                                                        patience=7, verbose=1, mode='max', \n",
    "                                                        restore_best_weights=True)\n",
    "\n",
    "        class_wgt = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2)}\n",
    "#             wgt = {0:round(class_wgt[0], 2), 1: round(class_wgt[1], 2), 2: round(class_wgt[2], 2)}\n",
    "\n",
    "        model = ablation_model(input_shape=ipShape, classes = num_classes)\n",
    "        mod_1 = inspect.getsource(mega_model_ecg)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=focal_loss_fx(), metrics=['acc', tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5, average = 'macro')])\n",
    "        print('Testing on {}'.format(i))\n",
    "\n",
    "        hist = model.fit([X], y, epochs=150, verbose=2, shuffle=True,\n",
    "                        batch_size = 256, validation_data = ([X_test], y_test),\n",
    "                        callbacks=[tb, callbacks_list]) # , class_weight=wgt\n",
    "        y_pred_i = model.predict([X_test], batch_size = 128)\n",
    "\n",
    "        pred_list = list()\n",
    "        test_y = list()\n",
    "\n",
    "        for n in range(len(y_pred_i)):\n",
    "            pred_list.append(np.argmax(y_pred_i[n]))\n",
    "            test_y.append(np.argmax(y_test[n]))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        print(classification_report(pred_list, test_y))\n",
    "        a = classification_report(pred_list, test_y,\n",
    "                                target_names = ['Baseline', 'Stress'],\n",
    "                                output_dict=True)\n",
    "\n",
    "        clr[i] = a\n",
    "        hs[i] = hist\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test.astype('int'), y_pred_i, multi_class='ovo', average='weighted')\n",
    "        scores = {'roc_auc': roc_auc, 'pred_prob': y_pred_i,\n",
    "                    'pred': pred_list, 'test_cat': y_test, 'test': test_y}\n",
    "\n",
    "        model.save(os.path.join(model_arch, 'model_{}'.format(i)))\n",
    "        model_wgt_path = os.path.join(model_weights, '_model_{}'.format(i))\n",
    "        model.save_weights(os.path.join(model_wgt_path, 'model_{}'.format(i)))\n",
    "\n",
    "        with open(os.path.join(model_report, 'Test_fold_{}_report.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(clr, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(model_data, 'Test_fold_{}_data.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(model_score, 'Test_fold_{}_scores.pickle'.format(i)), 'wb') as handle:\n",
    "            pickle.dump(scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        create_csv(model_files, a, method, mod_1, dataset_name=dataset_name)\n",
    "        K.clear_session()\n",
    "        \n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print('Classfication report for Ablation {}'.format(mod))    \n",
    "    score_class(clr)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ecg = [vstack(v) for k, v in sub_dict_ecg.items() if k != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 2560, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ecg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 117)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [x for z in y_test for x in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a81570c241a723ffe7e19a7b014b56b8a4aedbc95d5c2f1887147d199b1521a4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('acii')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
